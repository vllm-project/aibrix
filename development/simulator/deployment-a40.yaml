apiVersion: apps/v1
kind: Deployment
metadata:
  name: simulator-llama2-7b-a40
  namespace: aibrix-system
  labels:
    modeladapter.aibricks.ai/enabled: "true"
    model.aibrix.ai/name: "llama2-7b"
    model.aibrix.ai/port: "8000"
spec:
  replicas: 1
  selector:
    matchLabels:
      modeladapter.aibricks.ai/enabled: "true"
      model.aibrix.ai/name: "llama2-7b"
  template:
    metadata:
      labels:
        modeladapter.aibricks.ai/enabled: "true"
        model.aibrix.ai/name: "llama2-7b"
        app: "simulator-llama2-7b-a40"
    spec:
      serviceAccountName: pod-autoscaler
      automountServiceAccountToken: true # Important!
      containers:
        - name: llmengine-simulator
          image: aibrix/vllm-simulator-a40:nightly
          command: ["python", "app.py", "--replica_config_device", "a40"]
          ports:
            - containerPort: 8000
          env:
            - name: DEPLOYMENT_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.labels['app']
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MODEL_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.labels['model.aibrix.ai/name']
---
# Debug only: Make sure pod can be visited from controller that deployed in mac.
apiVersion: v1
kind: Service
metadata:
  name: llama2-7b
  namespace: aibrix-system
spec:
  selector:
    model.aibrix.ai/name: "llama2-7b"
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
      nodePort: 30081
  type: NodePort
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pod-autoscaler
  namespace: aibrix-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: aibrix-system
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: aibrix-system
subjects:
  - kind: ServiceAccount
    name: pod-autoscaler
    namespace: aibrix-system
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: aibrix-system
  name: deployment-reader
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: deployment-reader-binding
  namespace: aibrix-system
subjects:
  - kind: ServiceAccount
    name: pod-autoscaler
    namespace: aibrix-system
roleRef:
  kind: Role
  name: deployment-reader
  apiGroup: rbac.authorization.k8s.io
# ---
# for test-purpose, if need to create HTTPRoute object manually
# apiVersion: gateway.networking.k8s.io/v1
# kind: HTTPRoute
# metadata:
#   name: llama2-7b-router
#   namespace: aibrix-system
# spec:
#   parentRefs:
#     - name: aibrix-eg
#   rules:
#     - matches:
#         - headers:
#             - type: Exact
#               name: model
#               value: llama2-7b
#       backendRefs:
#         - name: llama2-7b
#           port: 8000
---
# Pod autoscaler works with gpu-optimizer
apiVersion: autoscaling.aibrix.ai/v1alpha1
kind: PodAutoscaler
metadata:
  name: podautoscaler-simulator-llama2-7b-a40
  labels:
    app.kubernetes.io/name: aibrix
    app.kubernetes.io/managed-by: kustomize
  namespace: aibrix-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: simulator-llama2-7b-a40
  minReplicas: 0
  maxReplicas: 10
  targetMetric: "avg_prompt_throughput_toks_per_s" # Ignore if metricsSources is configured
  metricsSources: 
    - endpoint: gpu-optimizer.aibrix-system.svc.cluster.local:8080
      path: /metrics/aibrix-system/simulator-llama2-7b-a40
      metric: "vllm:deployment_replicas"
  targetValue: "1"
  scalingStrategy: "KPA"