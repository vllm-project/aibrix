# ==============================================================================
# AIBrix Docker Compose Configuration
# ==============================================================================
# Copy this file to .env and customize for your environment:
#   cp .env.example .env
#
# Required settings are marked with [REQUIRED]
# ==============================================================================

# ==============================================================================
# Model Configuration [REQUIRED]
# ==============================================================================

# Model name/path - can be a HuggingFace model ID or local path
# Note: Llama models require HF_TOKEN (gated). Qwen models are open.
MODEL_NAME=Qwen/Qwen2.5-1.5B-Instruct

# Directory containing cached models (HuggingFace cache)
# This is mounted into the container at /root/.cache/huggingface
MODEL_DIR=~/.cache/huggingface

# HuggingFace token for gated models (like Llama)
# Get yours at: https://huggingface.co/settings/tokens
HF_TOKEN=

# ==============================================================================
# GPU Configuration
# ==============================================================================

# Simple mode: single vLLM engine
VLLM_GPU=0
VLLM_TP=1

# P/D Disaggregation mode: separate prefill/decode engines
# Use with: docker compose --profile pd up -d
PREFILL_GPU=0
PREFILL_TP=1
DECODE_GPU=1
DECODE_TP=1

# ==============================================================================
# Performance Tuning
# ==============================================================================

# Maximum context length
MAX_MODEL_LEN=8192

# GPU memory utilization (0.0 - 1.0)
GPU_MEMORY_UTILIZATION=0.9
PREFILL_GPU_MEMORY=0.85
DECODE_GPU_MEMORY=0.9

# Maximum concurrent sequences per engine
PREFILL_MAX_SEQS=64
DECODE_MAX_SEQS=256

# vLLM attention backend: FLASH_ATTN, XFORMERS, FLASHINFER
VLLM_ATTENTION_BACKEND=FLASH_ATTN

# ==============================================================================
# Network Ports
# ==============================================================================

# HTTP entry point (Envoy proxy)
HTTP_PORT=80

# Direct vLLM access (bypasses Envoy)
VLLM_PORT=8000

# P/D mode ports
PREFILL_PORT=8001
DECODE_PORT=8002

# Metadata service
METADATA_PORT=8090

# Redis
REDIS_PORT=6379

# Envoy admin interface
ENVOY_ADMIN_PORT=9901

# Gateway plugin (if enabled)
GATEWAY_GRPC_PORT=50052
GATEWAY_METRICS_PORT=8080

# ==============================================================================
# AIBrix Version
# ==============================================================================

# Use 'nightly' for latest, or pin to a specific version
AIBRIX_VERSION=nightly

# vLLM image - official or custom
VLLM_IMAGE=vllm/vllm-openai:v0.6.6

# ==============================================================================
# Routing (when gateway plugin is enabled)
# ==============================================================================

# Routing algorithm: least_request, least_load, random, prefix_cache_aware
ROUTING_ALGORITHM=least_request

# ==============================================================================
# Logging
# ==============================================================================

# Log levels: DEBUG, INFO, WARNING, ERROR (must be uppercase for Python services)
LOG_LEVEL=INFO
ENVOY_LOG_LEVEL=warn

# ==============================================================================
# Gateway Plugin Configuration
# ==============================================================================

# Endpoints configuration file
# Simple mode: ./configs/endpoints.yaml (default)
# P/D mode:    ./configs/endpoints-pd.yaml
ENDPOINTS_CONFIG=./configs/endpoints.yaml
