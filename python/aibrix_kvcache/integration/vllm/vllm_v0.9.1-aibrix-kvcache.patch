diff --git a/examples/online_serving/prometheus_grafana/grafana_kv_cache_offload.json b/examples/online_serving/prometheus_grafana/grafana_kv_cache_offload.json
new file mode 100644
index 000000000..311169501
--- /dev/null
+++ b/examples/online_serving/prometheus_grafana/grafana_kv_cache_offload.json
@@ -0,0 +1,4919 @@
+{
+    "annotations": {
+      "list": [
+        {
+          "builtIn": 1,
+          "datasource": {
+            "type": "grafana",
+            "uid": "-- Grafana --"
+          },
+          "enable": true,
+          "hide": true,
+          "iconColor": "rgba(0, 211, 255, 1)",
+          "name": "Annotations & Alerts",
+          "target": {
+            "limit": 100,
+            "matchAny": false,
+            "tags": [],
+            "type": "dashboard"
+          },
+          "type": "dashboard"
+        }
+      ]
+    },
+    "description": "Monitoring vLLM Inference Server",
+    "editable": true,
+    "fiscalYearStartMonth": 0,
+    "graphTooltip": 0,
+    "id": 4,
+    "links": [],
+    "liveNow": false,
+    "panels": [
+      {
+        "collapsed": false,
+        "gridPos": {
+          "h": 1,
+          "w": 24,
+          "x": 0,
+          "y": 0
+        },
+        "id": 19,
+        "panels": [],
+        "title": "vLLM Engine",
+        "type": "row"
+      },
+      {
+        "datasource": {
+          "default": true,
+          "type": "prometheus",
+          "uid": "cdy727ch5evi8d"
+        },
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "thresholds"
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 3,
+          "x": 0,
+          "y": 1
+        },
+        "id": 16,
+        "options": {
+          "colorMode": "value",
+          "graphMode": "area",
+          "justifyMode": "auto",
+          "orientation": "auto",
+          "percentChangeColorMode": "standard",
+          "reduceOptions": {
+            "calcs": [
+              "lastNotNull"
+            ],
+            "fields": "",
+            "values": false
+          },
+          "showPercentChange": false,
+          "textMode": "auto",
+          "wideLayout": true
+        },
+        "pluginVersion": "11.2.0",
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "editorMode": "code",
+            "expr": "sum(increase(vllm:request_success_total{finished_reason=\"stop\", model_name=\"$model_name\", job=\"pods\"}[$__range]))",
+            "instant": false,
+            "legendFormat": "Success Request Count(stop)",
+            "range": true,
+            "refId": "A"
+          }
+        ],
+        "title": "Success Request Count(Stop)",
+        "type": "stat"
+      },
+      {
+        "datasource": {
+          "default": true,
+          "type": "prometheus",
+          "uid": "cdy727ch5evi8d"
+        },
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 5,
+          "x": 3,
+          "y": 1
+        },
+        "id": 17,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "pluginVersion": "11.2.0",
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "editorMode": "code",
+            "expr": "sum by(finished_reason) (increase(vllm:request_success_total{model_name=\"$model_name\", job=\"pods\"}[$__range]))",
+            "instant": false,
+            "legendFormat": "Success Request Count",
+            "range": true,
+            "refId": "A"
+          }
+        ],
+        "title": "Success Request Count",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Heatmap of request prompt length",
+        "fieldConfig": {
+          "defaults": {
+            "custom": {
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "scaleDistribution": {
+                "type": "linear"
+              }
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 5,
+          "x": 8,
+          "y": 1
+        },
+        "id": 12,
+        "options": {
+          "calculate": false,
+          "cellGap": 1,
+          "cellValues": {
+            "unit": "none"
+          },
+          "color": {
+            "exponent": 0.5,
+            "fill": "dark-orange",
+            "min": 0,
+            "mode": "scheme",
+            "reverse": false,
+            "scale": "exponential",
+            "scheme": "Spectral",
+            "steps": 64
+          },
+          "exemplars": {
+            "color": "rgba(255,0,255,0.7)"
+          },
+          "filterValues": {
+            "le": 1e-9
+          },
+          "legend": {
+            "show": true,
+            "showLegend": true
+          },
+          "rowsFrame": {
+            "layout": "auto",
+            "value": "Request count"
+          },
+          "tooltip": {
+            "mode": "single",
+            "showColorScale": false,
+            "yHistogram": true
+          },
+          "yAxis": {
+            "axisLabel": "Prompt Length",
+            "axisPlacement": "left",
+            "reverse": false,
+            "unit": "none"
+          }
+        },
+        "pluginVersion": "11.2.0",
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum by(le) (increase(vllm:request_prompt_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "format": "heatmap",
+            "fullMetaSearch": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "legendFormat": "{{le}}",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          }
+        ],
+        "title": "Request Prompt Length",
+        "type": "heatmap"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Heatmap of request generation length",
+        "fieldConfig": {
+          "defaults": {
+            "custom": {
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "scaleDistribution": {
+                "type": "linear"
+              }
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 5,
+          "x": 13,
+          "y": 1
+        },
+        "id": 13,
+        "options": {
+          "calculate": false,
+          "cellGap": 1,
+          "cellValues": {
+            "unit": "none"
+          },
+          "color": {
+            "exponent": 0.5,
+            "fill": "dark-orange",
+            "min": 0,
+            "mode": "scheme",
+            "reverse": false,
+            "scale": "exponential",
+            "scheme": "Spectral",
+            "steps": 64
+          },
+          "exemplars": {
+            "color": "rgba(255,0,255,0.7)"
+          },
+          "filterValues": {
+            "le": 1e-9
+          },
+          "legend": {
+            "show": true
+          },
+          "rowsFrame": {
+            "layout": "auto",
+            "value": "Request count"
+          },
+          "tooltip": {
+            "mode": "single",
+            "showColorScale": false,
+            "yHistogram": true
+          },
+          "yAxis": {
+            "axisLabel": "Generation Length",
+            "axisPlacement": "left",
+            "reverse": false,
+            "unit": "none"
+          }
+        },
+        "pluginVersion": "11.2.0",
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum by(le) (increase(vllm:request_generation_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "format": "heatmap",
+            "fullMetaSearch": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "legendFormat": "{{le}}",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          }
+        ],
+        "title": "Request Generation Length",
+        "type": "heatmap"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Prompt & Decode length",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "none"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 6,
+          "x": 18,
+          "y": 1
+        },
+        "id": 18,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:request_prompt_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "Prompt-P99",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.9, sum by(le) (rate(vllm:request_prompt_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "Prompt-P90",
+            "range": true,
+            "refId": "C",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.5, sum by(le) (rate(vllm:request_prompt_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "Prompt-P50",
+            "range": true,
+            "refId": "D",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:request_generation_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Decode-P99",
+            "range": true,
+            "refId": "B"
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.90, sum by(le) (rate(vllm:request_generation_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Decode-P90",
+            "range": true,
+            "refId": "E"
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.50, sum by(le) (rate(vllm:request_generation_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Decode-P50",
+            "range": true,
+            "refId": "F"
+          }
+        ],
+        "title": "Prompt & Decode Length",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "End to end request latency measured in seconds.",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "s"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 8,
+          "x": 0,
+          "y": 8
+        },
+        "id": 9,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P99",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "builder",
+            "expr": "histogram_quantile(0.95, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{model_name=\"$model_name\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P95",
+            "range": true,
+            "refId": "B",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "builder",
+            "expr": "histogram_quantile(0.9, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{model_name=\"$model_name\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P90",
+            "range": true,
+            "refId": "C",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "builder",
+            "expr": "histogram_quantile(0.5, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{model_name=\"$model_name\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P50",
+            "range": true,
+            "refId": "D",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(rate(vllm:e2e_request_latency_seconds_sum{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])\n/\nrate(vllm:e2e_request_latency_seconds_count{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Average",
+            "range": true,
+            "refId": "E"
+          }
+        ],
+        "title": "E2E Request Latency",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "P50, P90, P95, and P99 TTFT latency in seconds.",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "s"
+          },
+          "overrides": [
+            {
+              "__systemRef": "hideSeriesFrom",
+              "matcher": {
+                "id": "byNames",
+                "options": {
+                  "mode": "exclude",
+                  "names": [
+                    "Average"
+                  ],
+                  "prefix": "All except:",
+                  "readOnly": true
+                }
+              },
+              "properties": [
+                {
+                  "id": "custom.hideFrom",
+                  "value": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": true
+                  }
+                }
+              ]
+            }
+          ]
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 8,
+          "x": 8,
+          "y": 8
+        },
+        "id": 5,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{model_name=\"$model_name\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P99",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.95, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P95",
+            "range": true,
+            "refId": "B",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.9, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P90",
+            "range": true,
+            "refId": "C",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.5, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P50",
+            "range": true,
+            "refId": "D",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(rate(vllm:time_to_first_token_seconds_sum{model_name=\"$model_name\",job=\"pods\"}[$__rate_interval])\n/\nrate(vllm:time_to_first_token_seconds_count{model_name=\"$model_name\",job=\"pods\"}[$__rate_interval]))",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Average",
+            "range": true,
+            "refId": "E"
+          }
+        ],
+        "title": "Time To First Token Latency",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Inter token latency in seconds.",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "s"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 8,
+          "x": 16,
+          "y": 8
+        },
+        "id": 10,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P99",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.95, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P95",
+            "range": true,
+            "refId": "B",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.9, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P90",
+            "range": true,
+            "refId": "C",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.5, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P50",
+            "range": true,
+            "refId": "D",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "rate(vllm:time_per_output_token_seconds_sum{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])\n/\nrate(vllm:time_per_output_token_seconds_count{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Mean",
+            "range": true,
+            "refId": "E"
+          }
+        ],
+        "title": "Time Per Output Token Latency",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Number of tokens processed per second",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 0,
+          "y": 15
+        },
+        "id": 8,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum(rate(vllm:prompt_tokens_total{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "Prompt Tokens/Sec {{model_name}}",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          }
+        ],
+        "title": "Prompt Token Throughput",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Number of tokens processed per second",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 8,
+          "y": 15
+        },
+        "id": 14,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum(rate(vllm:generation_tokens_total{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "Generation Tokens/Sec {{pod}}",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          }
+        ],
+        "title": "Decode Token Throughput",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Number of requests in RUNNING, WAITING, and SWAPPED state",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "none"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 16,
+          "y": 15
+        },
+        "id": 3,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum(vllm:num_requests_running{model_name=\"$model_name\", job=\"pods\"})",
+            "fullMetaSearch": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "legendFormat": "Num Running",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum(vllm:num_requests_swapped{model_name=\"$model_name\", job=\"pods\"})",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "legendFormat": "Num Swapped",
+            "range": true,
+            "refId": "B",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum(vllm:num_requests_waiting{model_name=\"$model_name\", job=\"pods\"})",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "legendFormat": "Num Waiting",
+            "range": true,
+            "refId": "C",
+            "useBackend": false
+          }
+        ],
+        "title": "Scheduler State",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Percentage of prefix cache hit rate",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "percentunit"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 0,
+          "y": 23
+        },
+        "id": 15,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(vllm:gpu_prefix_cache_hit_rate{model_name=\"$model_name\",job=\"pods\"})",
+            "instant": false,
+            "legendFormat": "GPU Prefix Cache Hit Ratio",
+            "range": true,
+            "refId": "A"
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(vllm:cpu_prefix_cache_hit_rate{model_name=\"$model_name\",job=\"pods\"})",
+            "hide": true,
+            "instant": false,
+            "legendFormat": "CPU Prefix Cache Hit Ratio",
+            "range": true,
+            "refId": "B"
+          }
+        ],
+        "title": "Prefix Cache Hit Rate",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Percentage of used cache blocks by vLLM.",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "percentunit"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 8,
+          "y": 23
+        },
+        "id": 4,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(vllm:gpu_cache_usage_perc{model_name=\"$model_name\", job=\"pods\"})",
+            "instant": false,
+            "legendFormat": "GPU Cache Usage",
+            "range": true,
+            "refId": "A"
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(vllm:cpu_cache_usage_perc{model_name=\"$model_name\", job=\"pods\"})",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "CPU Cache Usage",
+            "range": true,
+            "refId": "B"
+          }
+        ],
+        "title": "Cache Utilization",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Number of finished requests by their finish reason: either an EOS token was generated or the max sequence length was reached.",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 16,
+          "y": 23
+        },
+        "id": 11,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum by(finished_reason) (increase(vllm:request_success_total{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "interval": "",
+            "legendFormat": "__auto",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          }
+        ],
+        "title": "Finish Reason",
+        "type": "timeseries"
+      },
+      {
+        "collapsed": true,
+        "gridPos": {
+          "h": 1,
+          "w": 24,
+          "x": 0,
+          "y": 31
+        },
+        "id": 31,
+        "panels": [
+          {
+            "datasource": {
+              "default": true,
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "thresholds"
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 0,
+              "y": 32
+            },
+            "id": 39,
+            "options": {
+              "colorMode": "value",
+              "graphMode": "area",
+              "justifyMode": "auto",
+              "orientation": "auto",
+              "percentChangeColorMode": "standard",
+              "reduceOptions": {
+                "calcs": [
+                  "lastNotNull"
+                ],
+                "fields": "",
+                "values": false
+              },
+              "showPercentChange": false,
+              "textMode": "auto",
+              "wideLayout": true
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "cdy727ch5evi8d"
+                },
+                "editorMode": "code",
+                "expr": "sum by(op_type) (increase(vllm:ol_connector_num_ops_total{model_name=\"$model_name\", job=\"pods\"}[$__range]))",
+                "instant": false,
+                "legendFormat": "{{op_type}}",
+                "range": true,
+                "refId": "A"
+              }
+            ],
+            "title": "Total Ops",
+            "type": "stat"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Heatmap of prefix length",
+            "fieldConfig": {
+              "defaults": {
+                "custom": {
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "scaleDistribution": {
+                    "type": "linear"
+                  }
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 4,
+              "y": 32
+            },
+            "id": 29,
+            "options": {
+              "calculate": false,
+              "cellGap": 1,
+              "cellValues": {
+                "unit": "none"
+              },
+              "color": {
+                "exponent": 0.5,
+                "fill": "dark-orange",
+                "min": 0,
+                "mode": "scheme",
+                "reverse": false,
+                "scale": "exponential",
+                "scheme": "Spectral",
+                "steps": 64
+              },
+              "exemplars": {
+                "color": "rgba(255,0,255,0.7)"
+              },
+              "filterValues": {
+                "le": 1e-9
+              },
+              "legend": {
+                "show": true,
+                "showLegend": true
+              },
+              "rowsFrame": {
+                "layout": "auto",
+                "value": "Request count"
+              },
+              "tooltip": {
+                "mode": "single",
+                "showColorScale": false,
+                "yHistogram": true
+              },
+              "yAxis": {
+                "axisLabel": "Prompt Length",
+                "axisPlacement": "left",
+                "reverse": false,
+                "unit": "none"
+              }
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "sum by(le, op_type) (increase(vllm:ol_connector_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+                "format": "heatmap",
+                "fullMetaSearch": false,
+                "includeNullMetadata": true,
+                "instant": false,
+                "legendFormat": "{{op_type}}-{{le}}",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              }
+            ],
+            "title": "Prefix Length",
+            "type": "heatmap"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Prefix length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 9,
+              "y": 32
+            },
+            "id": 30,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:ol_connector_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:ol_connector_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:ol_connector_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Prefix Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Heatmap of token length",
+            "fieldConfig": {
+              "defaults": {
+                "custom": {
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "scaleDistribution": {
+                    "type": "linear"
+                  }
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 14,
+              "y": 32
+            },
+            "id": 32,
+            "options": {
+              "calculate": false,
+              "cellGap": 1,
+              "cellValues": {
+                "unit": "none"
+              },
+              "color": {
+                "exponent": 0.5,
+                "fill": "dark-orange",
+                "min": 0,
+                "mode": "scheme",
+                "reverse": false,
+                "scale": "exponential",
+                "scheme": "Spectral",
+                "steps": 64
+              },
+              "exemplars": {
+                "color": "rgba(255,0,255,0.7)"
+              },
+              "filterValues": {
+                "le": 1e-9
+              },
+              "legend": {
+                "show": true,
+                "showLegend": true
+              },
+              "rowsFrame": {
+                "layout": "auto",
+                "value": "Request count"
+              },
+              "tooltip": {
+                "mode": "single",
+                "showColorScale": false,
+                "yHistogram": true
+              },
+              "yAxis": {
+                "axisLabel": "Prompt Length",
+                "axisPlacement": "left",
+                "reverse": false,
+                "unit": "none"
+              }
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "sum by(le, op_type) (increase(vllm:ol_connector_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+                "format": "heatmap",
+                "fullMetaSearch": false,
+                "includeNullMetadata": true,
+                "instant": false,
+                "legendFormat": "{{op_type}}-{{le}}",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              }
+            ],
+            "title": "Token Length",
+            "type": "heatmap"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Token length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 19,
+              "y": 32
+            },
+            "id": 33,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:ol_connector_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:ol_connector_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:ol_connector_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Token Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Latency",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "ms"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 6,
+              "x": 0,
+              "y": 39
+            },
+            "id": 34,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:ol_connector_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:ol_connector_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:ol_connector_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:ol_connector_iteration_rebuild_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "hide": false,
+                "instant": false,
+                "legendFormat": "rebuild_model_input-P99",
+                "range": true,
+                "refId": "B"
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le) (rate(vllm:ol_connector_iteration_rebuild_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "hide": false,
+                "instant": false,
+                "legendFormat": "rebuild_model_input-P90",
+                "range": true,
+                "refId": "E"
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le) (rate(vllm:ol_connector_iteration_rebuild_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "hide": false,
+                "instant": false,
+                "legendFormat": "rebuild_model_input-P50",
+                "range": true,
+                "refId": "F"
+              }
+            ],
+            "title": "Latency",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Heatmap of sent tokens",
+            "fieldConfig": {
+              "defaults": {
+                "custom": {
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "scaleDistribution": {
+                    "type": "linear"
+                  }
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 6,
+              "y": 39
+            },
+            "id": 35,
+            "options": {
+              "calculate": false,
+              "cellGap": 1,
+              "cellValues": {
+                "unit": "none"
+              },
+              "color": {
+                "exponent": 0.5,
+                "fill": "dark-orange",
+                "min": 0,
+                "mode": "scheme",
+                "reverse": false,
+                "scale": "exponential",
+                "scheme": "Spectral",
+                "steps": 64
+              },
+              "exemplars": {
+                "color": "rgba(255,0,255,0.7)"
+              },
+              "filterValues": {
+                "le": 1e-9
+              },
+              "legend": {
+                "show": true,
+                "showLegend": true
+              },
+              "rowsFrame": {
+                "layout": "auto",
+                "value": "Request count"
+              },
+              "tooltip": {
+                "mode": "single",
+                "showColorScale": false,
+                "yHistogram": true
+              },
+              "yAxis": {
+                "axisLabel": "Prompt Length",
+                "axisPlacement": "left",
+                "reverse": false,
+                "unit": "none"
+              }
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "sum by(le, op_type) (increase(vllm:ol_connector_iteration_sent_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+                "format": "heatmap",
+                "fullMetaSearch": false,
+                "includeNullMetadata": true,
+                "instant": false,
+                "legendFormat": "{{op_type}}-{{le}}",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              }
+            ],
+            "title": "Sent Tokens",
+            "type": "heatmap"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Sent tokens",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 10,
+              "y": 39
+            },
+            "id": 36,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:ol_connector_iteration_sent_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:ol_connector_iteration_sent_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:ol_connector_iteration_sent_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Sent Tokens",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Heatmap of received tokens",
+            "fieldConfig": {
+              "defaults": {
+                "custom": {
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "scaleDistribution": {
+                    "type": "linear"
+                  }
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 15,
+              "y": 39
+            },
+            "id": 37,
+            "options": {
+              "calculate": false,
+              "cellGap": 1,
+              "cellValues": {
+                "unit": "none"
+              },
+              "color": {
+                "exponent": 0.5,
+                "fill": "dark-orange",
+                "min": 0,
+                "mode": "scheme",
+                "reverse": false,
+                "scale": "exponential",
+                "scheme": "Spectral",
+                "steps": 64
+              },
+              "exemplars": {
+                "color": "rgba(255,0,255,0.7)"
+              },
+              "filterValues": {
+                "le": 1e-9
+              },
+              "legend": {
+                "show": true,
+                "showLegend": true
+              },
+              "rowsFrame": {
+                "layout": "auto",
+                "value": "Request count"
+              },
+              "tooltip": {
+                "mode": "single",
+                "showColorScale": false,
+                "yHistogram": true
+              },
+              "yAxis": {
+                "axisLabel": "Prompt Length",
+                "axisPlacement": "left",
+                "reverse": false,
+                "unit": "none"
+              }
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "sum by(le, op_type) (increase(vllm:ol_connector_iteration_received_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+                "format": "heatmap",
+                "fullMetaSearch": false,
+                "includeNullMetadata": true,
+                "instant": false,
+                "legendFormat": "{{op_type}}-{{le}}",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              }
+            ],
+            "title": "Received Tokens",
+            "type": "heatmap"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Received tokens",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 19,
+              "y": 39
+            },
+            "id": 38,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:ol_connector_iteration_received_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:ol_connector_iteration_received_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:ol_connector_iteration_received_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Received Tokens",
+            "type": "timeseries"
+          }
+        ],
+        "title": "KV Cache Offloading - AIBrix Connector",
+        "type": "row"
+      },
+      {
+        "collapsed": true,
+        "gridPos": {
+          "h": 1,
+          "w": 24,
+          "x": 0,
+          "y": 32
+        },
+        "id": 40,
+        "panels": [
+          {
+            "datasource": {
+              "default": true,
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "thresholds"
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 0,
+              "y": 33
+            },
+            "id": 47,
+            "options": {
+              "colorMode": "value",
+              "graphMode": "area",
+              "justifyMode": "auto",
+              "orientation": "auto",
+              "percentChangeColorMode": "standard",
+              "reduceOptions": {
+                "calcs": [
+                  "lastNotNull"
+                ],
+                "fields": "",
+                "values": false
+              },
+              "showPercentChange": false,
+              "textMode": "auto",
+              "wideLayout": true
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "cdy727ch5evi8d"
+                },
+                "editorMode": "code",
+                "expr": "sum by(op_type) (increase(vllm:kv_cache_ol_num_ops_total{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__range]))",
+                "instant": false,
+                "legendFormat": "{{op_type}}",
+                "range": true,
+                "refId": "A"
+              }
+            ],
+            "title": "Total Ops",
+            "type": "stat"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Prefix length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 4,
+              "y": 33
+            },
+            "id": 50,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Prefix Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Token length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 9,
+              "y": 33
+            },
+            "id": 51,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Token Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Fetched tokens",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 14,
+              "y": 33
+            },
+            "id": 53,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Fetched Tokens",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Latency",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "ms"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 19,
+              "y": 33
+            },
+            "id": 45,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Latency",
+            "type": "timeseries"
+          }
+        ],
+        "title": "KV Cache Offloading - Internal - CacheMgr",
+        "type": "row"
+      },
+      {
+        "collapsed": true,
+        "gridPos": {
+          "h": 1,
+          "w": 24,
+          "x": 0,
+          "y": 33
+        },
+        "id": 57,
+        "panels": [
+          {
+            "datasource": {
+              "default": true,
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "thresholds"
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 0,
+              "y": 34
+            },
+            "id": 48,
+            "options": {
+              "colorMode": "value",
+              "graphMode": "area",
+              "justifyMode": "auto",
+              "orientation": "auto",
+              "percentChangeColorMode": "standard",
+              "reduceOptions": {
+                "calcs": [
+                  "lastNotNull"
+                ],
+                "fields": "",
+                "values": false
+              },
+              "showPercentChange": false,
+              "textMode": "auto",
+              "wideLayout": true
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "cdy727ch5evi8d"
+                },
+                "editorMode": "code",
+                "expr": "sum by(op_type) (increase(vllm:kv_cache_ol_num_ops_total{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__range]))",
+                "instant": false,
+                "legendFormat": "{{op_type}}",
+                "range": true,
+                "refId": "A"
+              }
+            ],
+            "title": "Total Ops",
+            "type": "stat"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Prefix length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 4,
+              "y": 34
+            },
+            "id": 43,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Prefix Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Token length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 9,
+              "y": 34
+            },
+            "id": 44,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Token Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Fetched tokens",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 14,
+              "y": 34
+            },
+            "id": 46,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Fetched Tokens",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Latency",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "ms"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 19,
+              "y": 34
+            },
+            "id": 55,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Latency",
+            "type": "timeseries"
+          }
+        ],
+        "title": "KV Cache Offloading - Internal - L1Cache",
+        "type": "row"
+      },
+      {
+        "collapsed": true,
+        "gridPos": {
+          "h": 1,
+          "w": 24,
+          "x": 0,
+          "y": 34
+        },
+        "id": 58,
+        "panels": [
+          {
+            "datasource": {
+              "default": true,
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "thresholds"
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 0,
+              "y": 35
+            },
+            "id": 41,
+            "options": {
+              "colorMode": "value",
+              "graphMode": "area",
+              "justifyMode": "auto",
+              "orientation": "auto",
+              "percentChangeColorMode": "standard",
+              "reduceOptions": {
+                "calcs": [
+                  "lastNotNull"
+                ],
+                "fields": "",
+                "values": false
+              },
+              "showPercentChange": false,
+              "textMode": "auto",
+              "wideLayout": true
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "cdy727ch5evi8d"
+                },
+                "editorMode": "code",
+                "expr": "sum by(op_type) (increase(vllm:kv_cache_ol_num_ops_total{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__range]))",
+                "instant": false,
+                "legendFormat": "{{op_type}}",
+                "range": true,
+                "refId": "A"
+              }
+            ],
+            "title": "Total Ops",
+            "type": "stat"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Prefix length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 4,
+              "y": 35
+            },
+            "id": 49,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Prefix Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Token length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 9,
+              "y": 35
+            },
+            "id": 52,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Token Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Fetched tokens",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 14,
+              "y": 35
+            },
+            "id": 54,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Fetched Tokens",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Latency",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "ms"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 19,
+              "y": 35
+            },
+            "id": 56,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Latency",
+            "type": "timeseries"
+          }
+        ],
+        "title": "KV Cache Offloading - Internal - L2Cache",
+        "type": "row"
+      }
+    ],
+    "refresh": "",
+    "schemaVersion": 39,
+    "tags": [
+      "vllm",
+      "model"
+    ],
+    "templating": {
+      "list": [
+        {
+          "current": {
+            "selected": false,
+            "text": "prometheus",
+            "value": "cdy727ch5evi8d"
+          },
+          "hide": 0,
+          "includeAll": false,
+          "label": "datasource",
+          "multi": false,
+          "name": "DS_PROMETHEUS",
+          "options": [],
+          "query": "prometheus",
+          "queryValue": "",
+          "refresh": 1,
+          "regex": "",
+          "skipUrlSync": false,
+          "type": "datasource"
+        },
+        {
+          "current": {
+            "selected": true,
+            "text": "/models/deepseek-coder-6.7b-instruct",
+            "value": "/models/deepseek-coder-6.7b-instruct"
+          },
+          "datasource": {
+            "type": "prometheus",
+            "uid": "${DS_PROMETHEUS}"
+          },
+          "definition": "label_values(model_name)",
+          "hide": 0,
+          "includeAll": false,
+          "label": "model_name",
+          "multi": false,
+          "name": "model_name",
+          "options": [],
+          "query": {
+            "qryType": 1,
+            "query": "label_values(model_name)",
+            "refId": "PrometheusVariableQueryEditor-VariableQuery"
+          },
+          "refresh": 1,
+          "regex": "",
+          "skipUrlSync": false,
+          "sort": 0,
+          "type": "query"
+        }
+      ]
+    },
+    "time": {
+      "from": "now-5m",
+      "to": "now"
+    },
+    "timepicker": {},
+    "timezone": "",
+    "title": "AIBrix Engine Dashboard (vLLM)",
+    "uid": "ce2azhsbhel8gd",
+    "version": 41,
+    "weekStart": ""
+  }
diff --git a/vllm/distributed/kv_transfer/kv_connector/aibrix_offloading_connector.py b/vllm/distributed/kv_transfer/kv_connector/aibrix_offloading_connector.py
new file mode 100644
index 000000000..5efa4ec80
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_connector/aibrix_offloading_connector.py
@@ -0,0 +1,1146 @@
+# SPDX-License-Identifier: Apache-2.0
+import copy
+import dataclasses
+import enum
+import itertools
+import logging
+import time
+from typing import TYPE_CHECKING, Optional, Union
+
+import torch
+from aibrix_kvcache import (BaseKVCacheManager, GroupAwareKVCacheManager,
+                            KVCacheBlockLayout, KVCacheBlockSpec,
+                            KVCacheConfig, KVCacheMetrics, KVCacheTensorSpec,
+                            ModelSpec, TokenListView)
+from aibrix_kvcache._custom_ops import (reshape_and_cache_multi_layer,
+                                        reshape_and_offload_multi_layer)
+from aibrix_kvcache.common.absl_logging import (getLogger, log_every_n_seconds,
+                                                log_if)
+from aibrix_kvcache.metrics import (MS_BUCKETS, TOKEN_BUCKETS,
+                                    BaseMetricsExporter,
+                                    KVCacheMetricsExporter, Metrics)
+from aibrix_kvcache.profiling import tag_wrapper
+from aibrix_kvcache.utils import perf_timer
+
+from vllm.attention import get_attn_backend
+from vllm.attention.backends.flash_attn import FlashAttentionBackend
+# from vllm.attention.backends.flashinfer import FlashInferBackend
+from vllm.attention.backends.xformers import XFormersBackend
+from vllm.distributed import broadcast_tensor_dict, get_tp_group
+from vllm.distributed.kv_transfer.kv_connector.base import KVConnectorBase
+from vllm.distributed.kv_transfer.kv_transfer_metrics import (
+    KVTransferMetrics, KVTransferMetricsExporter)
+from vllm.model_executor import SamplingMetadata
+from vllm.utils import get_kv_cache_torch_dtype, round_down
+
+if TYPE_CHECKING:
+    from vllm.config import VllmConfig
+    from vllm.sequence import IntermediateTensors
+    from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
+
+logger = getLogger(__name__)
+
+OFFLOADING_CONNECTOR_SKIP_THRESHOLD = 8
+OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS = {
+    FlashAttentionBackend.get_name():
+    KVCacheBlockLayout.LCND,
+    # TODO: support FlashInferBackend.
+    # FlashInferBackend.get_name(): KVCacheBlockLayout.LCND,
+    XFormersBackend.get_name():
+    KVCacheBlockLayout.LCND,
+}
+
+
+class AIBrixOffloadingConnectorComputeMetrics(Metrics):
+    """Compute metrics."""
+
+    num_tokens: list[int] = []
+    op_lat_ms: Optional[list[int]] = None
+
+    def __init__(
+        self,
+        enable_time_measurement: bool = True,
+    ) -> None:
+        self._enable_time_measurement = enable_time_measurement
+        self._init_optionals()
+
+    def _init_optionals(self) -> None:
+        if self._enable_time_measurement:
+            self.op_lat_ms = []
+
+    def add(
+        self,
+        num_tokens: int,
+        lat_ms: int,
+    ) -> None:
+        self.num_tokens.append(num_tokens)
+        if self._enable_time_measurement:
+            self.op_lat_ms.append(lat_ms)
+
+    def reset(self) -> None:
+        self.num_tokens = []
+        self._init_optionals()
+
+    def summary(self) -> str:
+        iter_len = len(self.num_tokens)
+        total_tokens = sum(self.num_tokens)
+        avg_tokens = total_tokens / iter_len if iter_len > 0 else 0
+        summary = f"COMPUTE: Num. of tokens (iter): " \
+                  f"total={total_tokens}, avg={avg_tokens:.2f}"
+        if self._enable_time_measurement:
+            total_lat_ms = sum(self.op_lat_ms)
+            avg_lat_ms = total_lat_ms / iter_len if iter_len > 0 else 0
+            summary += f", Latency (iter, ms): total={total_lat_ms:.2f}, " \
+                       f"avg={avg_lat_ms:.2f}"
+        return summary
+
+
+class AIBrixOffloadingConnectorOpMetrics(Metrics):
+    """Op metrics."""
+
+    class OP(enum.Enum):
+        SEND = enum.auto()
+        RECV = enum.auto()
+
+    num_ops: int = 0
+    total_tokens: int = 0
+    total_sent_or_recved_tokens: int = 0
+    num_prefixes: list[int] = []
+    num_tokens: list[int] = []
+    num_sent_or_recved_tokens: list[int] = []
+    op_lat_ms: Optional[list[int]] = None
+    # tracks the latency of rebuilding model_input
+    rebuild_lat_ms: Optional[list[int]] = None
+
+    def __init__(
+        self,
+        op: OP,
+        enable_time_measurement: bool = True,
+    ) -> None:
+        self._op = op
+        self._enable_time_measurement = enable_time_measurement
+        self._init_optionals()
+
+    def _init_optionals(self) -> None:
+        if self._enable_time_measurement:
+            self.op_lat_ms = []
+            self.rebuild_lat_ms = []
+
+    def add(
+        self,
+        num_prefix: int,
+        num_tokens: int,
+        num_sent_or_recved_tokens: int,
+        lat_ms: int,
+    ) -> None:
+        self.num_ops += 1
+        self.total_tokens += num_tokens
+        self.total_sent_or_recved_tokens += num_sent_or_recved_tokens
+        self.num_sent_or_recved_tokens.append(num_sent_or_recved_tokens)
+        self.num_prefixes.append(num_prefix)
+        self.num_tokens.append(num_tokens)
+        if self._enable_time_measurement:
+            self.op_lat_ms.append(lat_ms)
+
+    def record_rebuild_latency(self, lat_ms: int) -> None:
+        if self._enable_time_measurement:
+            self.rebuild_lat_ms.append(lat_ms)
+
+    def reset(self) -> None:
+        self.num_prefixes = []
+        self.num_tokens = []
+        self.num_sent_or_recved_tokens = []
+        self._init_optionals()
+
+    def summary(self) -> str:
+        iter_len = len(self.num_prefixes)
+        total_prefixes = sum(self.num_prefixes)
+        avg_prefixes = total_prefixes / iter_len if iter_len > 0 else 0
+        total_tokens = sum(self.num_tokens)
+        avg_tokens = total_tokens / iter_len if iter_len > 0 else 0
+        total_sent_or_recved_tokens = sum(self.num_sent_or_recved_tokens)
+        avg_sent_or_recved_tokens = (total_sent_or_recved_tokens /
+                                     iter_len if iter_len > 0 else 0)
+        summary = f"{self._op.name}: Num. of ops: {self.num_ops}, " \
+                  f"Total num. of tokens: {self.total_tokens}, "
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            summary += f"Total num. of sent tokens: " \
+                       f"{self.total_sent_or_recved_tokens}, "
+        else:
+            summary += f"Total num. of received tokens: " \
+                       f"{self.total_sent_or_recved_tokens}, "
+        summary += f"Num. of prefixes (iter): total={total_prefixes}, " \
+                   f"avg={avg_prefixes:.2f}, " \
+                   f"Num. of tokens (iter): total={total_tokens}, " \
+                   f"avg={avg_tokens:.2f}"
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            summary += f", Num. of sent tokens (iter): " \
+                       f"total={total_sent_or_recved_tokens}, " \
+                       f"avg={avg_sent_or_recved_tokens:.2f}"
+        else:
+            summary += f", Num. of received tokens (iter): " \
+                       f"total={total_sent_or_recved_tokens}, " \
+                       f"avg={avg_sent_or_recved_tokens:.2f}"
+        if self._enable_time_measurement:
+            total_lat_ms = sum(self.op_lat_ms)
+            avg_lat_ms = total_lat_ms / iter_len if iter_len > 0 else 0
+            summary += f", Latency (iter, ms): total={total_lat_ms:.2f}, " \
+                       f"avg={avg_lat_ms:.2f}"
+            if len(self.rebuild_lat_ms) > 0:
+                iter_len = len(self.rebuild_lat_ms)
+                total_rebuild_lat_ms = sum(self.rebuild_lat_ms)
+                avg_rebuild_lat_ms = total_rebuild_lat_ms / iter_len \
+                    if iter_len > 0 else 0
+                summary += f", model_input rebuild latency (iter, ms): " \
+                           f"total={total_rebuild_lat_ms:.2f}, " \
+                           f"avg={avg_rebuild_lat_ms:.2f}"
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.RECV:
+            hit_rate = (self.total_sent_or_recved_tokens * 100 /
+                        self.total_tokens) if self.total_tokens > 0 else 0
+            summary += f", Hit rate: {hit_rate:.2f}%"
+        return summary
+
+
+class AIBrixOffloadingConnectorOpMetricsExporter(BaseMetricsExporter):
+    OP_TYPE_LABELNAME = "op_type"
+
+    def __init__(self, *, prefix, labelnames, counter_cls, gauge_cls,
+                 histogram_cls) -> None:
+        labelnames = labelnames or []
+        labelnames.append(self.OP_TYPE_LABELNAME)
+
+        super().__init__(
+            prefix=f"{prefix}ol_connector_",
+            labelnames=labelnames,
+            counter_cls=counter_cls,
+            gauge_cls=gauge_cls,
+            histogram_cls=histogram_cls,
+        )
+
+        self._init_exporter_fields()
+
+    def _init_exporter_fields(self) -> None:
+        self.counter_num_ops = self._counter_cls(
+            name=f"{self._prefix}num_ops",
+            documentation="Cumulative number of operations.",
+            labelnames=self._labelnames)
+        self.histogram_iteration_prefixes = self._histogram_cls(
+            name=f"{self._prefix}iteration_prefixes",
+            documentation="Histogram of number of prefixes per iteration.",
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_tokens",
+            documentation="Histogram of number of tokens per iteration.",
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_sent_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_sent_tokens",
+            documentation=("Histogram of number of sent tokens "
+                           "per iteration."),
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_received_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_received_tokens",
+            documentation=("Histogram of number of received tokens "
+                           "per iteration."),
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_op_lat_ms = self._histogram_cls(
+            name=f"{self._prefix}iteration_op_lat_ms",
+            documentation=("Histogram of operation latencies "
+                           "per iteration in ms."),
+            labelnames=self._labelnames,
+            buckets=MS_BUCKETS)
+        self.histogram_iteration_rebuild_lat_ms = self._histogram_cls(
+            name=f"{self._prefix}iteration_rebuild_lat_ms",
+            documentation=("Histogram of rebuilding model_input latencies "
+                           "per iteration in ms."),
+            labelnames=self._labelnames,
+            buckets=MS_BUCKETS)
+
+    def export(
+        self,
+        labels: dict[str, str],
+        metrics: AIBrixOffloadingConnectorOpMetrics
+        | AIBrixOffloadingConnectorComputeMetrics,
+    ) -> None:
+        labels = labels.copy()
+
+        if isinstance(metrics, AIBrixOffloadingConnectorOpMetrics):
+            labels[self.OP_TYPE_LABELNAME] = metrics._op.name.lower()
+            self._export_op_metrics(labels, metrics)
+        else:
+            labels[self.OP_TYPE_LABELNAME] = "compute"
+            self._export_compute_metrics(labels, metrics)
+
+    def _export_op_metrics(
+        self,
+        labels: dict[str, str],
+        metrics: AIBrixOffloadingConnectorOpMetrics,
+    ) -> None:
+        self._export_counter(self.counter_num_ops, labels,
+                             len(metrics.num_prefixes))
+        self._export_histogram(self.histogram_iteration_prefixes, labels,
+                               metrics.num_prefixes)
+        self._export_histogram(self.histogram_iteration_tokens, labels,
+                               metrics.num_tokens)
+        if metrics._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            self._export_histogram(self.histogram_iteration_sent_tokens,
+                                   labels, metrics.num_sent_or_recved_tokens)
+        else:
+            self._export_histogram(self.histogram_iteration_received_tokens,
+                                   labels, metrics.num_sent_or_recved_tokens)
+        if metrics._enable_time_measurement:
+            self._export_histogram(self.histogram_iteration_op_lat_ms, labels,
+                                   metrics.op_lat_ms)
+            self._export_histogram(self.histogram_iteration_rebuild_lat_ms,
+                                   labels, metrics.rebuild_lat_ms)
+
+    def _export_compute_metrics(
+        self,
+        labels: dict[str, str],
+        metrics: AIBrixOffloadingConnectorComputeMetrics,
+    ) -> None:
+        self._export_histogram(self.histogram_iteration_tokens, labels,
+                               metrics.num_tokens)
+        if metrics._enable_time_measurement:
+            self._export_histogram(self.histogram_iteration_op_lat_ms, labels,
+                                   metrics.op_lat_ms)
+
+
+class AIBrixOffloadingConnectorMetrics(KVTransferMetrics):
+
+    def __init__(self, metrics: KVCacheMetrics) -> None:
+        self._cache_metrics = metrics
+        self._time_measurement_enabled = (
+            self._cache_metrics.time_measurement_enabled)
+        self._compute_metrics = AIBrixOffloadingConnectorComputeMetrics(
+            enable_time_measurement=self._time_measurement_enabled)
+        self._send_metrics = AIBrixOffloadingConnectorOpMetrics(
+            AIBrixOffloadingConnectorOpMetrics.OP.SEND,
+            enable_time_measurement=self._time_measurement_enabled,
+        )
+        self._recv_metrics = AIBrixOffloadingConnectorOpMetrics(
+            AIBrixOffloadingConnectorOpMetrics.OP.RECV,
+            enable_time_measurement=self._time_measurement_enabled,
+        )
+
+    @property
+    def time_measurement_enabled(self) -> bool:
+        return self._time_measurement_enabled
+
+    def reset(self) -> None:
+        self._cache_metrics.reset()
+        self._compute_metrics.reset()
+        self._send_metrics.reset()
+        self._recv_metrics.reset()
+
+    def __str__(self) -> str:
+        return f"AIBrixOffloadingConnector metrics: " \
+               f"{self._compute_metrics.summary()}" \
+               f"\n\t{self._send_metrics.summary()}" \
+               f"\n\t{self._recv_metrics.summary()}" \
+               f"\n\t{self._cache_metrics.summary()}"
+
+
+class AIBrixOffloadingConnectorMetricsExporter(KVTransferMetricsExporter):
+    """Metrics for AIBrixOffloadingConnector."""
+
+    def __init__(self, *, prefix, labelnames, gauge_cls, counter_cls,
+                 histogram_cls):
+        self.kv_cache_metrics_exporter = KVCacheMetricsExporter(
+            prefix=prefix,
+            labelnames=labelnames,
+            gauge_cls=gauge_cls,
+            counter_cls=counter_cls,
+            histogram_cls=histogram_cls,
+        )
+        self.ol_connector_op_metrics_exporter = \
+            AIBrixOffloadingConnectorOpMetricsExporter(
+            prefix=prefix,
+            labelnames=labelnames,
+            gauge_cls=gauge_cls,
+            counter_cls=counter_cls,
+            histogram_cls=histogram_cls,
+        )
+
+    def export(
+        self,
+        *,
+        metrics: KVTransferMetrics,
+        labels: dict[str, str],
+    ):
+        self.kv_cache_metrics_exporter.export(
+            metrics=metrics._cache_metrics,
+            labels=labels,
+        )
+        self.ol_connector_op_metrics_exporter.export(
+            metrics=metrics._compute_metrics,
+            labels=labels,
+        )
+        self.ol_connector_op_metrics_exporter.export(
+            metrics=metrics._send_metrics,
+            labels=labels,
+        )
+        self.ol_connector_op_metrics_exporter.export(
+            metrics=metrics._recv_metrics,
+            labels=labels,
+        )
+
+
+@dataclasses.dataclass
+class AIBrixOffloadingConnectorCachedMeta:
+    context_tokens: list[int] = dataclasses.field(default_factory=list)
+    context_tokens_offset: int = 0
+    context_tokens_view: Optional[TokenListView] = None
+
+    # When chunked prefill is enabled, the query length in each iteration may
+    # not be divisible by block_size. Consider the following example, assuming
+    # recv always returns nothing for simplicity. Suppose query_len = 511:
+    #
+    # - In the first iteration, since no tokens are received, we need to send
+    #   all 511 tokens. However, in AIBrixOffloadingConnector, the length is
+    #   aligned to 496, and the remaining 15 tokens are ignored.
+    # - In the second iteration, query_len is likely 511 again, while
+    #   context_len is 511. To prevent a gap in the storage layer caused by
+    #   skipping 15 tokens in the first iteration, AIBrixOffloadingConnector
+    #   shifts context_len to 496 and attempts to send (15 + 511) tokens.
+    #
+    # To handle this correctly, we need to determine the slot mapping for those
+    # 15 tokens. The actual number varies from 1 to 15 but is always less than
+    # block_size. Therefore, we store the slot mapping of the last block in the
+    # metadata.
+    last_block_slot_mapping: Optional[torch.Tensor] = None
+
+    def __init__(self, seq_len: int) -> None:
+        self.context_tokens = [-1] * seq_len
+
+    def get_context_tokens(self) -> list[int]:
+        return self.context_tokens[:self.context_tokens_offset]
+
+    def get_context_tokens_view(self) -> TokenListView:
+        if self.context_tokens_view is None:
+            self.context_tokens_view = TokenListView(self.get_context_tokens())
+        return self.context_tokens_view
+
+    def extend_context_tokens(self, tokens: list[int]) -> None:
+        offset = self.context_tokens_offset
+        length = len(tokens)
+        self.context_tokens[offset:offset + length] = tokens
+        self.context_tokens_offset += length
+        self.context_tokens_view = None
+
+    def clear_context_tokens(self) -> None:
+        self.context_tokens.clear()
+        self.context_tokens_offset = 0
+        self.context_tokens_view = None
+
+
+class AIBrixOffloadingConnector(KVConnectorBase):
+    """AIBrixOffloadingConnector is a KVConnector that offloads KV caches
+    and hidden states to the kv cache offloading service.
+    """
+
+    def __init__(
+        self,
+        rank: int,
+        local_rank: int,
+        config: "VllmConfig",
+    ):
+        cache_config = config.cache_config
+        model_config = config.model_config
+        parallel_config = config.parallel_config
+
+        tp_size = parallel_config.tensor_parallel_size
+        num_kv_heads = model_config.get_num_kv_heads(parallel_config)
+        hidden_size = model_config.get_hidden_size()
+        num_attention_heads = model_config.get_num_attention_heads(
+            parallel_config) * tp_size
+        head_size = int(hidden_size / num_attention_heads)
+        tp_rank = get_tp_group().rank_in_group
+
+        kv_head_ids = list(
+            range(num_kv_heads * tp_rank, num_kv_heads * (tp_rank + 1)))
+        layer_ids = list(
+            range(*model_config.get_layers_start_end_indices(parallel_config)))
+        num_layers = len(layer_ids)
+
+        block_ntokens = cache_config.block_size
+        block_dtype = get_kv_cache_torch_dtype(cache_config.cache_dtype,
+                                               model_config.dtype)
+
+        kv_cache_dtype = cache_config.cache_dtype
+
+        self.attn_backend = get_attn_backend(
+            model_config.get_head_size(),
+            model_config.dtype,
+            kv_cache_dtype,
+            block_ntokens,
+            model_config.is_attention_free,
+            use_mla=model_config.use_mla,
+        )
+
+        block_spec = KVCacheBlockSpec(
+            block_ntokens=block_ntokens,
+            block_dtype=block_dtype,
+            block_layout=self._get_block_layout(),
+            tensor_spec=KVCacheTensorSpec(
+                heads=kv_head_ids,
+                layers=layer_ids,
+                head_size=head_size,
+            ),
+        )
+
+        config = KVCacheConfig(block_spec=block_spec,
+                               model_spec=ModelSpec(
+                                   model_config.max_model_len))
+
+        if parallel_config.tensor_parallel_size == 1:
+            self.cache = BaseKVCacheManager(config=config)
+        else:
+            pg = get_tp_group().cpu_group
+            self.cache = GroupAwareKVCacheManager(config=config,
+                                                  process_group=pg)
+
+        self.rank = tp_rank
+        self.head_size = head_size
+        self.tp_size = tp_size
+        self.num_kv_heads = num_kv_heads
+        self.num_layers = num_layers
+        self.kv_head_ids = kv_head_ids
+        self.layer_ids = layer_ids
+        self.block_ntokens = block_ntokens
+        self.block_dtype = block_dtype
+        self.block_shape = block_spec.block_shape
+        self.block_spec = block_spec
+        self.block_layout = block_spec.block_layout
+        self.chunk_size = self.cache.chunk_size
+        self.cache_feature = self.cache.feature
+        self.kv_cache_dtype = kv_cache_dtype
+
+        self._connector_cache: dict[str,
+                                    AIBrixOffloadingConnectorCachedMeta] = {}
+        self._metrics = AIBrixOffloadingConnectorMetrics(self.cache.metrics)
+        # meta to track compute perf
+        self._compute_start_event = torch.cuda.Event(enable_timing=True)
+        self._compute_end_event = torch.cuda.Event(enable_timing=True)
+        self._compute_total_tokens = 0
+
+        self.k_scales: Optional[list[torch.Tensor]] = None
+        self.v_scales: Optional[list[torch.Tensor]] = None
+
+    @property
+    def metrics(self) -> KVTransferMetrics:
+        return self._metrics
+
+    def get_metrics_exporter_cls(self):
+        return AIBrixOffloadingConnectorMetricsExporter
+
+    def close(self) -> None:
+        if self.cache:
+            self.cache.close()
+            self.cache = None
+
+    def _get_block_layout(self) -> KVCacheBlockLayout:
+
+        if self.attn_backend.get_name() in \
+            OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS:
+            return KVCacheBlockLayout(
+                OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS[
+                    self.attn_backend.get_name()])
+        raise NotImplementedError(
+            f"Only support attn backends in "
+            f"{list(OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS.keys())}. "
+            f"{self.attn_backend.get_name()} is used.")
+
+    def _prepare_request_cache(
+        self,
+        model_input: "ModelInputForGPUWithSamplingMetadata",
+    ) -> None:
+        # remove finished requests
+        for req_id in model_input.finished_requests_ids:
+            if req_id in self._connector_cache:
+                self._connector_cache.pop(req_id)
+
+        # remove decode requests
+        seq_lens = model_input.seq_lens
+        num_prefills = model_input.attn_metadata.num_prefills
+        request_ids = list(model_input.request_ids_to_seq_ids.keys())
+        for seq_idx, _ in enumerate(seq_lens):
+            if seq_idx >= num_prefills:
+                seq_request_id = request_ids[seq_idx]
+                if seq_request_id in self._connector_cache:
+                    self._connector_cache.pop(seq_request_id)
+
+        # add new requests
+        for req_id in model_input.request_ids_to_seq_ids:
+            if req_id not in self._connector_cache:
+                self._connector_cache[req_id] = None
+
+    def _update_request_cache(
+        self,
+        seq_request_id: str,
+        seq_len: int,
+        seq_input_tokens: list[int],
+        seq_slot_mapping: torch.Tensor,
+        kv_transfer_context_tokens: list[int],
+    ) -> None:
+        if self._connector_cache[seq_request_id] is None:
+            self._connector_cache[
+                seq_request_id] = AIBrixOffloadingConnectorCachedMeta(seq_len)
+        seq_request_cache = self._connector_cache[seq_request_id]
+        kv_transfer_context_tokens = kv_transfer_context_tokens or []
+        if seq_len == len(kv_transfer_context_tokens):
+            # when using prefix caching, if all tokens are cached,
+            # we will leave a token in `seq_input_tokens` to avoid
+            # erroneous behavior. In this case, we need to ignore
+            # the token in `seq_input_tokens`.
+            # See `_compute_for_prefix_cache_hit` for more details.
+            seq_input_tokens = []
+        if seq_len != len(seq_request_cache.get_context_tokens()) + len(
+                kv_transfer_context_tokens) + len(seq_input_tokens):
+            # this is a preempted request to be recomputed, let's drop
+            # the context tokens
+            seq_request_cache.clear_context_tokens()
+        if len(kv_transfer_context_tokens) > 0:
+            assert seq_request_cache.context_tokens_offset == 0
+            seq_request_cache.extend_context_tokens(kv_transfer_context_tokens)
+        seq_request_cache.extend_context_tokens(seq_input_tokens)
+        last_block_len = min(self.block_ntokens, seq_slot_mapping.shape[0])
+        seq_request_cache.last_block_slot_mapping = seq_slot_mapping[
+            -last_block_len:]
+
+    def _get_chunk_slot_mapping(
+        self,
+        seq_request_id: str,
+        seq_slot_mapping: torch.Tensor,
+        offset: int,
+        length: int,
+    ) -> torch.Tensor:
+        if offset < 0:
+            chunk_slot_mapping = seq_slot_mapping[:offset + length]
+            # attach more to slot mapping
+            seq_last_block_slot_mapping = self._connector_cache[
+                seq_request_id].last_block_slot_mapping
+            assert seq_last_block_slot_mapping is not None
+            assert -offset <= seq_last_block_slot_mapping.shape[0]
+            prepend_slot_mapping = seq_last_block_slot_mapping[offset:]
+            chunk_slot_mapping = torch.cat(
+                (prepend_slot_mapping, chunk_slot_mapping))
+        else:
+            chunk_slot_mapping = seq_slot_mapping[offset:offset + length]
+
+        assert chunk_slot_mapping.shape[0] == length
+        return chunk_slot_mapping
+
+    def _ensure_kv_scales(self, model_executable: torch.nn.Module) -> None:
+        if self.k_scales is None:
+            start_layer = model_executable.model.start_layer
+            end_layer = model_executable.model.end_layer
+            layers = model_executable.model.layers[start_layer:end_layer]
+            self.k_scales = [layer.self_attn.attn._k_scale for layer in layers]
+            self.v_scales = [layer.self_attn.attn._v_scale for layer in layers]
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnector",
+        "func": "send_kv_caches_and_hidden_states"
+    })
+    def send_kv_caches_and_hidden_states(
+        self,
+        model_executable: torch.nn.Module,
+        model_input: "ModelInputForGPUWithSamplingMetadata",
+        kv_caches: list[torch.Tensor],
+        hidden_or_intermediate_states: Union[torch.Tensor,
+                                             "IntermediateTensors"],
+    ) -> None:
+        attn_metadata = model_input.attn_metadata
+        kv_transfer_metadata = model_input.kv_transfer_metadata
+
+        if kv_transfer_metadata is None:
+            return
+
+        assert attn_metadata is not None
+
+        self._prepare_request_cache(model_input)
+
+        # only send prompt KV caches
+        if attn_metadata.num_prefills <= 0:
+            return
+
+        self._ensure_kv_scales(model_executable)
+        num_prefills = attn_metadata.num_prefills
+        seq_lens = model_input.seq_lens[:num_prefills]
+        query_lens = model_input.query_lens[:num_prefills]
+        slot_mapping = model_input.attn_metadata.slot_mapping.flatten()
+        start_layer = model_executable.model.start_layer
+        end_layer = model_executable.model.end_layer
+        request_ids = list(model_input.request_ids_to_seq_ids.keys())
+
+        if self._metrics.time_measurement_enabled:
+            self._compute_end_event.record()
+            self._compute_end_event.synchronize()
+            compute_lat_ms = self._compute_start_event.elapsed_time(
+                self._compute_end_event)
+            self._metrics._compute_metrics.add(self._compute_total,
+                                               compute_lat_ms)
+
+        # query_lens contains new KV caches that need to be offloaded
+        for seq_idx, query_len in enumerate(query_lens):
+            start_pos = sum(query_lens[:seq_idx])
+            end_pos = start_pos + query_len
+            seq_request_id = request_ids[seq_idx]
+            seq_context_len = seq_lens[seq_idx] - query_len
+            seq_slot_mapping = slot_mapping[start_pos:end_pos]
+            seq_cached_meta = self._connector_cache[seq_request_id]
+            assert seq_cached_meta is not None
+            seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+            assert seq_all_tokens is not None
+            assert len(seq_all_tokens) == seq_lens[
+                seq_idx], f"{len(seq_all_tokens)}!={seq_lens[seq_idx]}"
+            prompt_len = kv_transfer_metadata.seq_groups[seq_idx].prompt_len
+
+            # align to block boundary
+            aligned_context_len = round_down(seq_context_len,
+                                             self.block_ntokens)
+            actual_query_len = seq_context_len + query_len - aligned_context_len
+            aligned_query_len = round_down(actual_query_len,
+                                           self.block_ntokens)
+
+            # skip if there are not enough tokens to send after alignment
+            if prompt_len == seq_lens[seq_idx]:
+                # If chunked prefill is not enabled or this is the last
+                # chunk, we use a larger skip threshold
+                skip_threshold = OFFLOADING_CONNECTOR_SKIP_THRESHOLD
+            else:
+                # This is an intermediate chunk, only skip if this is not
+                # a full block
+                skip_threshold = 1
+            if aligned_query_len <= skip_threshold * self.block_ntokens:
+                continue
+
+            assert len(
+                seq_all_tokens) >= aligned_context_len + aligned_query_len
+
+            prefix = seq_all_tokens[:aligned_context_len]
+            tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                    aligned_query_len]
+
+            if self._metrics.time_measurement_enabled:
+                start = torch.cuda.Event(enable_timing=True)
+                end = torch.cuda.Event(enable_timing=True)
+                start.record()
+
+            total_sent = 0
+            for (
+                    chunk_prefix,
+                    chunk_tokens,
+                    _,
+                    all,
+            ) in self.cache.cache_chunk_keys(prefix, tokens):
+                # |----------- seq_context_len ---------|- offset -|
+                # |- aligned_context_len -|- shift_len -|
+                #                         |--- n * chunk_size -----|
+                # |----------------- chunk_prefix -----------------|- tokens -|
+                #                         |-------- (n + 1) * chunk_size -----|
+                chunk_size = len(chunk_tokens)
+                offset = len(chunk_prefix) - seq_context_len
+                length = chunk_size
+
+                exists_status = self.cache.exists(chunk_prefix, chunk_tokens)
+                if exists_status.is_ok():
+                    num_existing_tokens = exists_status.value
+                    logger.info(
+                        "Request[id=%s] send(%d) encounters %d existing tokens",
+                        seq_request_id, length, num_existing_tokens)
+                    if chunk_size - num_existing_tokens < self.block_ntokens:
+                        continue
+                    else:
+                        # partially exists
+                        offset += num_existing_tokens
+                        length -= num_existing_tokens
+                        new_chunk_prefix_len = (len(chunk_prefix) +
+                                                num_existing_tokens)
+                        chunk_prefix = all[:new_chunk_prefix_len]
+                        chunk_tokens = all[
+                            new_chunk_prefix_len:new_chunk_prefix_len + length]
+
+                # allocate space for KV caches
+                status = self.cache.allocate_for(chunk_prefix, chunk_tokens)
+                if not status.is_ok():
+                    log_every_n_seconds(logger, logging.ERROR,
+                                        "Failed to allocate : %s", 3,
+                                        str(status))
+                    break
+                handle = status.value
+                tensors = handle.to_tensors()
+                length = len(tensors) * self.block_ntokens
+
+                chunk_slot_mapping = self._get_chunk_slot_mapping(
+                    seq_request_id, seq_slot_mapping, offset, length)
+
+                with perf_timer() as get_kernel_offload_dur_ms:
+                    reshape_and_offload_multi_layer(
+                        tensors,
+                        kv_caches[start_layer:end_layer],
+                        chunk_slot_mapping,
+                        self.block_ntokens,
+                        self.kv_cache_dtype,
+                        self.k_scales,
+                        self.v_scales,
+                        self.block_layout.name,
+                    )
+
+                logger.info("Request[id=%s] offloads %d tokens in %.4f ms",
+                            seq_request_id, length,
+                            get_kernel_offload_dur_ms())
+
+                # put KV caches to offloading service
+                status = self.cache.put(chunk_prefix, chunk_tokens[:length],
+                                        handle)
+                if not status.is_ok():
+                    # TODO: notify other ranks in the group to stop sending
+                    # if this is a fatal error
+                    log_every_n_seconds(
+                        logger, logging.ERROR,
+                        "Failed to put to offloading service: %s", 3,
+                        str(status))
+                    break
+                else:
+                    total_sent += length
+
+            log_if(
+                logger,
+                logging.INFO,
+                "Request[id=%s, prompt_len=%d, context_len=%d] sent %d tokens",
+                total_sent > 0,
+                seq_request_id,
+                prompt_len,
+                seq_context_len,
+                total_sent,
+            )
+
+            if self._metrics.time_measurement_enabled:
+                end.record()
+                end.synchronize()
+                lat_ms = start.elapsed_time(end)
+                self._metrics._send_metrics.add(aligned_context_len,
+                                                aligned_query_len, total_sent,
+                                                lat_ms)
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnector",
+        "func": "recv_kv_caches_and_hidden_states"
+    })
+    def recv_kv_caches_and_hidden_states(
+        self,
+        model_executable: torch.nn.Module,
+        model_input: "ModelInputForGPUWithSamplingMetadata",
+        kv_caches: list[torch.Tensor],
+    ) -> tuple[
+            Union[torch.Tensor, "IntermediateTensors"],
+            bool,
+            "ModelInputForGPUWithSamplingMetadata",
+    ]:
+        attn_metadata = model_input.attn_metadata
+        kv_transfer_metadata = model_input.kv_transfer_metadata
+
+        hidden_or_intermediate_states = None
+
+        # TODO:
+        # When using KV cache offloading with chunk-prefill enabled, for
+        # the first `n` chunks (except the last chunk before the decode
+        # phase), we can bypass the model execution if there is a full
+        # hit in the KV cache.
+        bypass_model_exec = False
+
+        if kv_transfer_metadata is None:
+            return hidden_or_intermediate_states, bypass_model_exec, model_input
+
+        assert attn_metadata is not None
+
+        self._compute_total = 0
+        self._prepare_request_cache(model_input)
+
+        # only recv prompt KV caches
+        if attn_metadata.num_prefills <= 0:
+            return hidden_or_intermediate_states, bypass_model_exec, model_input
+
+        self._ensure_kv_scales(model_executable)
+        num_prefills = attn_metadata.num_prefills
+        input_tokens_tensor = model_input.input_tokens
+        seq_lens = model_input.seq_lens[:num_prefills]
+        query_lens = model_input.query_lens[:num_prefills]
+        slot_mapping = model_input.attn_metadata.slot_mapping.flatten()
+        start_layer = model_executable.model.start_layer
+        end_layer = model_executable.model.end_layer
+        request_ids = list(model_input.request_ids_to_seq_ids.keys())
+
+        if kv_transfer_metadata.seq_group_metadata_list is not None:
+            assert len(request_ids) == len(
+                kv_transfer_metadata.seq_group_metadata_list)
+
+        model_config = model_executable.model.config
+        num_kv_heads = int(model_config.num_key_value_heads / self.tp_size)
+        hidden_size = model_config.hidden_size
+        num_attention_heads = model_config.num_attention_heads
+        head_size = int(hidden_size / num_attention_heads)
+
+        assert num_kv_heads == self.num_kv_heads
+        assert head_size == self.head_size
+
+        reused_lens = []
+        rebuild_model_input = False
+        # query_lens contains new KV caches to be received
+        for seq_idx, query_len in enumerate(query_lens):
+            start_pos = sum(query_lens[:seq_idx])
+            end_pos = start_pos + query_len
+            seq_request_id = request_ids[seq_idx]
+            seq_context_len = seq_lens[seq_idx] - query_len
+            seq_input_tokens = (
+                input_tokens_tensor[start_pos:end_pos].cpu().tolist())
+            prompt_len = kv_transfer_metadata.seq_groups[seq_idx].prompt_len
+
+            seq_slot_mapping = slot_mapping[start_pos:end_pos]
+            # will update these lens later if needed
+            reused_lens.append(0)
+
+            # align to block boundary
+            aligned_context_len = round_down(seq_context_len,
+                                             self.block_ntokens)
+            actual_query_len = seq_context_len + query_len - aligned_context_len
+            aligned_query_len = round_down(actual_query_len,
+                                           self.block_ntokens)
+            shift_len = seq_context_len - aligned_context_len
+
+            self._update_request_cache(
+                seq_request_id,
+                seq_lens[seq_idx],
+                seq_input_tokens,
+                seq_slot_mapping,
+                kv_transfer_metadata.seq_groups[seq_idx].context_tokens,
+            )
+
+            # skip if there are not enough tokens to receive after alignment
+            if prompt_len == seq_lens[seq_idx]:
+                # If chunked prefill is not enabled or this is the last
+                # chunk, we use a larger skip threshold
+                skip_threshold = OFFLOADING_CONNECTOR_SKIP_THRESHOLD
+            else:
+                # This is an intermediate chunk, only skip if this is not
+                # a full block
+                skip_threshold = 1
+            if aligned_query_len <= skip_threshold * self.block_ntokens:
+                continue
+
+            seq_cached_meta = self._connector_cache[seq_request_id]
+            assert seq_cached_meta is not None
+            seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+            assert seq_all_tokens is not None
+            assert len(seq_all_tokens) == seq_lens[
+                seq_idx], f"{len(seq_all_tokens)}!={seq_lens[seq_idx]}"
+
+            assert len(
+                seq_all_tokens) >= aligned_context_len + aligned_query_len
+
+            prefix = seq_all_tokens[:aligned_context_len]
+            tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                    aligned_query_len]
+
+            if self._metrics.time_measurement_enabled:
+                start = torch.cuda.Event(enable_timing=True)
+                end = torch.cuda.Event(enable_timing=True)
+                start.record()
+
+            seq_recv_len = 0
+            for (
+                    chunk_prefix,
+                    chunk_tokens,
+                    next_tokens,
+                    _,
+            ) in self.cache.cache_chunk_keys(prefix, tokens):
+                if next_tokens and len(next_tokens) > 0:
+                    # prefetch
+                    self.cache.prefetch(chunk_prefix + chunk_tokens,
+                                        next_tokens)
+
+                # get KV caches from offloading service
+                status = self.cache.acquire(chunk_prefix, chunk_tokens)
+
+                if not status.is_ok():
+                    if not status.is_not_found():
+                        log_every_n_seconds(
+                            logger, logging.ERROR,
+                            "Failed to get from offloading service: %s", 3,
+                            str(status))
+                    break
+
+                num_fetched_tokens, handle = status.value
+                kv_blocks = handle.to_tensors()
+
+                offset = len(chunk_prefix) - seq_context_len
+                length = num_fetched_tokens
+
+                chunk_slot_mapping = self._get_chunk_slot_mapping(
+                    seq_request_id, seq_slot_mapping, offset, length)
+
+                with perf_timer() as get_kernel_onload_dur_ms:
+                    reshape_and_cache_multi_layer(
+                        kv_blocks,
+                        kv_caches[start_layer:end_layer],
+                        chunk_slot_mapping,
+                        self.block_ntokens,
+                        self.kv_cache_dtype,
+                        self.k_scales,
+                        self.v_scales,
+                        self.block_layout.name,
+                    )
+
+                logger.info("Request[id=%s] onloads %d tokens in %.4f ms",
+                            seq_request_id, length, get_kernel_onload_dur_ms())
+
+                # update recv_len
+                seq_recv_len += num_fetched_tokens - shift_len
+                rebuild_model_input = True
+                # reset shift_len
+                shift_len = 0
+
+                # release handle
+                handle.release()
+
+                if num_fetched_tokens < len(chunk_tokens):
+                    # didn't receive all tokens for current chunk, break
+                    break
+
+            reused_lens[-1] = seq_recv_len
+            log_if(
+                logger,
+                logging.INFO,
+                ("Request[id=%s, prompt_len=%d, context_len=%d] "
+                 "reused %d tokens"),
+                seq_recv_len > 0,
+                seq_request_id,
+                prompt_len,
+                seq_context_len,
+                seq_recv_len,
+            )
+
+            if self._metrics.time_measurement_enabled:
+                end.record()
+                end.synchronize()
+                lat_ms = start.elapsed_time(end)
+                self._metrics._recv_metrics.add(aligned_context_len,
+                                                aligned_query_len,
+                                                seq_recv_len, lat_ms)
+
+        if rebuild_model_input:
+            if self._metrics.time_measurement_enabled:
+                start = time.perf_counter()
+            model_input = self._rebuild_model_input(model_input, reused_lens)
+            if self._metrics.time_measurement_enabled:
+                end = time.perf_counter()
+                lat_ms = (end - start) * 1000
+                self._metrics._recv_metrics.record_rebuild_latency(lat_ms)
+
+        if self._metrics.time_measurement_enabled:
+            self._compute_start_event.record()
+            self._compute_total = sum(query_lens) - sum(reused_lens)
+
+        return hidden_or_intermediate_states, bypass_model_exec, model_input
+
+    def _rebuild_model_input(
+        self,
+        model_input: "ModelInputForGPUWithSamplingMetadata",
+        reused_lens: list[int],
+    ) -> "ModelInputForGPUWithSamplingMetadata":
+        seq_group_metadata_list = (
+            model_input.kv_transfer_metadata.seq_group_metadata_list)
+        if seq_group_metadata_list:
+            new_model_input = self._driver_rebuild_model_input(
+                model_input, reused_lens)
+            if self.tp_size > 1:
+                broadcast_data = new_model_input.as_broadcastable_tensor_dict()
+                broadcast_tensor_dict(broadcast_data, src=0)
+        else:
+            # worker
+            runner = model_input.kv_transfer_metadata.runner
+            broadcast_data = broadcast_tensor_dict(src=0)
+            new_model_input = (
+                runner.make_model_input_from_broadcasted_tensor_dict(
+                    broadcast_data))
+
+        # replace fields
+        return dataclasses.replace(
+            model_input,
+            input_tokens=new_model_input.input_tokens,
+            input_positions=new_model_input.input_positions,
+            token_types=new_model_input.token_types,
+            attn_metadata=new_model_input.attn_metadata,
+            seq_lens=new_model_input.seq_lens,
+            query_lens=new_model_input.query_lens,
+            lora_mapping=new_model_input.lora_mapping,
+            lora_requests=new_model_input.lora_requests,
+            multi_modal_kwargs=new_model_input.multi_modal_kwargs,
+            prompt_adapter_mapping=new_model_input.prompt_adapter_mapping,
+            prompt_adapter_requests=new_model_input.prompt_adapter_requests,
+            sampling_metadata=new_model_input.sampling_metadata,
+        )
+
+    def _driver_rebuild_model_input(
+        self,
+        model_input: "ModelInputForGPUWithSamplingMetadata",
+        reused_lens: list[int],
+    ) -> "ModelInputForGPUWithSamplingMetadata":
+        seq_group_metadata_list = (
+            model_input.kv_transfer_metadata.seq_group_metadata_list)
+        runner = model_input.kv_transfer_metadata.runner
+        finished_requests_ids = model_input.finished_requests_ids
+
+        request_ids = list(model_input.request_ids_to_seq_ids.keys())
+        assert len(request_ids) == len(seq_group_metadata_list)
+
+        seq_lens = model_input.seq_lens
+        query_lens = model_input.query_lens
+
+        backup = [None] * len(seq_group_metadata_list)
+        for offset, seq_group_metadata in enumerate(seq_group_metadata_list):
+            if not seq_group_metadata.is_prompt:
+                break
+            if reused_lens[offset] == 0:
+                continue
+            # Prefill has only 1 sequence
+            backup[offset] = seq_group_metadata.computed_block_nums
+            context_len = seq_lens[offset] - query_lens[offset]
+            context_len += reused_lens[offset]
+            num_blocks = context_len // self.block_ntokens
+            seq_group_metadata.computed_block_nums = copy.deepcopy(
+                seq_group_metadata.computed_block_nums)
+            seq_group_metadata.computed_block_nums.extend(
+                itertools.repeat(0, num_blocks))
+
+        new_model_input = runner._prepare_model_input_tensors(
+            seq_group_metadata_list, finished_requests_ids)
+
+        if model_input.sampling_metadata is not None:
+            generators = runner.get_generators(finished_requests_ids)
+            sampling_metadata = SamplingMetadata.prepare(
+                seq_group_metadata_list,
+                new_model_input.seq_lens,
+                new_model_input.query_lens,
+                runner.device,
+                runner.pin_memory,
+                generators,
+                runner.sampling_metadata_cache,
+            )
+            new_model_input = dataclasses.replace(
+                new_model_input, sampling_metadata=sampling_metadata)
+
+        # revert changes on seq_group_metadata_list
+        for offset, seq_group_metadata in enumerate(seq_group_metadata_list):
+            if not seq_group_metadata.is_prompt:
+                break
+            if reused_lens[offset] == 0:
+                continue
+            seq_group_metadata.computed_block_nums = backup[offset]
+
+        return new_model_input
diff --git a/vllm/distributed/kv_transfer/kv_connector/base.py b/vllm/distributed/kv_transfer/kv_connector/base.py
index 181c33925..be54e4186 100644
--- a/vllm/distributed/kv_transfer/kv_connector/base.py
+++ b/vllm/distributed/kv_transfer/kv_connector/base.py
@@ -20,6 +20,9 @@ if TYPE_CHECKING:
     from vllm.config import VllmConfig
     from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
 
+    from ..kv_transfer_metrics import (KVTransferMetrics,
+                                       KVTransferMetricsExporter)
+
 
 class KVConnectorBase(ABC):
     """
@@ -51,6 +54,21 @@ class KVConnectorBase(ABC):
         """
         raise NotImplementedError
 
+    @property
+    def metrics(self) -> 'KVTransferMetrics':
+        """
+        Get the metrics object associated with the connector.
+        Returns:
+            KVTransferMetrics: The metrics object.
+        """
+        return None
+
+    def get_metrics_exporter_cls(self) -> 'KVTransferMetricsExporter':
+        """
+        Get the metrics exporter class associated with the connector.
+        """
+        return None
+
     @abstractmethod
     def send_kv_caches_and_hidden_states(
         self,
diff --git a/vllm/distributed/kv_transfer/kv_connector/factory.py b/vllm/distributed/kv_transfer/kv_connector/factory.py
index 58dfa251c..7c78f901c 100644
--- a/vllm/distributed/kv_transfer/kv_connector/factory.py
+++ b/vllm/distributed/kv_transfer/kv_connector/factory.py
@@ -126,3 +126,13 @@ KVConnectorFactory.register_connector(
     "MultiConnector",
     "vllm.distributed.kv_transfer.kv_connector.v1.multi_connector",
     "MultiConnector")
+
+KVConnectorFactory.register_connector(
+    "AIBrixOffloadingConnector",
+    "vllm.distributed.kv_transfer.kv_connector.aibrix_offloading_connector",
+    "AIBrixOffloadingConnector")
+
+KVConnectorFactory.register_connector(
+    "AIBrixOffloadingConnectorV1Type1",
+    "vllm.distributed.kv_transfer.kv_connector.v1"
+    ".aibrix_offloading_connector_type1", "AIBrixOffloadingConnector")
diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type1.py b/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type1.py
new file mode 100644
index 000000000..82789c732
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type1.py
@@ -0,0 +1,1240 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+import copy
+import dataclasses
+import enum
+import logging
+from functools import wraps
+from typing import TYPE_CHECKING, Any, Callable, Optional, TypeVar
+
+import torch
+from aibrix_kvcache import (BaseKVCacheManager, GroupAwareKVCacheManager,
+                            KVCacheBlockLayout, KVCacheBlockSpec,
+                            KVCacheConfig, KVCacheMetrics, KVCacheTensorSpec,
+                            ModelSpec, TokenListView)
+from aibrix_kvcache._custom_ops import (reshape_and_cache_multi_layer,
+                                        reshape_and_offload_multi_layer)
+from aibrix_kvcache.common.absl_logging import (getLogger, log_every_n_seconds,
+                                                log_if)
+from aibrix_kvcache.common.cached_pyobject import CachedPyObjectBase
+from aibrix_kvcache.metrics import (MS_BUCKETS, TOKEN_BUCKETS,
+                                    BaseMetricsExporter,
+                                    KVCacheMetricsExporter, Metrics)
+from aibrix_kvcache.profiling import tag_wrapper
+from aibrix_kvcache.utils import perf_timer
+
+from vllm.attention import get_attn_backend
+# from vllm.v1.attention.backends.flashinfer import FlashInferBackend
+# from vllm.v1.attention.backends.flex_attention import FlexAttentionBackend
+# from vllm.v1.attention.backends.triton_attn import TritonAttentionBackend
+from vllm.distributed import (get_tp_group, get_world_group,
+                              init_model_parallel_group)
+from vllm.distributed.kv_transfer.kv_connector.v1.base import (
+    KVConnectorBase_V1, KVConnectorMetadata, KVConnectorRole)
+from vllm.distributed.kv_transfer.kv_transfer_metrics import (
+    KVTransferMetrics, KVTransferMetricsExporter)
+from vllm.utils import get_kv_cache_torch_dtype, round_down
+from vllm.v1.attention.backends.flash_attn import FlashAttentionBackend
+
+if TYPE_CHECKING:
+    from vllm.attention.backends.abstract import AttentionMetadata
+    from vllm.config import VllmConfig
+    from vllm.forward_context import ForwardContext
+    from vllm.v1.core.kv_cache_manager import KVCacheBlocks
+    from vllm.v1.core.sched.output import SchedulerOutput
+    from vllm.v1.request import Request
+
+logger = getLogger(__name__)
+
+OFFLOADING_CONNECTOR_SKIP_THRESHOLD = 8
+OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS = {
+    FlashAttentionBackend.get_name(): KVCacheBlockLayout.LCND,
+}
+OFFLOADING_CONNECTOR_MAX_PENDING_REQUESTS_DEFAULT = 32
+
+T = TypeVar('T')
+
+
+def delegate_to(
+        member_name: str) -> Callable[[Callable[..., T]], Callable[..., T]]:
+    """
+    Decorator that delegates a method call to a member object.
+    
+    Args:
+        member_name: The name of the member attribute to delegate to.
+    """
+
+    def decorator(method: Callable[..., T]) -> Callable[..., T]:
+
+        @wraps(method)
+        def wrapper(self, *args, **kwargs) -> T:
+            member = getattr(self, member_name)
+            assert member is not None, f"{member_name} is not set"
+            member_method = getattr(member, method.__name__)
+            return member_method(*args, **kwargs)
+
+        return wrapper
+
+    return decorator
+
+
+class AIBrixOffloadingConnectorOpMetrics(Metrics):
+    """Op metrics."""
+
+    class OP(enum.Enum):
+        SEND = enum.auto()
+        RECV = enum.auto()
+
+    num_ops: int = 0
+    total_tokens: int = 0
+    total_sent_or_recved_tokens: int = 0
+    num_prefixes: list[int] = []
+    num_tokens: list[int] = []
+    num_sent_or_recved_tokens: list[int] = []
+    op_lat_ms: Optional[list[int]] = None
+
+    def __init__(
+        self,
+        op: OP,
+        enable_time_measurement: bool = True,
+    ) -> None:
+        self._op = op
+        self._enable_time_measurement = enable_time_measurement
+        self._init_optionals()
+
+    def _init_optionals(self) -> None:
+        if self._enable_time_measurement:
+            self.op_lat_ms = []
+
+    def add(
+        self,
+        num_prefix: int,
+        num_tokens: int,
+        num_sent_or_recved_tokens: int,
+        lat_ms: int,
+    ) -> None:
+        self.num_ops += 1
+        self.total_tokens += num_tokens
+        self.total_sent_or_recved_tokens += num_sent_or_recved_tokens
+        self.num_sent_or_recved_tokens.append(num_sent_or_recved_tokens)
+        self.num_prefixes.append(num_prefix)
+        self.num_tokens.append(num_tokens)
+        if self._enable_time_measurement:
+            self.op_lat_ms.append(lat_ms)
+
+    def reset(self) -> None:
+        self.num_prefixes = []
+        self.num_tokens = []
+        self.num_sent_or_recved_tokens = []
+        self._init_optionals()
+
+    def summary(self) -> str:
+        iter_len = len(self.num_prefixes)
+        total_prefixes = sum(self.num_prefixes)
+        avg_prefixes = total_prefixes / iter_len if iter_len > 0 else 0
+        total_tokens = sum(self.num_tokens)
+        avg_tokens = total_tokens / iter_len if iter_len > 0 else 0
+        total_sent_or_recved_tokens = sum(self.num_sent_or_recved_tokens)
+        avg_sent_or_recved_tokens = (total_sent_or_recved_tokens /
+                                     iter_len if iter_len > 0 else 0)
+        summary = f"{self._op.name}: Num. of ops: {self.num_ops}, " \
+                  f"Total num. of tokens: {self.total_tokens}, "
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            summary += f"Total num. of sent tokens: " \
+                       f"{self.total_sent_or_recved_tokens}, "
+        else:
+            summary += f"Total num. of received tokens: " \
+                       f"{self.total_sent_or_recved_tokens}, "
+        summary += f"Num. of prefixes (iter): total={total_prefixes}, " \
+                   f"avg={avg_prefixes:.2f}, " \
+                   f"Num. of tokens (iter): total={total_tokens}, " \
+                   f"avg={avg_tokens:.2f}"
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            summary += f", Num. of sent tokens (iter): " \
+                       f"total={total_sent_or_recved_tokens}, " \
+                       f"avg={avg_sent_or_recved_tokens:.2f}"
+        else:
+            summary += f", Num. of received tokens (iter): " \
+                       f"total={total_sent_or_recved_tokens}, " \
+                       f"avg={avg_sent_or_recved_tokens:.2f}"
+        if self._enable_time_measurement:
+            total_lat_ms = sum(self.op_lat_ms)
+            avg_lat_ms = total_lat_ms / iter_len if iter_len > 0 else 0
+            summary += f", Latency (iter, ms): total={total_lat_ms:.2f}, " \
+                       f"avg={avg_lat_ms:.2f}"
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.RECV:
+            hit_rate = (self.total_sent_or_recved_tokens * 100 /
+                        self.total_tokens) if self.total_tokens > 0 else 0
+            summary += f", Hit rate: {hit_rate:.2f}%"
+        return summary
+
+
+class AIBrixOffloadingConnectorOpMetricsExporter(BaseMetricsExporter):
+    OP_TYPE_LABELNAME = "op_type"
+
+    def __init__(self, *, prefix, labelnames, counter_cls, gauge_cls,
+                 histogram_cls) -> None:
+        labelnames = labelnames or []
+        labelnames.append(self.OP_TYPE_LABELNAME)
+
+        super().__init__(
+            prefix=f"{prefix}ol_connector_",
+            labelnames=labelnames,
+            counter_cls=counter_cls,
+            gauge_cls=gauge_cls,
+            histogram_cls=histogram_cls,
+        )
+
+        self._init_exporter_fields()
+
+    def _init_exporter_fields(self) -> None:
+        self.counter_num_ops = self._counter_cls(
+            name=f"{self._prefix}num_ops",
+            documentation="Cumulative number of operations.",
+            labelnames=self._labelnames)
+        self.histogram_iteration_prefixes = self._histogram_cls(
+            name=f"{self._prefix}iteration_prefixes",
+            documentation="Histogram of number of prefixes per iteration.",
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_tokens",
+            documentation="Histogram of number of tokens per iteration.",
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_sent_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_sent_tokens",
+            documentation=("Histogram of number of sent tokens "
+                           "per iteration."),
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_received_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_received_tokens",
+            documentation=("Histogram of number of received tokens "
+                           "per iteration."),
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_op_lat_ms = self._histogram_cls(
+            name=f"{self._prefix}iteration_op_lat_ms",
+            documentation=("Histogram of operation latencies "
+                           "per iteration in ms."),
+            labelnames=self._labelnames,
+            buckets=MS_BUCKETS)
+
+    def export(
+        self,
+        labels: dict[str, str],
+        metrics: AIBrixOffloadingConnectorOpMetrics,
+    ) -> None:
+        labels = labels.copy()
+
+        labels[self.OP_TYPE_LABELNAME] = metrics._op.name.lower()
+        self._export_op_metrics(labels, metrics)
+
+    def _export_op_metrics(
+        self,
+        labels: dict[str, str],
+        metrics: AIBrixOffloadingConnectorOpMetrics,
+    ) -> None:
+        self._export_counter(self.counter_num_ops, labels,
+                             len(metrics.num_prefixes))
+        self._export_histogram(self.histogram_iteration_prefixes, labels,
+                               metrics.num_prefixes)
+        self._export_histogram(self.histogram_iteration_tokens, labels,
+                               metrics.num_tokens)
+        if metrics._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            self._export_histogram(self.histogram_iteration_sent_tokens,
+                                   labels, metrics.num_sent_or_recved_tokens)
+        else:
+            self._export_histogram(self.histogram_iteration_received_tokens,
+                                   labels, metrics.num_sent_or_recved_tokens)
+        if metrics._enable_time_measurement:
+            self._export_histogram(self.histogram_iteration_op_lat_ms, labels,
+                                   metrics.op_lat_ms)
+
+
+class AIBrixOffloadingConnectorMetrics(KVTransferMetrics):
+
+    def __init__(self, metrics: KVCacheMetrics) -> None:
+        self._cache_metrics = metrics
+        self._time_measurement_enabled = (
+            self._cache_metrics.time_measurement_enabled)
+        self._send_metrics = AIBrixOffloadingConnectorOpMetrics(
+            AIBrixOffloadingConnectorOpMetrics.OP.SEND,
+            enable_time_measurement=self._time_measurement_enabled,
+        )
+        self._recv_metrics = AIBrixOffloadingConnectorOpMetrics(
+            AIBrixOffloadingConnectorOpMetrics.OP.RECV,
+            enable_time_measurement=self._time_measurement_enabled,
+        )
+
+    @property
+    def time_measurement_enabled(self) -> bool:
+        return self._time_measurement_enabled
+
+    def reset(self) -> None:
+        self._cache_metrics.reset()
+        self._send_metrics.reset()
+        self._recv_metrics.reset()
+
+    def __str__(self) -> str:
+        return f"AIBrixOffloadingConnector metrics: " \
+               f"{self._send_metrics.summary()}" \
+               f"\n\t{self._recv_metrics.summary()}" \
+               f"\n\t{self._cache_metrics.summary()}"
+
+    # NOTE: Functions `log` and `findCaller` are used as a workaround to
+    # log metrics on the worker side, will be removed once the worker is
+    # able to transfer metrics to the scheduler.
+    def log(self, level, msg, *args) -> None:
+        logger.log(level, str(self))
+        self.reset()
+
+    def findCaller(self) -> tuple[str, int, str, str | None]:
+        return logger.findCaller()
+
+
+class AIBrixOffloadingConnectorMetricsExporter(KVTransferMetricsExporter):
+    """Metrics for AIBrixOffloadingConnector."""
+
+    def __init__(self, *, prefix, labelnames, gauge_cls, counter_cls,
+                 histogram_cls):
+        self.kv_cache_metrics_exporter = KVCacheMetricsExporter(
+            prefix=prefix,
+            labelnames=labelnames,
+            gauge_cls=gauge_cls,
+            counter_cls=counter_cls,
+            histogram_cls=histogram_cls,
+        )
+        self.ol_connector_op_metrics_exporter = \
+            AIBrixOffloadingConnectorOpMetricsExporter(
+            prefix=prefix,
+            labelnames=labelnames,
+            gauge_cls=gauge_cls,
+            counter_cls=counter_cls,
+            histogram_cls=histogram_cls,
+        )
+
+    def export(
+        self,
+        *,
+        metrics: KVTransferMetrics,
+        labels: dict[str, str],
+    ):
+        self.kv_cache_metrics_exporter.export(
+            metrics=metrics._cache_metrics,
+            labels=labels,
+        )
+        self.ol_connector_op_metrics_exporter.export(
+            metrics=metrics._send_metrics,
+            labels=labels,
+        )
+        self.ol_connector_op_metrics_exporter.export(
+            metrics=metrics._recv_metrics,
+            labels=labels,
+        )
+
+
+class AIBrixOffloadingConnectorRequestState(enum.IntEnum):
+    INIT = enum.auto()
+    WAITING_FOR_ALLOC = enum.auto()
+    WAITING_FOR_SEND = enum.auto()
+    WAITING_FOR_RECV = enum.auto()
+    SENDING = enum.auto()
+    RECEIVING = enum.auto()
+
+
+@dataclasses.dataclass
+class AIBrixOffloadingConnectorRequestMetadata(CachedPyObjectBase):
+    req_id: str = ""
+    prompt_len: int = -1
+    context_len: int = -1
+    query_len: int = -1  # num of tokens to send/recv
+    seq_token_ids: list[int] = dataclasses.field(default_factory=list)
+    seq_token_ids_view: Optional[TokenListView] = None
+    seq_slot_mapping: Optional[torch.Tensor] = None
+    state: AIBrixOffloadingConnectorRequestState = (
+        AIBrixOffloadingConnectorRequestState.INIT)
+
+    def __str__(self) -> str:
+        return (f"AIBrixOffloadingConnectorRequestMetadata["
+                f"req_id={self.req_id}, "
+                f"prompt_len={self.prompt_len}, "
+                f"context_len={self.context_len}, "
+                f"query_len={self.query_len}, "
+                f"len(seq_token_ids)={len(self.seq_token_ids)}, "
+                f"seq_slot_mapping.shape={self.seq_slot_mapping.shape}, "
+                f"state={self.state.name}]")
+
+    def __repr__(self):
+        return self.__str__()
+
+
+class AIBrixOffloadingConnectorMetadata(KVConnectorMetadata):
+
+    def __init__(self,
+                 requests: dict[str,
+                                AIBrixOffloadingConnectorRequestMetadata]):
+        # Requests that need to load from external kvcache.
+        self.requests = requests
+
+    def __getitem__(self,
+                    key: str) -> AIBrixOffloadingConnectorRequestMetadata:
+        return self.requests[key]
+
+    def __contains__(self, key: str) -> bool:
+        return key in self.requests
+
+    def __iter__(self):
+        return iter(self.requests)
+
+    def __next__(self):
+        return next(self.requests)
+
+    def __len__(self):
+        return len(self.requests)
+
+    def items(self):
+        return self.requests.items()
+
+    def upsert_request(
+        self,
+        request_id: str,
+        **kwargs,
+    ) -> None:
+        self.requests.setdefault(
+            request_id,
+            AIBrixOffloadingConnectorRequestMetadata()).__dict__.update(
+                req_id=request_id, **kwargs)
+
+    def pop_request(
+        self,
+        request_id: str,
+    ) -> AIBrixOffloadingConnectorRequestMetadata | None:
+        return self.requests.pop(request_id, None)
+
+    def get(self, predicate: Callable) -> "AIBrixOffloadingConnectorMetadata":
+        requests = {k: v for k, v in self.requests.items() if predicate(v)}
+        return AIBrixOffloadingConnectorMetadata(requests=requests)
+
+    def filter_requests(self, predicate: Callable) -> None:
+        self.requests = {
+            k: v
+            for k, v in self.requests.items() if not predicate(v)
+        }
+
+    def extend(self, other: "AIBrixOffloadingConnectorMetadata") -> None:
+        self.requests.update(other.requests)
+
+    def clear(self) -> None:
+        self.requests.clear()
+
+
+class AIBrixOffloadingConnectorScheduler:
+
+    def __init__(self, config: "VllmConfig"):
+        self.kv_role = config.kv_transfer_config.kv_role
+        self.block_ntokens = config.cache_config.block_size
+
+        self._scheduler_meta = AIBrixOffloadingConnectorMetadata({})
+
+    def build_connector_meta(
+            self, scheduler_output: "SchedulerOutput") -> KVConnectorMetadata:
+        # 1. remove finished requests
+        for req_id in scheduler_output.finished_req_ids:
+            self._scheduler_meta.pop_request(req_id)
+
+        # 2. new requests
+        for req in scheduler_output.scheduled_new_reqs:
+            req_id = req.req_id
+
+            prompt_len = len(req.prompt_token_ids)
+            context_len = req.num_computed_tokens
+            query_len = scheduler_output.num_scheduled_tokens[req_id]
+            seq_len = min(context_len + query_len, prompt_len)
+
+            if context_len >= prompt_len:
+                continue
+
+            (block_ids, ) = req.block_ids
+            slot_mapping = self._block_ids_to_slot_mapping(block_ids)
+
+            self._scheduler_meta.upsert_request(
+                req_id,
+                prompt_len=prompt_len,
+                context_len=context_len,
+                query_len=query_len,
+                seq_token_ids=req.prompt_token_ids[:seq_len],
+                seq_slot_mapping=slot_mapping,
+                state=AIBrixOffloadingConnectorRequestState.WAITING_FOR_RECV,
+            )
+
+        # 3. cached requests
+        for req in scheduler_output.scheduled_cached_reqs:
+            req_id = req.req_id
+
+            if req_id not in self._scheduler_meta:
+                continue
+
+            req_meta = self._scheduler_meta[req_id]
+
+            prompt_len = req_meta.prompt_len
+            context_len = min(req.num_computed_tokens, prompt_len)
+            query_len = min(
+                scheduler_output.num_scheduled_tokens[req_id],
+                prompt_len - context_len,
+            )
+            seq_len = min(context_len + query_len, prompt_len)
+
+            if context_len >= prompt_len:
+                continue
+
+            (block_ids, ) = req.new_block_ids
+            new_slot_mapping = self._block_ids_to_slot_mapping(block_ids)
+
+            if req.resumed_from_preemption:
+                logger.debug(
+                    "Got preempt Request[id=%s, context_len=%d, query_len=%d]",
+                    req_id,
+                    context_len,
+                    query_len,
+                )
+                seq_token_ids = req.new_token_ids
+                seq_slot_mapping = new_slot_mapping
+            else:
+                seq_token_ids = req_meta.seq_token_ids + req.new_token_ids
+                if new_slot_mapping.shape[0] > 0:
+                    seq_slot_mapping = torch.cat(
+                        [req_meta.seq_slot_mapping, new_slot_mapping])
+                else:
+                    seq_slot_mapping = req_meta.seq_slot_mapping
+
+            self._scheduler_meta.upsert_request(
+                req_id,
+                prompt_len=prompt_len,
+                context_len=context_len,
+                query_len=query_len,
+                seq_token_ids=seq_token_ids[:seq_len],
+                seq_slot_mapping=seq_slot_mapping,
+                state=AIBrixOffloadingConnectorRequestState.WAITING_FOR_RECV,
+            )
+
+        # 4. keep requests that are in the WAITING_FOR_RECV state
+        meta = copy.deepcopy(
+            self._scheduler_meta.get(lambda req: req.state == \
+                AIBrixOffloadingConnectorRequestState.WAITING_FOR_RECV
+            ))
+
+        logger.debug("SCHEDULER: build_connector_meta, meta=%s", meta.__dict__)
+
+        # 5. update scheduled requests
+        for req_id in meta:
+            self._scheduler_meta.upsert_request(
+                req_id,
+                state=AIBrixOffloadingConnectorRequestState.RECEIVING,
+            )
+
+        logger.debug(
+            "Num. of scheduled requests: %s",
+            len(
+                self._scheduler_meta.get(lambda req: req.state == \
+                    AIBrixOffloadingConnectorRequestState.RECEIVING
+                )),
+        )
+        return meta
+
+    def request_finished(
+        self,
+        request: "Request",
+        block_ids: list[int],
+    ) -> tuple[bool, Optional[dict[str, Any]]]:
+        req_id = request.request_id
+        logger.debug("SCHEDULER: Request[id=%s] finished", req_id)
+
+        if req_id not in self._scheduler_meta:
+            return False, None
+
+        # use sync sending
+        self._scheduler_meta.pop_request(req_id)
+        return False, None
+
+    def _block_ids_to_slot_mapping(self, block_ids: list[int]) -> torch.Tensor:
+        block_ids_tensor = torch.tensor(block_ids)
+        num_blocks = block_ids_tensor.shape[0]
+        block_offsets = torch.arange(0, self.block_ntokens)
+        slot_mapping = block_offsets.reshape((1, self.block_ntokens)) + \
+                block_ids_tensor.reshape((num_blocks, 1)) * self.block_ntokens
+        return slot_mapping.flatten()
+
+
+class AIBrixOffloadingConnectorWorker:
+    """AIBrixOffloadingConnectorWorker carries out the data-plane operations.
+    """
+
+    def __init__(self, config: "VllmConfig"):
+
+        cache_config = config.cache_config
+        model_config = config.model_config
+        parallel_config = config.parallel_config
+
+        tp_size = parallel_config.tensor_parallel_size
+        num_kv_heads = model_config.get_num_kv_heads(parallel_config)
+        hidden_size = model_config.get_hidden_size()
+        num_attention_heads = model_config.get_num_attention_heads(
+            parallel_config) * tp_size
+        head_size = int(hidden_size / num_attention_heads)
+
+        rank = get_tp_group().rank_in_group
+        kv_head_ids = list(
+            range(num_kv_heads * rank, num_kv_heads * (rank + 1)))
+        layer_ids = list(
+            range(*model_config.get_layers_start_end_indices(parallel_config)))
+        num_layers = len(layer_ids)
+
+        block_ntokens = cache_config.block_size
+        block_dtype = get_kv_cache_torch_dtype(cache_config.cache_dtype,
+                                               model_config.dtype)
+
+        kv_cache_dtype = cache_config.cache_dtype
+
+        self.attn_backend = get_attn_backend(
+            model_config.get_head_size(),
+            model_config.dtype,
+            kv_cache_dtype,
+            block_ntokens,
+            model_config.is_attention_free,
+            use_mla=model_config.use_mla,
+        )
+
+        block_spec = KVCacheBlockSpec(
+            block_ntokens=block_ntokens,
+            block_dtype=block_dtype,
+            block_layout=self._get_block_layout(),
+            tensor_spec=KVCacheTensorSpec(
+                heads=kv_head_ids,
+                layers=layer_ids,
+                head_size=head_size,
+            ),
+        )
+
+        kv_config = KVCacheConfig(block_spec=block_spec,
+                                  model_spec=ModelSpec(
+                                      model_config.max_model_len))
+
+        if parallel_config.tensor_parallel_size == 1:
+            self.cache = BaseKVCacheManager(config=kv_config)
+        else:
+            backend = torch.distributed.get_backend(get_world_group()\
+                .device_group)
+            world_size = parallel_config.world_size
+            dp_size = parallel_config.data_parallel_size
+            pp_size = parallel_config.pipeline_parallel_size
+            # the layout order is: ExternalDP x DP x PP x TP
+            # ExternalDP is the data parallel group that is not part of the
+            # model, every dp rank can generate independently (in verl
+            # integration).
+            # DP is the data parallel group that is part of the model,
+            # all the ranks in the same DP group should generate simultaneously,
+            # i.e. the `generate` call in the same DP group should be called
+            # together, otherwise it will cause deadlock.
+            # to get group_ranks for each dimension, transpose that dimension to
+            # the last dimension, then reshape to 2D, then unbind the last
+            # dimension
+            all_ranks = torch.arange(world_size).reshape(
+                -1, dp_size, pp_size, tp_size)
+
+            # Build the kv model-parallel groups.
+            group_ranks = all_ranks.view(-1, tp_size).unbind(0)
+            group_ranks = [x.tolist() for x in group_ranks]
+
+            kv_group = init_model_parallel_group(
+                group_ranks,
+                get_world_group().local_rank,
+                backend,
+                group_name="kvcache",
+            )
+            assert rank == kv_group.rank_in_group
+            self.cache = GroupAwareKVCacheManager(
+                config=kv_config, process_group=kv_group.cpu_group)
+
+        self.rank = rank
+        self.head_size = head_size
+        self.tp_size = tp_size
+        self.num_kv_heads = num_kv_heads
+        self.num_layers = num_layers
+        self.kv_head_ids = kv_head_ids
+        self.layer_ids = layer_ids
+        self.block_ntokens = block_ntokens
+        self.block_dtype = block_dtype
+        self.block_shape = block_spec.block_shape
+        self.block_spec = block_spec
+        self.block_layout = block_spec.block_layout
+        self.chunk_size = self.cache.chunk_size
+        self.cache_feature = self.cache.feature
+        self.kv_cache_dtype = kv_cache_dtype
+
+        # KV caches and kv scales will be init'ed later
+        self.no_compile_layers = config.compilation_config.\
+            static_forward_context
+        self.kv_caches: dict[str, torch.Tensor] | None = None
+        self.layers_kv_caches: list[torch.Tensor] | None = None
+        self.k_scales: list[torch.Tensor] | None = None
+        self.v_scales: list[torch.Tensor] | None = None
+
+        # metrics
+        self._metrics = AIBrixOffloadingConnectorMetrics(self.cache.metrics)
+
+    def __del__(self) -> None:
+        if getattr(self, "cache", None) is not None:
+            self.cache.close()
+            self.cache = None
+
+    def _get_block_layout(self) -> KVCacheBlockLayout:
+
+        if self.attn_backend.get_name() in \
+            OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS:
+            return KVCacheBlockLayout(
+                OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS[
+                    self.attn_backend.get_name()])
+        raise NotImplementedError(
+            f"Only support attn backends in "
+            f"{list(OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS.keys())}. "
+            f"{self.attn_backend.get_name()} is used.")
+
+    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]) -> None:
+        self.kv_caches = kv_caches
+        layer_names = self.no_compile_layers.keys()
+        self.layers_kv_caches = [
+            self.kv_caches[layer_name] for layer_name in layer_names
+        ]
+        layers = self.no_compile_layers.values()
+        self.k_scales = [layer._k_scale for layer in layers]
+        self.v_scales = [layer._v_scale for layer in layers]
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnectorV1Type1",
+        "func": "start_load_kv_before_update"
+    })
+    def start_load_kv_before_update(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+    ) -> dict[str, int]:
+        stats = {}
+        for seq_request_id, seq_request_meta in metadata.items():
+            num_fetched_tokens = self._recv_kv_sync_impl(seq_request_meta)
+            if num_fetched_tokens > 0:
+                stats[seq_request_id] = num_fetched_tokens
+                # update seq_request_meta
+                seq_request_meta.query_len -= num_fetched_tokens
+                seq_request_meta.context_len += num_fetched_tokens
+
+            seq_request_meta.state = \
+                AIBrixOffloadingConnectorRequestState.WAITING_FOR_SEND
+
+        return stats
+
+    def _recv_kv_sync_impl(
+        self,
+        seq_request_meta: AIBrixOffloadingConnectorRequestMetadata,
+    ) -> int:
+        logger.debug("_recv_kv_sync_impl: %s", seq_request_meta)
+        seq_request_id = seq_request_meta.req_id
+        seq_context_len = seq_request_meta.context_len
+        if seq_request_meta.seq_token_ids_view is None:
+            seq_request_meta.seq_token_ids_view = TokenListView(
+                seq_request_meta.seq_token_ids)
+        seq_all_tokens = seq_request_meta.seq_token_ids_view
+        assert seq_all_tokens is not None, "seq_all_tokens is None"
+
+        prompt_len = seq_request_meta.prompt_len
+        query_len = seq_request_meta.query_len
+
+        seq_slot_mapping = seq_request_meta.seq_slot_mapping.cuda(
+            non_blocking=True)
+
+        # align to block boundary
+        aligned_context_len = round_down(seq_context_len, self.block_ntokens)
+        actual_query_len = seq_context_len + query_len - aligned_context_len
+        aligned_query_len = round_down(actual_query_len, self.block_ntokens)
+        shift_len = seq_context_len - aligned_context_len
+
+        assert prompt_len >= aligned_context_len + aligned_query_len, \
+            f"{prompt_len}<{aligned_context_len}+{aligned_query_len}"
+
+        if aligned_query_len < OFFLOADING_CONNECTOR_SKIP_THRESHOLD * \
+                self.block_ntokens:
+            logger.debug(
+                "Skip Request[id=%s, context_len=%d, query_len=%d]",
+                seq_request_id,
+                aligned_context_len,
+                aligned_query_len,
+            )
+            return 0
+
+        prefix = seq_all_tokens[:aligned_context_len]
+        tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                aligned_query_len]
+
+        if self._metrics.time_measurement_enabled:
+            start = torch.cuda.Event(enable_timing=True)
+            end = torch.cuda.Event(enable_timing=True)
+            start.record()
+
+        seq_recv_len = 0
+        for (
+                chunk_prefix,
+                chunk_tokens,
+                next_tokens,
+                _,
+        ) in self.cache.cache_chunk_keys(prefix, tokens):
+            if next_tokens and len(next_tokens) > 0:
+                # prefetch
+                self.cache.prefetch(chunk_prefix + chunk_tokens, next_tokens)
+
+            # get KV caches from offloading service
+            status = self.cache.acquire(chunk_prefix, chunk_tokens)
+
+            if not status.is_ok():
+                if not status.is_not_found():
+                    log_every_n_seconds(
+                        logger,
+                        logging.ERROR,
+                        "Failed to get from offloading service: %s",
+                        3,
+                        str(status),
+                    )
+                break
+
+            num_fetched_tokens, handle = status.value
+            kv_blocks = handle.to_tensors()
+
+            offset = len(chunk_prefix)
+            length = num_fetched_tokens
+
+            chunk_slot_mapping = seq_slot_mapping[offset:offset + length]
+
+            with perf_timer() as get_kernel_onload_dur_ms:
+                reshape_and_cache_multi_layer(
+                    kv_blocks,
+                    self.layers_kv_caches,
+                    chunk_slot_mapping,
+                    self.block_ntokens,
+                    self.kv_cache_dtype,
+                    self.k_scales,
+                    self.v_scales,
+                    self.block_layout.name,
+                )
+
+            logger.info(
+                "Request[id=%s] onloads %d tokens in %.4f ms",
+                seq_request_id,
+                length,
+                get_kernel_onload_dur_ms(),
+            )
+
+            # update recv_len
+            seq_recv_len += num_fetched_tokens - shift_len
+            # reset shift_len
+            shift_len = 0
+
+            # release handle
+            handle.release()
+
+            if num_fetched_tokens < len(chunk_tokens):
+                # didn't receive all tokens for current chunk, break
+                break
+
+        log_if(
+            logger,
+            logging.INFO,
+            "Request[id=%s, prompt_len=%d, context_len=%d] reused %d tokens",
+            seq_recv_len > 0,
+            seq_request_id,
+            prompt_len,
+            seq_context_len,
+            seq_recv_len,
+        )
+
+        if self._metrics.time_measurement_enabled:
+            end.record()
+            end.synchronize()
+            lat_ms = start.elapsed_time(end)
+            self._metrics._recv_metrics.add(aligned_context_len,
+                                            aligned_query_len, seq_recv_len,
+                                            lat_ms)
+
+        return seq_recv_len
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnectorV1Type1",
+        "func": "wait_for_save"
+    })
+    def wait_for_save(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+    ) -> None:
+        assert self.layers_kv_caches is not None, "layers_kv_caches is None"
+
+        for seq_request_id, seq_request_meta in metadata.items():
+            if seq_request_meta.query_len == 0:
+                continue
+            self._send_kv_sync_impl(seq_request_meta)
+
+        if self._metrics.time_measurement_enabled:
+            log_every_n_seconds(self._metrics, logging.INFO, "UNUSED", 10)
+
+    def _send_kv_sync_impl(
+        self,
+        seq_request_meta: AIBrixOffloadingConnectorRequestMetadata,
+    ) -> None:
+        logger.debug("_send_kv_sync_impl: %s", seq_request_meta)
+        seq_request_id = seq_request_meta.req_id
+        seq_context_len = seq_request_meta.context_len
+        seq_slot_mapping = seq_request_meta.seq_slot_mapping.cuda(
+            non_blocking=True)
+        seq_all_tokens = seq_request_meta.seq_token_ids_view
+        assert seq_all_tokens is not None, "seq_all_tokens is None"
+        prompt_len = seq_request_meta.prompt_len
+        query_len = seq_request_meta.query_len
+
+        # align to block boundary
+        aligned_context_len = round_down(seq_context_len, self.block_ntokens)
+        actual_query_len = seq_context_len + query_len - aligned_context_len
+        aligned_query_len = round_down(actual_query_len, self.block_ntokens)
+
+        assert prompt_len >= aligned_context_len + aligned_query_len, \
+            f"{prompt_len}<{aligned_context_len}+{aligned_query_len}"
+
+        prefix = seq_all_tokens[:aligned_context_len]
+        tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                aligned_query_len]
+
+        if self._metrics.time_measurement_enabled:
+            start = torch.cuda.Event(enable_timing=True)
+            end = torch.cuda.Event(enable_timing=True)
+            start.record()
+
+        total_sent = 0
+        for (
+                chunk_prefix,
+                chunk_tokens,
+                _,
+                all,
+        ) in self.cache.cache_chunk_keys(prefix, tokens):
+            # |----------- seq_context_len ---------|- offset -|
+            # |- aligned_context_len -|- shift_len -|
+            #                         |--- n * chunk_size -----|
+            # |----------------- chunk_prefix -----------------|- tokens -|
+            #                         |-------- (n + 1) * chunk_size -----|
+            chunk_size = len(chunk_tokens)
+            offset = len(chunk_prefix)
+            length = chunk_size
+
+            exists_status = self.cache.exists(chunk_prefix, chunk_tokens)
+            if exists_status.is_ok():
+                num_existing_tokens = exists_status.value
+                logger.info(
+                    "Request[id=%s] send(%d) encounters %d existing tokens",
+                    seq_request_id, length, num_existing_tokens)
+                if chunk_size - num_existing_tokens < self.block_ntokens:
+                    continue
+                else:
+                    # partially exists
+                    offset += num_existing_tokens
+                    length -= num_existing_tokens
+                    new_chunk_prefix_len = len(
+                        chunk_prefix) + num_existing_tokens
+                    chunk_prefix = all[:new_chunk_prefix_len]
+                    chunk_tokens = all[
+                        new_chunk_prefix_len:new_chunk_prefix_len + length]
+
+            # allocate space for KV caches
+            status = self.cache.allocate_for(chunk_prefix, chunk_tokens)
+            if not status.is_ok():
+                log_every_n_seconds(logger, logging.ERROR,
+                                    "Failed to allocate : %s", 3, str(status))
+                break
+            handle = status.value
+            tensors = handle.to_tensors()
+            length = len(tensors) * self.block_ntokens
+
+            chunk_slot_mapping = seq_slot_mapping[offset:offset + length]
+
+            with perf_timer() as get_kernel_offload_dur_ms:
+                reshape_and_offload_multi_layer(
+                    tensors,
+                    self.layers_kv_caches,
+                    chunk_slot_mapping,
+                    self.block_ntokens,
+                    self.kv_cache_dtype,
+                    self.k_scales,
+                    self.v_scales,
+                    self.block_layout.name,
+                )
+
+            logger.info("Request[id=%s] offloads %d tokens in %.4f ms",
+                        seq_request_id, length, get_kernel_offload_dur_ms())
+
+            # put KV caches to offloading service
+            status = self.cache.put(chunk_prefix, chunk_tokens[:length],
+                                    handle)
+            if not status.is_ok():
+                # TODO: notify other ranks in the group to stop sending
+                # if this is a fatal error
+                log_every_n_seconds(logger, logging.ERROR,
+                                    "Failed to put to offloading service: %s",
+                                    3, str(status))
+                break
+            else:
+                total_sent += length
+
+        log_if(
+            logger,
+            logging.INFO,
+            "Request[id=%s, prompt_len=%d, context_len=%d] sent %d tokens",
+            total_sent > 0,
+            seq_request_id,
+            prompt_len,
+            seq_context_len,
+            total_sent,
+        )
+
+        if self._metrics.time_measurement_enabled:
+            end.record()
+            end.synchronize()
+            lat_ms = start.elapsed_time(end)
+            self._metrics._send_metrics.add(aligned_context_len,
+                                            aligned_query_len, total_sent,
+                                            lat_ms)
+
+
+class AIBrixOffloadingConnector(KVConnectorBase_V1):
+    """AIBrixOffloadingConnector is a KVConnector that offloads KV caches
+    to the kv cache offloading service.
+    """
+
+    def __init__(self, config: "VllmConfig", role: KVConnectorRole):
+        super().__init__(vllm_config=config, role=role)
+
+        self.connector_scheduler: Optional[
+            AIBrixOffloadingConnectorScheduler] = None
+
+        self.connector_worker: Optional[AIBrixOffloadingConnectorWorker] = None
+        if role == KVConnectorRole.SCHEDULER:
+            self.connector_scheduler = AIBrixOffloadingConnectorScheduler(
+                config)
+        elif role == KVConnectorRole.WORKER:
+            self.connector_worker = AIBrixOffloadingConnectorWorker(config)
+
+    @delegate_to("connector_worker")
+    @property
+    def metrics(self) -> 'KVTransferMetrics':
+        """
+        Get the metrics object associated with the connector.
+        Returns:
+            KVTransferMetrics: The metrics object.
+        """
+        pass
+
+    @delegate_to("connector_worker")
+    def get_metrics_exporter_cls(self) -> 'KVTransferMetricsExporter':
+        """
+        Get the metrics exporter class associated with the connector.
+        """
+        pass
+
+    # ==============================
+    # Worker-side methods
+    # ==============================
+
+    @delegate_to("connector_worker")
+    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):
+        """
+        Initialize with the KV caches. Useful for pre-registering the
+        KV Caches in the KVConnector (e.g. for NIXL).
+
+        Args: kv_caches:
+            dictionary of layer names, kv cache
+        """
+        pass
+
+    def start_load_kv_before_update(self, **kwargs) -> dict[str, int]:
+        """
+        Start loading the KV cache from the connector to vLLM's paged
+        KV buffer before gpu runner updating its states.
+
+        Args:
+            **kwargs: additional arguments for the load operation
+
+        Returns:
+            dict[str, int]: a dictionary of request ids and the number of
+            tokens loaded for each request.
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        return self.connector_worker.start_load_kv_before_update(
+            self._connector_metadata)
+
+    def start_load_kv(self, forward_context: "ForwardContext",
+                      **kwargs) -> None:
+        """
+        Start loading the KV cache from the connector to vLLM's paged
+        KV buffer. This is called from the forward context before the
+        forward pass to enable async loading during model execution.
+
+        Args:
+            forward_context (ForwardContext): the forward context.
+            **kwargs: additional arguments for the load operation
+
+        Note:
+            The number of elements in kv_caches and layer_names should be 
+            the same.
+            
+        """
+        return
+
+    def wait_for_layer_load(self, layer_name: str) -> None:
+        """
+        Block until the KV for a specific layer is loaded into vLLM's
+        paged buffer. This is called from within attention layer to ensure
+        async copying from start_load_kv is complete.
+        
+        This interface will be useful for layer-by-layer pipelining.
+
+        Args:
+            layer_name: the name of that layer
+        """
+        """Not supported yet"""
+        pass
+
+    def save_kv_layer(self, layer_name: str, kv_layer: torch.Tensor,
+                      attn_metadata: "AttentionMetadata", **kwargs) -> None:
+        """
+        Start saving a layer of KV cache from vLLM's paged buffer 
+        to the connector. This is called from within attention layer to
+        enable async copying during execution.
+
+        Args:
+            layer_name (str): the name of the layer.
+            kv_layer (torch.Tensor): the paged KV buffer of the current 
+                layer in vLLM.
+            attn_metadata (AttentionMetadata): the attention metadata.
+            **kwargs: additional arguments for the save operation.
+        """
+        """Not supported yet"""
+        pass
+
+    def wait_for_save(self) -> None:
+        """
+        Block until all the save operations is done. This is called
+        as the forward context exits to ensure that the async saving
+        from save_kv_layer is complete before finishing the forward.
+
+        This prevents overwrites of paged KV buffer before saving done.
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        self.connector_worker.wait_for_save(self._connector_metadata)
+
+    def get_finished(
+        self, finished_req_ids: set[str]
+    ) -> tuple[Optional[set[str]], Optional[set[str | tuple[str, int]]]]:
+        """
+        Notifies worker-side connector ids of requests that have
+        finished generating tokens.
+
+        Returns:
+            ids of requests that have finished asynchronous transfer
+            (requests that previously returned True from request_finished()),
+            tuple of (sending/saving ids, recving/loading ids or
+            (recving/loading id, num. of recv'ed/loaded tokens) pairs).
+            The finished saves/sends req ids must belong to a set provided in a
+            call to this method (this call or a prior one).
+        """
+        return None, None
+
+    # ==============================
+    # Scheduler-side methods
+    # ==============================
+
+    def get_num_new_matched_tokens(
+        self,
+        request: "Request",
+        num_computed_tokens: int,
+    ) -> tuple[int, bool]:
+        """
+        Get number of new tokens that can be loaded from the
+        external KV cache beyond the num_computed_tokens.
+        
+        Args:
+            request (Request): the request object.
+            num_computed_tokens (int): the number of locally
+                computed tokens for this request
+
+        Returns:
+            A tuple with the following elements:
+                - The number of tokens that can be loaded from the 
+                  external KV cache beyond what is already computed.
+                - `True` if external KV cache tokens will be loaded
+                  asynchronously (between scheduler steps). Must be
+                  'False' if the first element is 0.
+        """
+        return 0, False
+
+    def update_state_after_alloc(self, request: "Request",
+                                 blocks: "KVCacheBlocks",
+                                 num_external_tokens: int):
+        """
+        Update KVConnector state after block allocation.
+
+        If get_num_new_matched_tokens previously returned True for a
+        request, this function may be called twice for that same request -
+        first when blocks are allocated for the connector tokens to be
+        asynchronously loaded into, and second when any additional blocks
+        are allocated, after the load/transfer is complete.
+
+        Args:
+            request (Request): the request object.
+            blocks (KVCacheBlocks): the blocks allocated for the request.
+            num_external_tokens (int): the number of tokens that will be
+                loaded from the external KV cache.
+        """
+        return
+
+    @delegate_to("connector_scheduler")
+    def build_connector_meta(
+            self, scheduler_output: "SchedulerOutput") -> KVConnectorMetadata:
+        """
+        Build the connector metadata for this step.
+
+        This function should NOT modify fields in the scheduler_output.
+        Also, calling this function will reset the state of the connector.
+
+        Args:
+            scheduler_output (SchedulerOutput): the scheduler output object.
+        """
+        pass
+
+    @delegate_to("connector_scheduler")
+    def request_finished(
+        self,
+        request: "Request",
+        block_ids: list[int],
+    ) -> tuple[bool, Optional[dict[str, Any]]]:
+        """
+        Called when a request has finished, before its blocks are freed.
+
+        Returns:
+            True if the request is being saved/sent asynchronously and blocks
+            should not be freed until the request_id is returned from
+            get_finished().
+            Optional KVTransferParams to be included in the request outputs
+            returned by the engine.
+        """
+        pass
diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/base.py b/vllm/distributed/kv_transfer/kv_connector/v1/base.py
index f80b5eba2..7b3703525 100644
--- a/vllm/distributed/kv_transfer/kv_connector/v1/base.py
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/base.py
@@ -124,6 +124,20 @@ class KVConnectorBase_V1(ABC):
         """
         return
 
+    def start_load_kv_before_update(self, **kwargs) -> dict[str, int]:
+        """
+        Start loading the KV cache from the connector to vLLM's paged
+        KV buffer before gpu runner updating its states.
+
+        Args:
+            **kwargs: additional arguments for the load operation
+
+        Returns:
+            dict[str, int]: a dictionary of request ids and the number of
+            tokens loaded for each request.
+        """
+        return {}
+
     @abstractmethod
     def start_load_kv(self, forward_context: "ForwardContext",
                       **kwargs) -> None:
diff --git a/vllm/distributed/kv_transfer/kv_connector_agent.py b/vllm/distributed/kv_transfer/kv_connector_agent.py
index 8633fdaf5..13f5b544f 100644
--- a/vllm/distributed/kv_transfer/kv_connector_agent.py
+++ b/vllm/distributed/kv_transfer/kv_connector_agent.py
@@ -9,6 +9,8 @@ This implementation is a shim wrapper on two APIs exposed by `kv_connector`:
 from typing import TYPE_CHECKING, Union
 
 if TYPE_CHECKING:
+    from .kv_transfer_metrics import (KVTransferMetrics,
+                                      KVTransferMetricsExporter)
     from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
     from vllm.config import VllmConfig
 
@@ -50,6 +52,23 @@ class KVTransferAgent:
         self.connector = KVConnectorFactory.create_connector_v0(
             rank, local_rank, config)
 
+    @property
+    def metrics(self) -> "KVTransferMetrics":
+        """
+        Get the metrics object associated with the connector.
+        Returns:
+            KVTransferMetrics: The metrics object.
+        """
+        return self.connector.metrics
+
+    def get_metrics_exporter_cls(self) -> "KVTransferMetricsExporter":
+        """
+        Get the metrics object associated with the connector.
+        Returns:
+            KVTransferMetrics: The metrics object.
+        """
+        return self.connector.get_metrics_exporter_cls()
+
     def send_kv_caches_and_hidden_states(
         self,
         model_executable: torch.nn.Module,
diff --git a/vllm/distributed/kv_transfer/kv_transfer_metadata.py b/vllm/distributed/kv_transfer/kv_transfer_metadata.py
new file mode 100644
index 000000000..95c129633
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_transfer_metadata.py
@@ -0,0 +1,119 @@
+# SPDX-License-Identifier: Apache-2.0
+
+from dataclasses import dataclass
+from typing import TYPE_CHECKING, List, Optional
+
+from vllm.sequence import SequenceGroupMetadata
+from vllm.utils import PyObjectCache
+
+if TYPE_CHECKING:
+    from vllm.worker.model_runner import GPUModelRunnerBase
+
+
+@dataclass
+class SequenceGroupToKVTransfer:
+    """
+    Metadata mainly used by kv_both scenarios.
+    """
+    prompt_len: Optional[int]
+    # Context tokens in prefix cache
+    context_tokens: Optional[List[int]]
+
+
+def seq_group_to_kv_transfer_builder():
+    return SequenceGroupToKVTransfer(
+        prompt_len=0,
+        context_tokens=None,
+    )
+
+
+class KVTransferMetadataCache:
+    """Used to cache SequenceGroupToKVTransfer objects between
+    scheduler iterations.
+    """
+
+    def __init__(self):
+        self._seq_group_to_kv_transfer_cache: PyObjectCache = PyObjectCache(
+            seq_group_to_kv_transfer_builder)
+
+    def get_cached_seq_group_to_kv_transfer(self):
+        obj = self._seq_group_to_kv_transfer_cache.get_object()
+        obj.prompt_len = 0
+        obj.context_tokens = None
+        return obj
+
+    def reset(self):
+        self._seq_group_to_kv_transfer_cache.reset()
+
+
+class KVTransferMetadata:
+    """Metadata for input sequences. Used in KV transfer.
+
+    Args:
+        seq_groups: List of batched sequence groups.
+        model_input_builder: Model input builder for rebuilding model input.
+    """
+
+    def __init__(
+        self,
+        seq_groups: List[SequenceGroupToKVTransfer],
+    ) -> None:
+        self.seq_groups = seq_groups
+        # only driver has seq_group_metadata_list and runner
+        self.seq_group_metadata_list: Optional[
+            List[SequenceGroupMetadata]] = None
+        self.runner: Optional[GPUModelRunnerBase] = None
+
+    @staticmethod
+    def prepare(
+        seq_group_metadata_list: List[SequenceGroupMetadata],
+        cache: Optional[KVTransferMetadataCache] = None,
+    ) -> "KVTransferMetadata":
+        """
+        context_lens include num of tokens in prefix cache.
+        """
+        seq_groups: List[SequenceGroupToKVTransfer] = []
+        ctx_idx = 0
+        for seq_group_metadata in seq_group_metadata_list:
+            seq_ids = list(seq_group_metadata.seq_data.keys())
+            for i in range(len(seq_ids)):
+                if cache is not None:
+                    seq_group_obj = cache.get_cached_seq_group_to_kv_transfer()
+                else:
+                    seq_group_obj = seq_group_to_kv_transfer_builder()
+
+                seq_id = seq_ids[i]
+                seq_data = seq_group_metadata.seq_data[seq_id]
+
+                seq_group_obj.prompt_len = seq_data.get_prompt_len()
+
+                seq_groups.append(seq_group_obj)
+
+            # prefill sequence group only has one sequence
+            if seq_group_metadata.is_prompt:
+                seq_id = seq_ids[0]
+                nblocks = len(seq_group_metadata.computed_block_nums or [])
+                seq_data = seq_group_metadata.seq_data[seq_id]
+                context_len = seq_data.get_num_cached_tokens()
+                # prefix caching:
+                #     context_len > 0 and nblocks > 0
+                # chunked prefill intermediate steps:
+                #     context_len > 0 and nblocks == 0
+                #
+                # We only carry context_tokens for prefix caching since
+                # chunked prefill intermediate steps can use the cached
+                # context tokens in the offloading connector.
+                if context_len > 0 and nblocks > 0:
+                    seq_groups[-1].context_tokens = seq_data.get_token_ids(
+                    )[:context_len]
+
+            ctx_idx += len(seq_ids)
+
+        if cache is not None:
+            cache.reset()
+
+        metadata = KVTransferMetadata(seq_groups=seq_groups)
+        return metadata
+
+    def __repr__(self) -> str:
+        return f"KVTransferMetadata(seq_groups={self.seq_groups})"
diff --git a/vllm/distributed/kv_transfer/kv_transfer_metrics.py b/vllm/distributed/kv_transfer/kv_transfer_metrics.py
new file mode 100644
index 000000000..e92fc8935
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_transfer_metrics.py
@@ -0,0 +1,48 @@
+# SPDX-License-Identifier: Apache-2.0
+from abc import ABC, abstractmethod
+from typing import Dict
+
+
+class KVTransferMetrics(ABC):
+    """
+    KVTransferMetrics is used to collect metrics for KVTransferAgent.
+    """
+
+    @abstractmethod
+    def reset(self) -> None:
+        raise NotImplementedError
+
+    @abstractmethod
+    def __str__(self) -> str:
+        raise NotImplementedError
+
+
+class KVTransferMetricsExporter(ABC):
+    """
+    KVTransferMetrics is used to collect metrics for KVTransferAgent.
+    """
+
+    def __init__(
+        self,
+        *,
+        prefix,
+        labelnames,
+        gauge_cls,
+        counter_cls,
+        histogram_cls,
+    ):
+        self._prefix = prefix
+        self._labelnames = labelnames
+        self._gauge_cls = gauge_cls
+        self._counter_cls = counter_cls
+        self._histogram_cls = histogram_cls
+
+    @abstractmethod
+    def export(
+        self,
+        *,
+        metrics: KVTransferMetrics,
+        labels: Dict[str, str],
+    ) -> None:
+        """Export metrics to external systems."""
+        raise NotImplementedError
diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py
index 8fccf9bd2..f43c2e054 100644
--- a/vllm/engine/llm_engine.py
+++ b/vllm/engine/llm_engine.py
@@ -21,6 +21,7 @@ from vllm.config import (DecodingConfig, LoRAConfig, ModelConfig,
                          ObservabilityConfig, ParallelConfig, SchedulerConfig,
                          VllmConfig)
 from vllm.core.scheduler import ScheduledSequenceGroup, SchedulerOutputs
+from vllm.distributed.kv_transfer import get_kv_transfer_group
 from vllm.engine.arg_utils import EngineArgs
 from vllm.engine.metrics_types import StatLoggerBase, Stats
 from vllm.engine.output_processor.interfaces import (
@@ -1784,6 +1785,12 @@ class LLMEngine:
         else:
             spec_decode_metrics = None
 
+        if (self.vllm_config.kv_transfer_config is not None and
+                self.vllm_config.kv_transfer_config.is_kv_transfer_instance):
+            kv_transfer_metrics = get_kv_transfer_group().metrics
+        else:
+            kv_transfer_metrics = None
+
         return Stats(
             now=now,
             # System stats
@@ -1806,6 +1813,7 @@ class LLMEngine:
             time_per_output_tokens_iter=time_per_output_tokens_iter,
             spec_decode_metrics=spec_decode_metrics,
             num_preemption_iter=num_preemption_iter,
+            kv_transfer_metrics=kv_transfer_metrics,
 
             # Request stats
             #   Latency
diff --git a/vllm/engine/metrics.py b/vllm/engine/metrics.py
index 8d51f0472..6510cda6f 100644
--- a/vllm/engine/metrics.py
+++ b/vllm/engine/metrics.py
@@ -10,6 +10,7 @@ import numpy as np
 import prometheus_client
 
 from vllm.config import SupportsMetricsInfo, VllmConfig
+from vllm.distributed.kv_transfer import get_kv_transfer_group
 from vllm.engine.metrics_types import StatLoggerBase, Stats
 from vllm.executor.ray_utils import ray
 from vllm.logger import init_logger
@@ -223,6 +224,18 @@ class Metrics:
             documentation="Number of emitted tokens.",
             labelnames=labelnames))
 
+        self.kv_transfer_metrics_exporter = None
+        if (vllm_config.kv_transfer_config is not None
+                and vllm_config.kv_transfer_config.is_kv_transfer_instance):
+            exporter_cls = get_kv_transfer_group().get_metrics_exporter_cls()
+            if exporter_cls is not None:
+                self.kv_transfer_metrics_exporter = exporter_cls(
+                    prefix="vllm:",
+                    labelnames=labelnames,
+                    gauge_cls=self._gauge_cls,
+                    counter_cls=self._counter_cls,
+                    histogram_cls=self._histogram_cls,
+                )
 
 # --8<-- [end:metrics-definitions]
 
@@ -440,6 +453,9 @@ class LoggingStatLogger(StatLoggerBase):
                     self._format_spec_decode_metrics_str(
                         self.spec_decode_metrics))
 
+            if stats.kv_transfer_metrics is not None:
+                log_fn(str(stats.kv_transfer_metrics))
+
             self._reset(stats, prompt_throughput, generation_throughput)
 
     def _reset(self, stats, prompt_throughput, generation_throughput) -> None:
@@ -450,6 +466,8 @@ class LoggingStatLogger(StatLoggerBase):
         self.spec_decode_metrics = None
         self.last_prompt_throughput = prompt_throughput
         self.last_generation_throughput = generation_throughput
+        if stats.kv_transfer_metrics is not None:
+            stats.kv_transfer_metrics.reset()
 
     def _format_spec_decode_metrics_str(
             self, metrics: "SpecDecodeWorkerMetrics") -> str:
@@ -569,6 +587,12 @@ class PrometheusStatLogger(StatLoggerBase):
             stats.max_num_generation_tokens_requests)
         self._log_histogram(self.metrics.histogram_max_tokens_request,
                             stats.max_tokens_requests)
+        if (stats.kv_transfer_metrics is not None
+                and self.metrics.kv_transfer_metrics_exporter is not None):
+            self.metrics.kv_transfer_metrics_exporter.export(
+                metrics=stats.kv_transfer_metrics,
+                labels=self.labels,
+            )
 
     def log(self, stats: Stats):
         """Logs to prometheus and tracked stats every iteration."""
@@ -606,6 +630,8 @@ class PrometheusStatLogger(StatLoggerBase):
             self.num_generation_tokens = []
             self.last_local_log = stats.now
             self.spec_decode_metrics = None
+            if stats.kv_transfer_metrics is not None:
+                stats.kv_transfer_metrics.reset()
 
     def info(self, type: str, obj: SupportsMetricsInfo) -> None:
         # Info type metrics are syntactic sugar for a gauge permanently set to 1
diff --git a/vllm/engine/metrics_types.py b/vllm/engine/metrics_types.py
index 9375dc4c4..bdb8e9e6f 100644
--- a/vllm/engine/metrics_types.py
+++ b/vllm/engine/metrics_types.py
@@ -19,6 +19,7 @@ from dataclasses import dataclass
 from typing import List, Optional
 
 from vllm.config import SupportsMetricsInfo, VllmConfig
+from vllm.distributed.kv_transfer.kv_transfer_metrics import KVTransferMetrics
 from vllm.spec_decode.metrics import SpecDecodeWorkerMetrics
 
 
@@ -66,6 +67,7 @@ class Stats:
     max_lora: str
 
     spec_decode_metrics: Optional["SpecDecodeWorkerMetrics"] = None
+    kv_transfer_metrics: Optional["KVTransferMetrics"] = None
 
 
 class StatLoggerBase(ABC):
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index b1bc727e1..ccecc4d15 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -326,7 +326,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
     def _sync_device(self) -> None:
         torch.cuda.synchronize()
 
-    def _update_states(self, scheduler_output: "SchedulerOutput") -> None:
+    def _update_states(self, scheduler_output: "SchedulerOutput",
+                       load_results: dict[str, int]) -> None:
         """Update the cached states and the persistent batch with the scheduler
         output.
 
@@ -381,6 +382,20 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # Add new requests to the cached states.
         for new_req_data in scheduler_output.scheduled_new_reqs:
             req_id = new_req_data.req_id
+
+            num_loaded_tokens = load_results.get(req_id, 0)
+            if num_loaded_tokens > 0:
+                num_scheduled_tokens = \
+                    scheduler_output.num_scheduled_tokens[req_id]
+                if num_loaded_tokens == num_scheduled_tokens:
+                    num_loaded_tokens -= 1
+
+                new_req_data.num_computed_tokens += num_loaded_tokens
+                scheduler_output.num_scheduled_tokens[req_id] -= \
+                    num_loaded_tokens
+                scheduler_output.total_num_scheduled_tokens -= \
+                    num_loaded_tokens
+
             sampling_params = new_req_data.sampling_params
             if sampling_params.sampling_type == SamplingType.RANDOM_SEED:
                 generator = torch.Generator(device=self.device)
@@ -445,9 +460,24 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             req_id = req_data.req_id
             req_state = self.requests[req_id]
 
+            num_loaded_tokens = load_results.get(req_id, 0)
+            if num_loaded_tokens > 0:
+                num_scheduled_tokens = \
+                    scheduler_output.num_scheduled_tokens[req_id]
+                if num_loaded_tokens == num_scheduled_tokens:
+                    num_loaded_tokens -= 1
+
+                req_data.num_computed_tokens += num_loaded_tokens
+                scheduler_output.num_scheduled_tokens[req_id] -= \
+                    num_loaded_tokens
+                scheduler_output.total_num_scheduled_tokens -= \
+                    num_loaded_tokens
+
             # Update the cached states.
-            num_computed_tokens = req_data.num_computed_tokens
-            req_state.num_computed_tokens = num_computed_tokens
+            num_computed_tokens = req_data.num_computed_tokens - \
+                num_loaded_tokens
+            new_num_computed_tokens = req_data.num_computed_tokens
+            req_state.num_computed_tokens = new_num_computed_tokens
             # Add the sampled token(s) from the previous step (if any).
             # This doesn't include "unverified" tokens like spec decode tokens.
             num_new_tokens = (num_computed_tokens +
@@ -482,7 +512,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
 
             # Update the persistent batch.
             self.input_batch.num_computed_tokens_cpu[req_index] = (
-                num_computed_tokens)
+                new_num_computed_tokens)
             self.input_batch.block_table.append_row(req_data.new_block_ids,
                                                     req_index)
             # Add new_token_ids to token_ids_cpu.
@@ -1174,7 +1204,12 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         intermediate_tensors: Optional[IntermediateTensors] = None,
     ) -> Union[ModelRunnerOutput, IntermediateTensors]:
 
-        self._update_states(scheduler_output)
+        load_results = {}
+        if has_kv_transfer_group():
+            load_results = \
+                self.kv_connector_load_before_update(scheduler_output)
+
+        self._update_states(scheduler_output, load_results)
         if not scheduler_output.total_num_scheduled_tokens:
             if not has_kv_transfer_group():
                 # Return empty ModelRunnerOutput if there's no work to do.
@@ -1507,6 +1542,15 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             finished_recving=finished_recving,
         )
 
+    def kv_connector_load_before_update(
+            self, scheduler_output: "SchedulerOutput") -> dict[str, int]:
+        kv_connector = get_kv_transfer_group()
+        assert isinstance(kv_connector, KVConnectorBase_V1)
+        assert scheduler_output.kv_connector_metadata is not None
+        kv_connector.bind_connector_metadata(
+            scheduler_output.kv_connector_metadata)
+        return kv_connector.start_load_kv_before_update()
+
     def kv_connector_no_forward(
             self, scheduler_output: "SchedulerOutput") -> ModelRunnerOutput:
         # KV send/recv even if no work to do.
diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index 82db6617b..9edc9e3fc 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -26,6 +26,8 @@ from vllm.config import CompilationLevel, VllmConfig
 from vllm.core.scheduler import SchedulerOutputs
 from vllm.distributed import broadcast_tensor_dict, get_pp_group
 from vllm.distributed.kv_transfer import get_kv_transfer_group
+from vllm.distributed.kv_transfer.kv_transfer_metadata import (
+    KVTransferMetadata, KVTransferMetadataCache)
 from vllm.distributed.parallel_state import (get_tensor_model_parallel_rank,
                                              graph_capture)
 from vllm.forward_context import get_forward_context, set_forward_context
@@ -58,8 +60,10 @@ from vllm.utils import (DeviceMemoryProfiler, GiB_bytes, PyObjectCache,
 from vllm.worker.model_runner_base import (
     InputProcessingError, ModelRunnerBase, ModelRunnerInputBase,
     ModelRunnerInputBuilderBase, _add_attn_metadata_broadcastable_dict,
+    _add_kv_transfer_metadata_broadcastable_dict,
     _add_sampling_metadata_broadcastable_dict,
     _init_attn_metadata_from_tensor_dict,
+    _init_kv_transfer_metadata_from_tensor_dict,
     _init_sampling_metadata_from_tensor_dict)
 
 if TYPE_CHECKING:
@@ -104,12 +108,15 @@ class ModelInputForGPU(ModelRunnerInputBase):
     async_callback: Optional[Callable] = None
     scheduler_outputs: Optional[SchedulerOutputs] = None
     previous_hidden_states: Optional[torch.Tensor] = None
+    kv_transfer_metadata: Optional[KVTransferMetadata] = None
 
     def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:
         tensor_dict = {
             "input_tokens": self.input_tokens,
             "inputs_embeds": self.inputs_embeds,
             "input_positions": self.input_positions,
+            "seq_lens": self.seq_lens,
+            "query_lens": self.query_lens,
             "lora_requests": self.lora_requests,
             "lora_mapping": self.lora_mapping,
             "multi_modal_kwargs": self.multi_modal_kwargs,
@@ -120,6 +127,8 @@ class ModelInputForGPU(ModelRunnerInputBase):
             "finished_requests_ids": self.finished_requests_ids,
         }
         _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)
+        _add_kv_transfer_metadata_broadcastable_dict(tensor_dict,
+                                                     self.kv_transfer_metadata)
         return tensor_dict
 
     @classmethod
@@ -131,6 +140,7 @@ class ModelInputForGPU(ModelRunnerInputBase):
         if attn_backend is not None:
             tensor_dict = _init_attn_metadata_from_tensor_dict(
                 attn_backend, tensor_dict)
+        _init_kv_transfer_metadata_from_tensor_dict(tensor_dict)
         return cls(**tensor_dict)
 
     # Exclude `async_callback` to be able to pickle this object
@@ -161,6 +171,8 @@ class ModelInputForGPUWithSamplingMetadata(ModelInputForGPU):
             "input_tokens": self.input_tokens,
             "inputs_embeds": self.inputs_embeds,
             "input_positions": self.input_positions,
+            "seq_lens": self.seq_lens,
+            "query_lens": self.query_lens,
             "lora_requests": self.lora_requests,
             "lora_mapping": self.lora_mapping,
             "multi_modal_kwargs": self.multi_modal_kwargs,
@@ -171,6 +183,8 @@ class ModelInputForGPUWithSamplingMetadata(ModelInputForGPU):
             "finished_requests_ids": self.finished_requests_ids,
         }
         _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)
+        _add_kv_transfer_metadata_broadcastable_dict(tensor_dict,
+                                                     self.kv_transfer_metadata)
         _add_sampling_metadata_broadcastable_dict(tensor_dict,
                                                   self.sampling_metadata)
         return tensor_dict
@@ -185,6 +199,7 @@ class ModelInputForGPUWithSamplingMetadata(ModelInputForGPU):
         if attn_backend is not None:
             tensor_dict = _init_attn_metadata_from_tensor_dict(
                 attn_backend, tensor_dict)
+        _init_kv_transfer_metadata_from_tensor_dict(tensor_dict)
         return cls(**tensor_dict)
 
 
@@ -1163,6 +1178,10 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
               SamplingMetadataCache() \
                 if self.parallel_config.pipeline_parallel_size == 1 else None
 
+        self.kv_transfer_metadata_cache: KVTransferMetadataCache = (
+            KVTransferMetadataCache()
+            if self.vllm_config.kv_transfer_config is not None else None)
+
         if hasattr(self, "_builder_cls"):
             # multi-step model runner does not have `_builder_cls`
             self.builder = self._builder_cls(weakref.proxy(self))
@@ -1747,8 +1766,21 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
             sampling_metadata = None
         is_prompt = (seq_group_metadata_list[0].is_prompt
                      if seq_group_metadata_list else None)
+
+        if self.vllm_config.kv_transfer_config is not None:
+            kv_transfer_metadata = KVTransferMetadata.prepare(
+                seq_group_metadata_list,
+                self.kv_transfer_metadata_cache,
+            )
+            # attach seq_group_metadata_list for rebuilding model_input
+            # on the driver side
+            kv_transfer_metadata.seq_group_metadata_list = \
+                seq_group_metadata_list
+        else:
+            kv_transfer_metadata = None
         return dataclasses.replace(model_input,
                                    sampling_metadata=sampling_metadata,
+                                   kv_transfer_metadata=kv_transfer_metadata,
                                    is_prompt=is_prompt,
                                    virtual_engine=virtual_engine)
 
@@ -1814,6 +1846,9 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
         # NOTE: The receive operation is blocking
         bypass_model_exec = False
         if self.need_recv_kv(model_input, kv_caches):
+            # attach runner for rebuilding model_input
+            model_input.kv_transfer_metadata.runner = weakref.proxy(
+                self)  # type: ignore
             hidden_or_intermediate_states, bypass_model_exec, model_input = \
                 get_kv_transfer_group().recv_kv_caches_and_hidden_states(
                     # model is used to know which layer the current worker
diff --git a/vllm/worker/model_runner_base.py b/vllm/worker/model_runner_base.py
index d567ce4a6..5b5360e75 100644
--- a/vllm/worker/model_runner_base.py
+++ b/vllm/worker/model_runner_base.py
@@ -17,6 +17,8 @@ from vllm.sequence import IntermediateTensors, SequenceGroupMetadata
 if TYPE_CHECKING:
     from vllm.attention import AttentionMetadata
     from vllm.attention.backends.abstract import AttentionBackend
+    from vllm.distributed.kv_transfer.kv_transfer_metadata import (
+        KVTransferMetadata)
     from vllm.model_executor import SamplingMetadata
 
 logger = init_logger(__name__)
@@ -47,7 +49,7 @@ def _init_attn_metadata_from_tensor_dict(
     valid_attn_kwargs = {}
     for field in dataclasses.fields(attn_backend.get_metadata_cls()):
         if field.name in tensor_dict:
-            if field.name == "input_positions":
+            if field.name in ["input_positions", "seq_lens"]:
                 valid_attn_kwargs[field.name] = tensor_dict[field.name]
             else:
                 valid_attn_kwargs[field.name] = tensor_dict.pop(field.name)
@@ -90,6 +92,34 @@ def _add_sampling_metadata_broadcastable_dict(
             sampling_metadata.selected_token_indices)
 
 
+def _init_kv_transfer_metadata_from_tensor_dict(
+        tensor_dict: Dict[str, Any]) -> Dict[str, Any]:
+    """
+    Helper method to initialize KVTransferMetadata based on broadcastable
+    KVTransferMetadata fields.
+    """
+    from vllm.distributed.kv_transfer.kv_transfer_metadata import (
+        KVTransferMetadata)
+
+    kv_transfer_metadata = tensor_dict.pop("kv_transfer_metadata", None)
+    if kv_transfer_metadata is not None:
+        seq_groups = kv_transfer_metadata
+        tensor_dict["kv_transfer_metadata"] = KVTransferMetadata(
+            seq_groups=seq_groups)
+    return tensor_dict
+
+
+def _add_kv_transfer_metadata_broadcastable_dict(
+        tensor_dict: Dict[str, Any],
+        kv_transfer_metadata: Optional["KVTransferMetadata"]) -> None:
+    """
+    Helper method to update tensor_dict with broadcastable
+    KVTransferMetadata fields.
+    """
+    if kv_transfer_metadata is not None:
+        tensor_dict["kv_transfer_metadata"] = kv_transfer_metadata.seq_groups
+
+
 def _init_frozen_model_input_from_tensor_dict(
         frozen_model_input_cls: Type["ModelRunnerInputBase"],
         tensor_dict: Dict[str, Any]) -> Dict[str, Any]:
