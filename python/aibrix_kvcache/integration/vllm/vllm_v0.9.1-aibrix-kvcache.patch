diff --git a/examples/online_serving/prometheus_grafana/grafana_kv_cache_offload.json b/examples/online_serving/prometheus_grafana/grafana_kv_cache_offload.json
new file mode 100644
index 000000000..311169501
--- /dev/null
+++ b/examples/online_serving/prometheus_grafana/grafana_kv_cache_offload.json
@@ -0,0 +1,4919 @@
+{
+    "annotations": {
+      "list": [
+        {
+          "builtIn": 1,
+          "datasource": {
+            "type": "grafana",
+            "uid": "-- Grafana --"
+          },
+          "enable": true,
+          "hide": true,
+          "iconColor": "rgba(0, 211, 255, 1)",
+          "name": "Annotations & Alerts",
+          "target": {
+            "limit": 100,
+            "matchAny": false,
+            "tags": [],
+            "type": "dashboard"
+          },
+          "type": "dashboard"
+        }
+      ]
+    },
+    "description": "Monitoring vLLM Inference Server",
+    "editable": true,
+    "fiscalYearStartMonth": 0,
+    "graphTooltip": 0,
+    "id": 4,
+    "links": [],
+    "liveNow": false,
+    "panels": [
+      {
+        "collapsed": false,
+        "gridPos": {
+          "h": 1,
+          "w": 24,
+          "x": 0,
+          "y": 0
+        },
+        "id": 19,
+        "panels": [],
+        "title": "vLLM Engine",
+        "type": "row"
+      },
+      {
+        "datasource": {
+          "default": true,
+          "type": "prometheus",
+          "uid": "cdy727ch5evi8d"
+        },
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "thresholds"
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 3,
+          "x": 0,
+          "y": 1
+        },
+        "id": 16,
+        "options": {
+          "colorMode": "value",
+          "graphMode": "area",
+          "justifyMode": "auto",
+          "orientation": "auto",
+          "percentChangeColorMode": "standard",
+          "reduceOptions": {
+            "calcs": [
+              "lastNotNull"
+            ],
+            "fields": "",
+            "values": false
+          },
+          "showPercentChange": false,
+          "textMode": "auto",
+          "wideLayout": true
+        },
+        "pluginVersion": "11.2.0",
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "editorMode": "code",
+            "expr": "sum(increase(vllm:request_success_total{finished_reason=\"stop\", model_name=\"$model_name\", job=\"pods\"}[$__range]))",
+            "instant": false,
+            "legendFormat": "Success Request Count(stop)",
+            "range": true,
+            "refId": "A"
+          }
+        ],
+        "title": "Success Request Count(Stop)",
+        "type": "stat"
+      },
+      {
+        "datasource": {
+          "default": true,
+          "type": "prometheus",
+          "uid": "cdy727ch5evi8d"
+        },
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 5,
+          "x": 3,
+          "y": 1
+        },
+        "id": 17,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "pluginVersion": "11.2.0",
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "editorMode": "code",
+            "expr": "sum by(finished_reason) (increase(vllm:request_success_total{model_name=\"$model_name\", job=\"pods\"}[$__range]))",
+            "instant": false,
+            "legendFormat": "Success Request Count",
+            "range": true,
+            "refId": "A"
+          }
+        ],
+        "title": "Success Request Count",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Heatmap of request prompt length",
+        "fieldConfig": {
+          "defaults": {
+            "custom": {
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "scaleDistribution": {
+                "type": "linear"
+              }
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 5,
+          "x": 8,
+          "y": 1
+        },
+        "id": 12,
+        "options": {
+          "calculate": false,
+          "cellGap": 1,
+          "cellValues": {
+            "unit": "none"
+          },
+          "color": {
+            "exponent": 0.5,
+            "fill": "dark-orange",
+            "min": 0,
+            "mode": "scheme",
+            "reverse": false,
+            "scale": "exponential",
+            "scheme": "Spectral",
+            "steps": 64
+          },
+          "exemplars": {
+            "color": "rgba(255,0,255,0.7)"
+          },
+          "filterValues": {
+            "le": 1e-9
+          },
+          "legend": {
+            "show": true,
+            "showLegend": true
+          },
+          "rowsFrame": {
+            "layout": "auto",
+            "value": "Request count"
+          },
+          "tooltip": {
+            "mode": "single",
+            "showColorScale": false,
+            "yHistogram": true
+          },
+          "yAxis": {
+            "axisLabel": "Prompt Length",
+            "axisPlacement": "left",
+            "reverse": false,
+            "unit": "none"
+          }
+        },
+        "pluginVersion": "11.2.0",
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum by(le) (increase(vllm:request_prompt_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "format": "heatmap",
+            "fullMetaSearch": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "legendFormat": "{{le}}",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          }
+        ],
+        "title": "Request Prompt Length",
+        "type": "heatmap"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Heatmap of request generation length",
+        "fieldConfig": {
+          "defaults": {
+            "custom": {
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "scaleDistribution": {
+                "type": "linear"
+              }
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 5,
+          "x": 13,
+          "y": 1
+        },
+        "id": 13,
+        "options": {
+          "calculate": false,
+          "cellGap": 1,
+          "cellValues": {
+            "unit": "none"
+          },
+          "color": {
+            "exponent": 0.5,
+            "fill": "dark-orange",
+            "min": 0,
+            "mode": "scheme",
+            "reverse": false,
+            "scale": "exponential",
+            "scheme": "Spectral",
+            "steps": 64
+          },
+          "exemplars": {
+            "color": "rgba(255,0,255,0.7)"
+          },
+          "filterValues": {
+            "le": 1e-9
+          },
+          "legend": {
+            "show": true
+          },
+          "rowsFrame": {
+            "layout": "auto",
+            "value": "Request count"
+          },
+          "tooltip": {
+            "mode": "single",
+            "showColorScale": false,
+            "yHistogram": true
+          },
+          "yAxis": {
+            "axisLabel": "Generation Length",
+            "axisPlacement": "left",
+            "reverse": false,
+            "unit": "none"
+          }
+        },
+        "pluginVersion": "11.2.0",
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum by(le) (increase(vllm:request_generation_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "format": "heatmap",
+            "fullMetaSearch": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "legendFormat": "{{le}}",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          }
+        ],
+        "title": "Request Generation Length",
+        "type": "heatmap"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Prompt & Decode length",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "none"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 6,
+          "x": 18,
+          "y": 1
+        },
+        "id": 18,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:request_prompt_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "Prompt-P99",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.9, sum by(le) (rate(vllm:request_prompt_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "Prompt-P90",
+            "range": true,
+            "refId": "C",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.5, sum by(le) (rate(vllm:request_prompt_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "Prompt-P50",
+            "range": true,
+            "refId": "D",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:request_generation_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Decode-P99",
+            "range": true,
+            "refId": "B"
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.90, sum by(le) (rate(vllm:request_generation_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Decode-P90",
+            "range": true,
+            "refId": "E"
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.50, sum by(le) (rate(vllm:request_generation_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Decode-P50",
+            "range": true,
+            "refId": "F"
+          }
+        ],
+        "title": "Prompt & Decode Length",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "End to end request latency measured in seconds.",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "s"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 8,
+          "x": 0,
+          "y": 8
+        },
+        "id": 9,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P99",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "builder",
+            "expr": "histogram_quantile(0.95, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{model_name=\"$model_name\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P95",
+            "range": true,
+            "refId": "B",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "builder",
+            "expr": "histogram_quantile(0.9, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{model_name=\"$model_name\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P90",
+            "range": true,
+            "refId": "C",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "builder",
+            "expr": "histogram_quantile(0.5, sum by(le) (rate(vllm:e2e_request_latency_seconds_bucket{model_name=\"$model_name\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P50",
+            "range": true,
+            "refId": "D",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(rate(vllm:e2e_request_latency_seconds_sum{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])\n/\nrate(vllm:e2e_request_latency_seconds_count{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Average",
+            "range": true,
+            "refId": "E"
+          }
+        ],
+        "title": "E2E Request Latency",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "P50, P90, P95, and P99 TTFT latency in seconds.",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "s"
+          },
+          "overrides": [
+            {
+              "__systemRef": "hideSeriesFrom",
+              "matcher": {
+                "id": "byNames",
+                "options": {
+                  "mode": "exclude",
+                  "names": [
+                    "Average"
+                  ],
+                  "prefix": "All except:",
+                  "readOnly": true
+                }
+              },
+              "properties": [
+                {
+                  "id": "custom.hideFrom",
+                  "value": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": true
+                  }
+                }
+              ]
+            }
+          ]
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 8,
+          "x": 8,
+          "y": 8
+        },
+        "id": 5,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{model_name=\"$model_name\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P99",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.95, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P95",
+            "range": true,
+            "refId": "B",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.9, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P90",
+            "range": true,
+            "refId": "C",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.5, sum by(le) (rate(vllm:time_to_first_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P50",
+            "range": true,
+            "refId": "D",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(rate(vllm:time_to_first_token_seconds_sum{model_name=\"$model_name\",job=\"pods\"}[$__rate_interval])\n/\nrate(vllm:time_to_first_token_seconds_count{model_name=\"$model_name\",job=\"pods\"}[$__rate_interval]))",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Average",
+            "range": true,
+            "refId": "E"
+          }
+        ],
+        "title": "Time To First Token Latency",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Inter token latency in seconds.",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "s"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 7,
+          "w": 8,
+          "x": 16,
+          "y": 8
+        },
+        "id": 10,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P99",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.95, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P95",
+            "range": true,
+            "refId": "B",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.9, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P90",
+            "range": true,
+            "refId": "C",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "histogram_quantile(0.5, sum by(le) (rate(vllm:time_per_output_token_seconds_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "P50",
+            "range": true,
+            "refId": "D",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "rate(vllm:time_per_output_token_seconds_sum{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])\n/\nrate(vllm:time_per_output_token_seconds_count{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "Mean",
+            "range": true,
+            "refId": "E"
+          }
+        ],
+        "title": "Time Per Output Token Latency",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Number of tokens processed per second",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 0,
+          "y": 15
+        },
+        "id": 8,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum(rate(vllm:prompt_tokens_total{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "Prompt Tokens/Sec {{model_name}}",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          }
+        ],
+        "title": "Prompt Token Throughput",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Number of tokens processed per second",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 8,
+          "y": 15
+        },
+        "id": 14,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum(rate(vllm:generation_tokens_total{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": false,
+            "instant": false,
+            "legendFormat": "Generation Tokens/Sec {{pod}}",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          }
+        ],
+        "title": "Decode Token Throughput",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Number of requests in RUNNING, WAITING, and SWAPPED state",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "none"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 16,
+          "y": 15
+        },
+        "id": 3,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum(vllm:num_requests_running{model_name=\"$model_name\", job=\"pods\"})",
+            "fullMetaSearch": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "legendFormat": "Num Running",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum(vllm:num_requests_swapped{model_name=\"$model_name\", job=\"pods\"})",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "legendFormat": "Num Swapped",
+            "range": true,
+            "refId": "B",
+            "useBackend": false
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum(vllm:num_requests_waiting{model_name=\"$model_name\", job=\"pods\"})",
+            "fullMetaSearch": false,
+            "hide": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "legendFormat": "Num Waiting",
+            "range": true,
+            "refId": "C",
+            "useBackend": false
+          }
+        ],
+        "title": "Scheduler State",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Percentage of prefix cache hit rate",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "percentunit"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 0,
+          "y": 23
+        },
+        "id": 15,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(vllm:gpu_prefix_cache_hit_rate{model_name=\"$model_name\",job=\"pods\"})",
+            "instant": false,
+            "legendFormat": "GPU Prefix Cache Hit Ratio",
+            "range": true,
+            "refId": "A"
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(vllm:cpu_prefix_cache_hit_rate{model_name=\"$model_name\",job=\"pods\"})",
+            "hide": true,
+            "instant": false,
+            "legendFormat": "CPU Prefix Cache Hit Ratio",
+            "range": true,
+            "refId": "B"
+          }
+        ],
+        "title": "Prefix Cache Hit Rate",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Percentage of used cache blocks by vLLM.",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            },
+            "unit": "percentunit"
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 8,
+          "y": 23
+        },
+        "id": 4,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(vllm:gpu_cache_usage_perc{model_name=\"$model_name\", job=\"pods\"})",
+            "instant": false,
+            "legendFormat": "GPU Cache Usage",
+            "range": true,
+            "refId": "A"
+          },
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "editorMode": "code",
+            "expr": "avg(vllm:cpu_cache_usage_perc{model_name=\"$model_name\", job=\"pods\"})",
+            "hide": false,
+            "instant": false,
+            "legendFormat": "CPU Cache Usage",
+            "range": true,
+            "refId": "B"
+          }
+        ],
+        "title": "Cache Utilization",
+        "type": "timeseries"
+      },
+      {
+        "datasource": {
+          "default": false,
+          "type": "prometheus",
+          "uid": "${DS_PROMETHEUS}"
+        },
+        "description": "Number of finished requests by their finish reason: either an EOS token was generated or the max sequence length was reached.",
+        "fieldConfig": {
+          "defaults": {
+            "color": {
+              "mode": "palette-classic"
+            },
+            "custom": {
+              "axisBorderShow": false,
+              "axisCenteredZero": false,
+              "axisColorMode": "text",
+              "axisLabel": "",
+              "axisPlacement": "auto",
+              "barAlignment": 0,
+              "barWidthFactor": 0.6,
+              "drawStyle": "line",
+              "fillOpacity": 0,
+              "gradientMode": "none",
+              "hideFrom": {
+                "legend": false,
+                "tooltip": false,
+                "viz": false
+              },
+              "insertNulls": false,
+              "lineInterpolation": "linear",
+              "lineWidth": 1,
+              "pointSize": 5,
+              "scaleDistribution": {
+                "type": "linear"
+              },
+              "showPoints": "auto",
+              "spanNulls": false,
+              "stacking": {
+                "group": "A",
+                "mode": "none"
+              },
+              "thresholdsStyle": {
+                "mode": "off"
+              }
+            },
+            "mappings": [],
+            "thresholds": {
+              "mode": "absolute",
+              "steps": [
+                {
+                  "color": "green",
+                  "value": null
+                },
+                {
+                  "color": "red",
+                  "value": 80
+                }
+              ]
+            }
+          },
+          "overrides": []
+        },
+        "gridPos": {
+          "h": 8,
+          "w": 8,
+          "x": 16,
+          "y": 23
+        },
+        "id": 11,
+        "options": {
+          "legend": {
+            "calcs": [],
+            "displayMode": "list",
+            "placement": "bottom",
+            "showLegend": true
+          },
+          "tooltip": {
+            "mode": "single",
+            "sort": "none"
+          }
+        },
+        "targets": [
+          {
+            "datasource": {
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "disableTextWrap": false,
+            "editorMode": "code",
+            "expr": "sum by(finished_reason) (increase(vllm:request_success_total{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+            "fullMetaSearch": false,
+            "includeNullMetadata": true,
+            "instant": false,
+            "interval": "",
+            "legendFormat": "__auto",
+            "range": true,
+            "refId": "A",
+            "useBackend": false
+          }
+        ],
+        "title": "Finish Reason",
+        "type": "timeseries"
+      },
+      {
+        "collapsed": true,
+        "gridPos": {
+          "h": 1,
+          "w": 24,
+          "x": 0,
+          "y": 31
+        },
+        "id": 31,
+        "panels": [
+          {
+            "datasource": {
+              "default": true,
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "thresholds"
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 0,
+              "y": 32
+            },
+            "id": 39,
+            "options": {
+              "colorMode": "value",
+              "graphMode": "area",
+              "justifyMode": "auto",
+              "orientation": "auto",
+              "percentChangeColorMode": "standard",
+              "reduceOptions": {
+                "calcs": [
+                  "lastNotNull"
+                ],
+                "fields": "",
+                "values": false
+              },
+              "showPercentChange": false,
+              "textMode": "auto",
+              "wideLayout": true
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "cdy727ch5evi8d"
+                },
+                "editorMode": "code",
+                "expr": "sum by(op_type) (increase(vllm:ol_connector_num_ops_total{model_name=\"$model_name\", job=\"pods\"}[$__range]))",
+                "instant": false,
+                "legendFormat": "{{op_type}}",
+                "range": true,
+                "refId": "A"
+              }
+            ],
+            "title": "Total Ops",
+            "type": "stat"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Heatmap of prefix length",
+            "fieldConfig": {
+              "defaults": {
+                "custom": {
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "scaleDistribution": {
+                    "type": "linear"
+                  }
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 4,
+              "y": 32
+            },
+            "id": 29,
+            "options": {
+              "calculate": false,
+              "cellGap": 1,
+              "cellValues": {
+                "unit": "none"
+              },
+              "color": {
+                "exponent": 0.5,
+                "fill": "dark-orange",
+                "min": 0,
+                "mode": "scheme",
+                "reverse": false,
+                "scale": "exponential",
+                "scheme": "Spectral",
+                "steps": 64
+              },
+              "exemplars": {
+                "color": "rgba(255,0,255,0.7)"
+              },
+              "filterValues": {
+                "le": 1e-9
+              },
+              "legend": {
+                "show": true,
+                "showLegend": true
+              },
+              "rowsFrame": {
+                "layout": "auto",
+                "value": "Request count"
+              },
+              "tooltip": {
+                "mode": "single",
+                "showColorScale": false,
+                "yHistogram": true
+              },
+              "yAxis": {
+                "axisLabel": "Prompt Length",
+                "axisPlacement": "left",
+                "reverse": false,
+                "unit": "none"
+              }
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "sum by(le, op_type) (increase(vllm:ol_connector_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+                "format": "heatmap",
+                "fullMetaSearch": false,
+                "includeNullMetadata": true,
+                "instant": false,
+                "legendFormat": "{{op_type}}-{{le}}",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              }
+            ],
+            "title": "Prefix Length",
+            "type": "heatmap"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Prefix length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 9,
+              "y": 32
+            },
+            "id": 30,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:ol_connector_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:ol_connector_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:ol_connector_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Prefix Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Heatmap of token length",
+            "fieldConfig": {
+              "defaults": {
+                "custom": {
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "scaleDistribution": {
+                    "type": "linear"
+                  }
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 14,
+              "y": 32
+            },
+            "id": 32,
+            "options": {
+              "calculate": false,
+              "cellGap": 1,
+              "cellValues": {
+                "unit": "none"
+              },
+              "color": {
+                "exponent": 0.5,
+                "fill": "dark-orange",
+                "min": 0,
+                "mode": "scheme",
+                "reverse": false,
+                "scale": "exponential",
+                "scheme": "Spectral",
+                "steps": 64
+              },
+              "exemplars": {
+                "color": "rgba(255,0,255,0.7)"
+              },
+              "filterValues": {
+                "le": 1e-9
+              },
+              "legend": {
+                "show": true,
+                "showLegend": true
+              },
+              "rowsFrame": {
+                "layout": "auto",
+                "value": "Request count"
+              },
+              "tooltip": {
+                "mode": "single",
+                "showColorScale": false,
+                "yHistogram": true
+              },
+              "yAxis": {
+                "axisLabel": "Prompt Length",
+                "axisPlacement": "left",
+                "reverse": false,
+                "unit": "none"
+              }
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "sum by(le, op_type) (increase(vllm:ol_connector_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+                "format": "heatmap",
+                "fullMetaSearch": false,
+                "includeNullMetadata": true,
+                "instant": false,
+                "legendFormat": "{{op_type}}-{{le}}",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              }
+            ],
+            "title": "Token Length",
+            "type": "heatmap"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Token length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 19,
+              "y": 32
+            },
+            "id": 33,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:ol_connector_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:ol_connector_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:ol_connector_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Token Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Latency",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "ms"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 6,
+              "x": 0,
+              "y": 39
+            },
+            "id": 34,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:ol_connector_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:ol_connector_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:ol_connector_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le) (rate(vllm:ol_connector_iteration_rebuild_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "hide": false,
+                "instant": false,
+                "legendFormat": "rebuild_model_input-P99",
+                "range": true,
+                "refId": "B"
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le) (rate(vllm:ol_connector_iteration_rebuild_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "hide": false,
+                "instant": false,
+                "legendFormat": "rebuild_model_input-P90",
+                "range": true,
+                "refId": "E"
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le) (rate(vllm:ol_connector_iteration_rebuild_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "hide": false,
+                "instant": false,
+                "legendFormat": "rebuild_model_input-P50",
+                "range": true,
+                "refId": "F"
+              }
+            ],
+            "title": "Latency",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Heatmap of sent tokens",
+            "fieldConfig": {
+              "defaults": {
+                "custom": {
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "scaleDistribution": {
+                    "type": "linear"
+                  }
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 6,
+              "y": 39
+            },
+            "id": 35,
+            "options": {
+              "calculate": false,
+              "cellGap": 1,
+              "cellValues": {
+                "unit": "none"
+              },
+              "color": {
+                "exponent": 0.5,
+                "fill": "dark-orange",
+                "min": 0,
+                "mode": "scheme",
+                "reverse": false,
+                "scale": "exponential",
+                "scheme": "Spectral",
+                "steps": 64
+              },
+              "exemplars": {
+                "color": "rgba(255,0,255,0.7)"
+              },
+              "filterValues": {
+                "le": 1e-9
+              },
+              "legend": {
+                "show": true,
+                "showLegend": true
+              },
+              "rowsFrame": {
+                "layout": "auto",
+                "value": "Request count"
+              },
+              "tooltip": {
+                "mode": "single",
+                "showColorScale": false,
+                "yHistogram": true
+              },
+              "yAxis": {
+                "axisLabel": "Prompt Length",
+                "axisPlacement": "left",
+                "reverse": false,
+                "unit": "none"
+              }
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "sum by(le, op_type) (increase(vllm:ol_connector_iteration_sent_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+                "format": "heatmap",
+                "fullMetaSearch": false,
+                "includeNullMetadata": true,
+                "instant": false,
+                "legendFormat": "{{op_type}}-{{le}}",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              }
+            ],
+            "title": "Sent Tokens",
+            "type": "heatmap"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Sent tokens",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 10,
+              "y": 39
+            },
+            "id": 36,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:ol_connector_iteration_sent_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:ol_connector_iteration_sent_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:ol_connector_iteration_sent_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Sent Tokens",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Heatmap of received tokens",
+            "fieldConfig": {
+              "defaults": {
+                "custom": {
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "scaleDistribution": {
+                    "type": "linear"
+                  }
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 15,
+              "y": 39
+            },
+            "id": 37,
+            "options": {
+              "calculate": false,
+              "cellGap": 1,
+              "cellValues": {
+                "unit": "none"
+              },
+              "color": {
+                "exponent": 0.5,
+                "fill": "dark-orange",
+                "min": 0,
+                "mode": "scheme",
+                "reverse": false,
+                "scale": "exponential",
+                "scheme": "Spectral",
+                "steps": 64
+              },
+              "exemplars": {
+                "color": "rgba(255,0,255,0.7)"
+              },
+              "filterValues": {
+                "le": 1e-9
+              },
+              "legend": {
+                "show": true,
+                "showLegend": true
+              },
+              "rowsFrame": {
+                "layout": "auto",
+                "value": "Request count"
+              },
+              "tooltip": {
+                "mode": "single",
+                "showColorScale": false,
+                "yHistogram": true
+              },
+              "yAxis": {
+                "axisLabel": "Prompt Length",
+                "axisPlacement": "left",
+                "reverse": false,
+                "unit": "none"
+              }
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "sum by(le, op_type) (increase(vllm:ol_connector_iteration_received_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval]))",
+                "format": "heatmap",
+                "fullMetaSearch": false,
+                "includeNullMetadata": true,
+                "instant": false,
+                "legendFormat": "{{op_type}}-{{le}}",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              }
+            ],
+            "title": "Received Tokens",
+            "type": "heatmap"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Received tokens",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 19,
+              "y": 39
+            },
+            "id": 38,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:ol_connector_iteration_received_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:ol_connector_iteration_received_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:ol_connector_iteration_received_tokens_bucket{model_name=\"$model_name\", job=\"pods\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Received Tokens",
+            "type": "timeseries"
+          }
+        ],
+        "title": "KV Cache Offloading - AIBrix Connector",
+        "type": "row"
+      },
+      {
+        "collapsed": true,
+        "gridPos": {
+          "h": 1,
+          "w": 24,
+          "x": 0,
+          "y": 32
+        },
+        "id": 40,
+        "panels": [
+          {
+            "datasource": {
+              "default": true,
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "thresholds"
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 0,
+              "y": 33
+            },
+            "id": 47,
+            "options": {
+              "colorMode": "value",
+              "graphMode": "area",
+              "justifyMode": "auto",
+              "orientation": "auto",
+              "percentChangeColorMode": "standard",
+              "reduceOptions": {
+                "calcs": [
+                  "lastNotNull"
+                ],
+                "fields": "",
+                "values": false
+              },
+              "showPercentChange": false,
+              "textMode": "auto",
+              "wideLayout": true
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "cdy727ch5evi8d"
+                },
+                "editorMode": "code",
+                "expr": "sum by(op_type) (increase(vllm:kv_cache_ol_num_ops_total{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__range]))",
+                "instant": false,
+                "legendFormat": "{{op_type}}",
+                "range": true,
+                "refId": "A"
+              }
+            ],
+            "title": "Total Ops",
+            "type": "stat"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Prefix length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 4,
+              "y": 33
+            },
+            "id": 50,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Prefix Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Token length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 9,
+              "y": 33
+            },
+            "id": 51,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Token Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Fetched tokens",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 14,
+              "y": 33
+            },
+            "id": 53,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Fetched Tokens",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Latency",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "ms"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 19,
+              "y": 33
+            },
+            "id": 45,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"cachemgr\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Latency",
+            "type": "timeseries"
+          }
+        ],
+        "title": "KV Cache Offloading - Internal - CacheMgr",
+        "type": "row"
+      },
+      {
+        "collapsed": true,
+        "gridPos": {
+          "h": 1,
+          "w": 24,
+          "x": 0,
+          "y": 33
+        },
+        "id": 57,
+        "panels": [
+          {
+            "datasource": {
+              "default": true,
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "thresholds"
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 0,
+              "y": 34
+            },
+            "id": 48,
+            "options": {
+              "colorMode": "value",
+              "graphMode": "area",
+              "justifyMode": "auto",
+              "orientation": "auto",
+              "percentChangeColorMode": "standard",
+              "reduceOptions": {
+                "calcs": [
+                  "lastNotNull"
+                ],
+                "fields": "",
+                "values": false
+              },
+              "showPercentChange": false,
+              "textMode": "auto",
+              "wideLayout": true
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "cdy727ch5evi8d"
+                },
+                "editorMode": "code",
+                "expr": "sum by(op_type) (increase(vllm:kv_cache_ol_num_ops_total{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__range]))",
+                "instant": false,
+                "legendFormat": "{{op_type}}",
+                "range": true,
+                "refId": "A"
+              }
+            ],
+            "title": "Total Ops",
+            "type": "stat"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Prefix length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 4,
+              "y": 34
+            },
+            "id": 43,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Prefix Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Token length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 9,
+              "y": 34
+            },
+            "id": 44,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Token Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Fetched tokens",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 14,
+              "y": 34
+            },
+            "id": 46,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Fetched Tokens",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Latency",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "ms"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 19,
+              "y": 34
+            },
+            "id": 55,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l1cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Latency",
+            "type": "timeseries"
+          }
+        ],
+        "title": "KV Cache Offloading - Internal - L1Cache",
+        "type": "row"
+      },
+      {
+        "collapsed": true,
+        "gridPos": {
+          "h": 1,
+          "w": 24,
+          "x": 0,
+          "y": 34
+        },
+        "id": 58,
+        "panels": [
+          {
+            "datasource": {
+              "default": true,
+              "type": "prometheus",
+              "uid": "cdy727ch5evi8d"
+            },
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "thresholds"
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                }
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 4,
+              "x": 0,
+              "y": 35
+            },
+            "id": 41,
+            "options": {
+              "colorMode": "value",
+              "graphMode": "area",
+              "justifyMode": "auto",
+              "orientation": "auto",
+              "percentChangeColorMode": "standard",
+              "reduceOptions": {
+                "calcs": [
+                  "lastNotNull"
+                ],
+                "fields": "",
+                "values": false
+              },
+              "showPercentChange": false,
+              "textMode": "auto",
+              "wideLayout": true
+            },
+            "pluginVersion": "11.2.0",
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "cdy727ch5evi8d"
+                },
+                "editorMode": "code",
+                "expr": "sum by(op_type) (increase(vllm:kv_cache_ol_num_ops_total{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__range]))",
+                "instant": false,
+                "legendFormat": "{{op_type}}",
+                "range": true,
+                "refId": "A"
+              }
+            ],
+            "title": "Total Ops",
+            "type": "stat"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Prefix length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 4,
+              "y": 35
+            },
+            "id": 49,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_prefixes_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Prefix Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Token length",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 9,
+              "y": 35
+            },
+            "id": 52,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Token Length",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Fetched tokens",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "none"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 14,
+              "y": 35
+            },
+            "id": 54,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_fetched_tokens_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Fetched Tokens",
+            "type": "timeseries"
+          },
+          {
+            "datasource": {
+              "default": false,
+              "type": "prometheus",
+              "uid": "${DS_PROMETHEUS}"
+            },
+            "description": "Latency",
+            "fieldConfig": {
+              "defaults": {
+                "color": {
+                  "mode": "palette-classic"
+                },
+                "custom": {
+                  "axisBorderShow": false,
+                  "axisCenteredZero": false,
+                  "axisColorMode": "text",
+                  "axisLabel": "",
+                  "axisPlacement": "auto",
+                  "barAlignment": 0,
+                  "barWidthFactor": 0.6,
+                  "drawStyle": "line",
+                  "fillOpacity": 0,
+                  "gradientMode": "none",
+                  "hideFrom": {
+                    "legend": false,
+                    "tooltip": false,
+                    "viz": false
+                  },
+                  "insertNulls": false,
+                  "lineInterpolation": "linear",
+                  "lineWidth": 1,
+                  "pointSize": 5,
+                  "scaleDistribution": {
+                    "type": "linear"
+                  },
+                  "showPoints": "auto",
+                  "spanNulls": false,
+                  "stacking": {
+                    "group": "A",
+                    "mode": "none"
+                  },
+                  "thresholdsStyle": {
+                    "mode": "off"
+                  }
+                },
+                "mappings": [],
+                "thresholds": {
+                  "mode": "absolute",
+                  "steps": [
+                    {
+                      "color": "green",
+                      "value": null
+                    },
+                    {
+                      "color": "red",
+                      "value": 80
+                    }
+                  ]
+                },
+                "unit": "ms"
+              },
+              "overrides": []
+            },
+            "gridPos": {
+              "h": 7,
+              "w": 5,
+              "x": 19,
+              "y": 35
+            },
+            "id": 56,
+            "options": {
+              "legend": {
+                "calcs": [],
+                "displayMode": "list",
+                "placement": "bottom",
+                "showLegend": true
+              },
+              "tooltip": {
+                "mode": "single",
+                "sort": "none"
+              }
+            },
+            "targets": [
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.99, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P99",
+                "range": true,
+                "refId": "A",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.9, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P90",
+                "range": true,
+                "refId": "C",
+                "useBackend": false
+              },
+              {
+                "datasource": {
+                  "type": "prometheus",
+                  "uid": "${DS_PROMETHEUS}"
+                },
+                "disableTextWrap": false,
+                "editorMode": "code",
+                "expr": "histogram_quantile(0.5, sum by(le, op_type) (rate(vllm:kv_cache_ol_iteration_op_lat_ms_bucket{model_name=\"$model_name\", job=\"pods\", cache_type=\"l2cache\"}[$__rate_interval])))",
+                "fullMetaSearch": false,
+                "hide": false,
+                "includeNullMetadata": false,
+                "instant": false,
+                "legendFormat": "{{op_type}}-P50",
+                "range": true,
+                "refId": "D",
+                "useBackend": false
+              }
+            ],
+            "title": "Latency",
+            "type": "timeseries"
+          }
+        ],
+        "title": "KV Cache Offloading - Internal - L2Cache",
+        "type": "row"
+      }
+    ],
+    "refresh": "",
+    "schemaVersion": 39,
+    "tags": [
+      "vllm",
+      "model"
+    ],
+    "templating": {
+      "list": [
+        {
+          "current": {
+            "selected": false,
+            "text": "prometheus",
+            "value": "cdy727ch5evi8d"
+          },
+          "hide": 0,
+          "includeAll": false,
+          "label": "datasource",
+          "multi": false,
+          "name": "DS_PROMETHEUS",
+          "options": [],
+          "query": "prometheus",
+          "queryValue": "",
+          "refresh": 1,
+          "regex": "",
+          "skipUrlSync": false,
+          "type": "datasource"
+        },
+        {
+          "current": {
+            "selected": true,
+            "text": "/models/deepseek-coder-6.7b-instruct",
+            "value": "/models/deepseek-coder-6.7b-instruct"
+          },
+          "datasource": {
+            "type": "prometheus",
+            "uid": "${DS_PROMETHEUS}"
+          },
+          "definition": "label_values(model_name)",
+          "hide": 0,
+          "includeAll": false,
+          "label": "model_name",
+          "multi": false,
+          "name": "model_name",
+          "options": [],
+          "query": {
+            "qryType": 1,
+            "query": "label_values(model_name)",
+            "refId": "PrometheusVariableQueryEditor-VariableQuery"
+          },
+          "refresh": 1,
+          "regex": "",
+          "skipUrlSync": false,
+          "sort": 0,
+          "type": "query"
+        }
+      ]
+    },
+    "time": {
+      "from": "now-5m",
+      "to": "now"
+    },
+    "timepicker": {},
+    "timezone": "",
+    "title": "AIBrix Engine Dashboard (vLLM)",
+    "uid": "ce2azhsbhel8gd",
+    "version": 41,
+    "weekStart": ""
+  }
diff --git a/vllm/distributed/kv_transfer/kv_connector/aibrix_offloading_connector.py b/vllm/distributed/kv_transfer/kv_connector/aibrix_offloading_connector.py
new file mode 100644
index 000000000..5520efa29
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_connector/aibrix_offloading_connector.py
@@ -0,0 +1,1160 @@
+# SPDX-License-Identifier: Apache-2.0
+import copy
+import dataclasses
+import enum
+import itertools
+import logging
+import time
+from typing import TYPE_CHECKING, Optional, Union
+
+import torch
+from aibrix_kvcache import (BaseKVCacheManager, GroupAwareKVCacheManager,
+                            KVCacheBlockLayout, KVCacheBlockSpec,
+                            KVCacheConfig, KVCacheMetrics, KVCacheTensorSpec,
+                            ModelSpec, TokenListView)
+from aibrix_kvcache._custom_ops import (reshape_and_cache_multi_layer,
+                                        reshape_and_offload_multi_layer)
+from aibrix_kvcache.common.absl_logging import (getLogger, log_every_n_seconds,
+                                                log_if)
+from aibrix_kvcache.metrics import (MS_BUCKETS, TOKEN_BUCKETS,
+                                    BaseMetricsExporter,
+                                    KVCacheMetricsExporter, Metrics)
+from aibrix_kvcache.profiling import tag_wrapper
+from aibrix_kvcache.utils import perf_timer
+
+from vllm.attention import get_attn_backend
+from vllm.attention.backends.flash_attn import FlashAttentionBackend
+# from vllm.attention.backends.flashinfer import FlashInferBackend
+from vllm.attention.backends.xformers import XFormersBackend
+from vllm.distributed import broadcast_tensor_dict, get_tp_group
+from vllm.distributed.kv_transfer.kv_connector.base import KVConnectorBase
+from vllm.distributed.kv_transfer.kv_transfer_metrics import (
+    KVTransferMetrics, KVTransferMetricsExporter)
+from vllm.model_executor import SamplingMetadata
+from vllm.utils import get_kv_cache_torch_dtype, round_down
+
+if TYPE_CHECKING:
+    from vllm.config import VllmConfig
+    from vllm.sequence import IntermediateTensors
+    from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
+
+logger = getLogger(__name__)
+
+OFFLOADING_CONNECTOR_SKIP_THRESHOLD = 8
+OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS = {
+    FlashAttentionBackend.get_name():
+    KVCacheBlockLayout.LCND,
+    # TODO: support FlashInferBackend.
+    # FlashInferBackend.get_name(): KVCacheBlockLayout.LCND,
+    XFormersBackend.get_name():
+    KVCacheBlockLayout.LCND,
+}
+
+
+class AIBrixOffloadingConnectorComputeMetrics(Metrics):
+    """Compute metrics."""
+
+    num_tokens: list[int] = []
+    op_lat_ms: Optional[list[int]] = None
+
+    def __init__(
+        self,
+        enable_time_measurement: bool = True,
+    ) -> None:
+        self._enable_time_measurement = enable_time_measurement
+        self._init_optionals()
+
+    def _init_optionals(self) -> None:
+        if self._enable_time_measurement:
+            self.op_lat_ms = []
+
+    def add(
+        self,
+        num_tokens: int,
+        lat_ms: int,
+    ) -> None:
+        self.num_tokens.append(num_tokens)
+        if self._enable_time_measurement:
+            self.op_lat_ms.append(lat_ms)
+
+    def reset(self) -> None:
+        self.num_tokens = []
+        self._init_optionals()
+
+    def summary(self) -> str:
+        iter_len = len(self.num_tokens)
+        total_tokens = sum(self.num_tokens)
+        avg_tokens = total_tokens / iter_len if iter_len > 0 else 0
+        summary = f"COMPUTE: Num. of tokens (iter): " \
+                  f"total={total_tokens}, avg={avg_tokens:.2f}"
+        if self._enable_time_measurement:
+            total_lat_ms = sum(self.op_lat_ms)
+            avg_lat_ms = total_lat_ms / iter_len if iter_len > 0 else 0
+            summary += f", Latency (iter, ms): total={total_lat_ms:.2f}, " \
+                       f"avg={avg_lat_ms:.2f}"
+        return summary
+
+
+class AIBrixOffloadingConnectorOpMetrics(Metrics):
+    """Op metrics."""
+
+    class OP(enum.Enum):
+        SEND = enum.auto()
+        RECV = enum.auto()
+
+    num_ops: int = 0
+    total_tokens: int = 0
+    total_sent_or_recved_tokens: int = 0
+    num_prefixes: list[int] = []
+    num_tokens: list[int] = []
+    num_sent_or_recved_tokens: list[int] = []
+    op_lat_ms: Optional[list[int]] = None
+    # tracks the latency of rebuilding model_input
+    rebuild_lat_ms: Optional[list[int]] = None
+
+    def __init__(
+        self,
+        op: OP,
+        enable_time_measurement: bool = True,
+    ) -> None:
+        self._op = op
+        self._enable_time_measurement = enable_time_measurement
+        self._init_optionals()
+
+    def _init_optionals(self) -> None:
+        if self._enable_time_measurement:
+            self.op_lat_ms = []
+            self.rebuild_lat_ms = []
+
+    def add(
+        self,
+        num_prefix: int,
+        num_tokens: int,
+        num_sent_or_recved_tokens: int,
+        lat_ms: int,
+    ) -> None:
+        self.num_ops += 1
+        self.total_tokens += num_tokens
+        self.total_sent_or_recved_tokens += num_sent_or_recved_tokens
+        self.num_sent_or_recved_tokens.append(num_sent_or_recved_tokens)
+        self.num_prefixes.append(num_prefix)
+        self.num_tokens.append(num_tokens)
+        if self._enable_time_measurement:
+            self.op_lat_ms.append(lat_ms)
+
+    def record_rebuild_latency(self, lat_ms: int) -> None:
+        if self._enable_time_measurement:
+            self.rebuild_lat_ms.append(lat_ms)
+
+    def reset(self) -> None:
+        self.num_prefixes = []
+        self.num_tokens = []
+        self.num_sent_or_recved_tokens = []
+        self._init_optionals()
+
+    def summary(self) -> str:
+        iter_len = len(self.num_prefixes)
+        total_prefixes = sum(self.num_prefixes)
+        avg_prefixes = total_prefixes / iter_len if iter_len > 0 else 0
+        total_tokens = sum(self.num_tokens)
+        avg_tokens = total_tokens / iter_len if iter_len > 0 else 0
+        total_sent_or_recved_tokens = sum(self.num_sent_or_recved_tokens)
+        avg_sent_or_recved_tokens = (total_sent_or_recved_tokens /
+                                     iter_len if iter_len > 0 else 0)
+        summary = f"{self._op.name}: Num. of ops: {self.num_ops}, " \
+                  f"Total num. of tokens: {self.total_tokens}, "
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            summary += f"Total num. of sent tokens: " \
+                       f"{self.total_sent_or_recved_tokens}, "
+        else:
+            summary += f"Total num. of received tokens: " \
+                       f"{self.total_sent_or_recved_tokens}, "
+        summary += f"Num. of prefixes (iter): total={total_prefixes}, " \
+                   f"avg={avg_prefixes:.2f}, " \
+                   f"Num. of tokens (iter): total={total_tokens}, " \
+                   f"avg={avg_tokens:.2f}"
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            summary += f", Num. of sent tokens (iter): " \
+                       f"total={total_sent_or_recved_tokens}, " \
+                       f"avg={avg_sent_or_recved_tokens:.2f}"
+        else:
+            summary += f", Num. of received tokens (iter): " \
+                       f"total={total_sent_or_recved_tokens}, " \
+                       f"avg={avg_sent_or_recved_tokens:.2f}"
+        if self._enable_time_measurement:
+            total_lat_ms = sum(self.op_lat_ms)
+            avg_lat_ms = total_lat_ms / iter_len if iter_len > 0 else 0
+            summary += f", Latency (iter, ms): total={total_lat_ms:.2f}, " \
+                       f"avg={avg_lat_ms:.2f}"
+            if len(self.rebuild_lat_ms) > 0:
+                iter_len = len(self.rebuild_lat_ms)
+                total_rebuild_lat_ms = sum(self.rebuild_lat_ms)
+                avg_rebuild_lat_ms = total_rebuild_lat_ms / iter_len \
+                    if iter_len > 0 else 0
+                summary += f", model_input rebuild latency (iter, ms): " \
+                           f"total={total_rebuild_lat_ms:.2f}, " \
+                           f"avg={avg_rebuild_lat_ms:.2f}"
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.RECV:
+            hit_rate = (self.total_sent_or_recved_tokens * 100 /
+                        self.total_tokens) if self.total_tokens > 0 else 0
+            summary += f", Hit rate: {hit_rate:.2f}%"
+        return summary
+
+
+class AIBrixOffloadingConnectorOpMetricsExporter(BaseMetricsExporter):
+    OP_TYPE_LABELNAME = "op_type"
+
+    def __init__(self, *, prefix, labelnames, counter_cls, gauge_cls,
+                 histogram_cls) -> None:
+        labelnames = labelnames or []
+        labelnames.append(self.OP_TYPE_LABELNAME)
+
+        super().__init__(
+            prefix=f"{prefix}ol_connector_",
+            labelnames=labelnames,
+            counter_cls=counter_cls,
+            gauge_cls=gauge_cls,
+            histogram_cls=histogram_cls,
+        )
+
+        self._init_exporter_fields()
+
+    def _init_exporter_fields(self) -> None:
+        self.counter_num_ops = self._counter_cls(
+            name=f"{self._prefix}num_ops",
+            documentation="Cumulative number of operations.",
+            labelnames=self._labelnames)
+        self.histogram_iteration_prefixes = self._histogram_cls(
+            name=f"{self._prefix}iteration_prefixes",
+            documentation="Histogram of number of prefixes per iteration.",
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_tokens",
+            documentation="Histogram of number of tokens per iteration.",
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_sent_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_sent_tokens",
+            documentation=("Histogram of number of sent tokens "
+                           "per iteration."),
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_received_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_received_tokens",
+            documentation=("Histogram of number of received tokens "
+                           "per iteration."),
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_op_lat_ms = self._histogram_cls(
+            name=f"{self._prefix}iteration_op_lat_ms",
+            documentation=("Histogram of operation latencies "
+                           "per iteration in ms."),
+            labelnames=self._labelnames,
+            buckets=MS_BUCKETS)
+        self.histogram_iteration_rebuild_lat_ms = self._histogram_cls(
+            name=f"{self._prefix}iteration_rebuild_lat_ms",
+            documentation=("Histogram of rebuilding model_input latencies "
+                           "per iteration in ms."),
+            labelnames=self._labelnames,
+            buckets=MS_BUCKETS)
+
+    def export(
+        self,
+        labels: dict[str, str],
+        metrics: AIBrixOffloadingConnectorOpMetrics
+        | AIBrixOffloadingConnectorComputeMetrics,
+    ) -> None:
+        labels = labels.copy()
+
+        if isinstance(metrics, AIBrixOffloadingConnectorOpMetrics):
+            labels[self.OP_TYPE_LABELNAME] = metrics._op.name.lower()
+            self._export_op_metrics(labels, metrics)
+        else:
+            labels[self.OP_TYPE_LABELNAME] = "compute"
+            self._export_compute_metrics(labels, metrics)
+
+    def _export_op_metrics(
+        self,
+        labels: dict[str, str],
+        metrics: AIBrixOffloadingConnectorOpMetrics,
+    ) -> None:
+        self._export_counter(self.counter_num_ops, labels,
+                             len(metrics.num_prefixes))
+        self._export_histogram(self.histogram_iteration_prefixes, labels,
+                               metrics.num_prefixes)
+        self._export_histogram(self.histogram_iteration_tokens, labels,
+                               metrics.num_tokens)
+        if metrics._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            self._export_histogram(self.histogram_iteration_sent_tokens,
+                                   labels, metrics.num_sent_or_recved_tokens)
+        else:
+            self._export_histogram(self.histogram_iteration_received_tokens,
+                                   labels, metrics.num_sent_or_recved_tokens)
+        if metrics._enable_time_measurement:
+            self._export_histogram(self.histogram_iteration_op_lat_ms, labels,
+                                   metrics.op_lat_ms)
+            self._export_histogram(self.histogram_iteration_rebuild_lat_ms,
+                                   labels, metrics.rebuild_lat_ms)
+
+    def _export_compute_metrics(
+        self,
+        labels: dict[str, str],
+        metrics: AIBrixOffloadingConnectorComputeMetrics,
+    ) -> None:
+        self._export_histogram(self.histogram_iteration_tokens, labels,
+                               metrics.num_tokens)
+        if metrics._enable_time_measurement:
+            self._export_histogram(self.histogram_iteration_op_lat_ms, labels,
+                                   metrics.op_lat_ms)
+
+
+class AIBrixOffloadingConnectorMetrics(KVTransferMetrics):
+
+    def __init__(self, metrics: KVCacheMetrics) -> None:
+        self._cache_metrics = metrics
+        self._time_measurement_enabled = (
+            self._cache_metrics.time_measurement_enabled)
+        self._compute_metrics = AIBrixOffloadingConnectorComputeMetrics(
+            enable_time_measurement=self._time_measurement_enabled)
+        self._send_metrics = AIBrixOffloadingConnectorOpMetrics(
+            AIBrixOffloadingConnectorOpMetrics.OP.SEND,
+            enable_time_measurement=self._time_measurement_enabled,
+        )
+        self._recv_metrics = AIBrixOffloadingConnectorOpMetrics(
+            AIBrixOffloadingConnectorOpMetrics.OP.RECV,
+            enable_time_measurement=self._time_measurement_enabled,
+        )
+
+    @property
+    def time_measurement_enabled(self) -> bool:
+        return self._time_measurement_enabled
+
+    def reset(self) -> None:
+        self._cache_metrics.reset()
+        self._compute_metrics.reset()
+        self._send_metrics.reset()
+        self._recv_metrics.reset()
+
+    def __str__(self) -> str:
+        return f"AIBrixOffloadingConnector metrics: " \
+               f"{self._compute_metrics.summary()}" \
+               f"\n\t{self._send_metrics.summary()}" \
+               f"\n\t{self._recv_metrics.summary()}" \
+               f"\n\t{self._cache_metrics.summary()}"
+
+
+class AIBrixOffloadingConnectorMetricsExporter(KVTransferMetricsExporter):
+    """Metrics for AIBrixOffloadingConnector."""
+
+    def __init__(self, *, prefix, labelnames, gauge_cls, counter_cls,
+                 histogram_cls):
+        self.kv_cache_metrics_exporter = KVCacheMetricsExporter(
+            prefix=prefix,
+            labelnames=labelnames,
+            gauge_cls=gauge_cls,
+            counter_cls=counter_cls,
+            histogram_cls=histogram_cls,
+        )
+        self.ol_connector_op_metrics_exporter = \
+            AIBrixOffloadingConnectorOpMetricsExporter(
+            prefix=prefix,
+            labelnames=labelnames,
+            gauge_cls=gauge_cls,
+            counter_cls=counter_cls,
+            histogram_cls=histogram_cls,
+        )
+
+    def export(
+        self,
+        *,
+        metrics: KVTransferMetrics,
+        labels: dict[str, str],
+    ):
+        self.kv_cache_metrics_exporter.export(
+            metrics=metrics._cache_metrics,
+            labels=labels,
+        )
+        self.ol_connector_op_metrics_exporter.export(
+            metrics=metrics._compute_metrics,
+            labels=labels,
+        )
+        self.ol_connector_op_metrics_exporter.export(
+            metrics=metrics._send_metrics,
+            labels=labels,
+        )
+        self.ol_connector_op_metrics_exporter.export(
+            metrics=metrics._recv_metrics,
+            labels=labels,
+        )
+
+
+@dataclasses.dataclass
+class AIBrixOffloadingConnectorCachedMeta:
+    context_tokens: list[int] = dataclasses.field(default_factory=list)
+    context_tokens_offset: int = 0
+    context_tokens_view: Optional[TokenListView] = None
+
+    # When chunked prefill is enabled, the query length in each iteration may
+    # not be divisible by block_size. Consider the following example, assuming
+    # recv always returns nothing for simplicity. Suppose query_len = 511:
+    #
+    # - In the first iteration, since no tokens are received, we need to send
+    #   all 511 tokens. However, in AIBrixOffloadingConnector, the length is
+    #   aligned to 496, and the remaining 15 tokens are ignored.
+    # - In the second iteration, query_len is likely 511 again, while
+    #   context_len is 511. To prevent a gap in the storage layer caused by
+    #   skipping 15 tokens in the first iteration, AIBrixOffloadingConnector
+    #   shifts context_len to 496 and attempts to send (15 + 511) tokens.
+    #
+    # To handle this correctly, we need to determine the slot mapping for those
+    # 15 tokens. The actual number varies from 1 to 15 but is always less than
+    # block_size. Therefore, we store the slot mapping of the last block in the
+    # metadata.
+    last_block_slot_mapping: Optional[torch.Tensor] = None
+
+    def __init__(self, seq_len: int) -> None:
+        self.context_tokens = [-1] * seq_len
+
+    def get_context_tokens(self) -> list[int]:
+        return self.context_tokens[:self.context_tokens_offset]
+
+    def get_context_tokens_view(self) -> TokenListView:
+        if self.context_tokens_view is None:
+            self.context_tokens_view = TokenListView(self.get_context_tokens())
+        return self.context_tokens_view
+
+    def extend_context_tokens(self, tokens: list[int]) -> None:
+        offset = self.context_tokens_offset
+        length = len(tokens)
+        self.context_tokens[offset:offset + length] = tokens
+        self.context_tokens_offset += length
+        self.context_tokens_view = None
+
+    def clear_context_tokens(self) -> None:
+        self.context_tokens.clear()
+        self.context_tokens_offset = 0
+        self.context_tokens_view = None
+
+
+class AIBrixOffloadingConnector(KVConnectorBase):
+    """AIBrixOffloadingConnector is a KVConnector that offloads KV caches
+    and hidden states to the kv cache offloading service.
+    """
+
+    def __init__(
+        self,
+        rank: int,
+        local_rank: int,
+        config: "VllmConfig",
+    ):
+        cache_config = config.cache_config
+        model_config = config.model_config
+        parallel_config = config.parallel_config
+
+        tp_size = parallel_config.tensor_parallel_size
+        num_kv_heads = model_config.get_num_kv_heads(parallel_config)
+        head_size = model_config.get_head_size()
+        tp_rank = get_tp_group().rank_in_group
+
+        kv_head_ids = list(
+            range(num_kv_heads * tp_rank, num_kv_heads * (tp_rank + 1)))
+        layer_ids = list(
+            range(*model_config.get_layers_start_end_indices(parallel_config)))
+        num_layers = len(layer_ids)
+
+        block_ntokens = cache_config.block_size
+        block_dtype = get_kv_cache_torch_dtype(cache_config.cache_dtype,
+                                               model_config.dtype)
+
+        kv_cache_dtype = cache_config.cache_dtype
+
+        self.attn_backend = get_attn_backend(
+            model_config.get_head_size(),
+            model_config.dtype,
+            kv_cache_dtype,
+            block_ntokens,
+            model_config.is_attention_free,
+            use_mla=model_config.use_mla,
+        )
+
+        block_spec = KVCacheBlockSpec(
+            block_ntokens=block_ntokens,
+            block_dtype=block_dtype,
+            block_layout=self._get_block_layout(),
+            tensor_spec=KVCacheTensorSpec(
+                heads=kv_head_ids,
+                layers=layer_ids,
+                head_size=head_size,
+            ),
+        )
+
+        config = KVCacheConfig(
+            block_spec=block_spec,
+            model_spec=ModelSpec(
+                model_config.max_model_len,
+                config.scheduler_config.max_num_batched_tokens,
+            ),
+        )
+
+        if parallel_config.tensor_parallel_size == 1:
+            self.cache = BaseKVCacheManager(config=config)
+        else:
+            pg = get_tp_group().cpu_group
+            self.cache = GroupAwareKVCacheManager(config=config,
+                                                  process_group=pg)
+
+        self.rank = tp_rank
+        self.head_size = head_size
+        self.tp_size = tp_size
+        self.num_kv_heads = num_kv_heads
+        self.num_layers = num_layers
+        self.kv_head_ids = kv_head_ids
+        self.layer_ids = layer_ids
+        self.engine_block_ntokens = block_ntokens
+        self.cache_block_ntokens = self.cache.block_size
+        self.block_dtype = block_dtype
+        self.block_shape = block_spec.block_shape
+        self.block_spec = block_spec
+        self.block_layout = block_spec.block_layout
+        self.chunk_size = self.cache.chunk_size
+        self.cache_feature = self.cache.feature
+        self.kv_cache_dtype = kv_cache_dtype
+
+        self._connector_cache: dict[str,
+                                    AIBrixOffloadingConnectorCachedMeta] = {}
+        self._metrics = AIBrixOffloadingConnectorMetrics(self.cache.metrics)
+        # meta to track compute perf
+        self._compute_start_event = torch.cuda.Event(enable_timing=True)
+        self._compute_end_event = torch.cuda.Event(enable_timing=True)
+        self._compute_total_tokens = 0
+
+        self.k_scales: Optional[list[torch.Tensor]] = None
+        self.v_scales: Optional[list[torch.Tensor]] = None
+
+        logger.info(
+            "AIBrixOffloadingConnector is initialized, "
+            "engine_block_ntokens=%d, cache_block_ntokens=%d",
+            self.engine_block_ntokens,
+            self.cache_block_ntokens,
+        )
+
+    @property
+    def metrics(self) -> KVTransferMetrics:
+        return self._metrics
+
+    def get_metrics_exporter_cls(self):
+        return AIBrixOffloadingConnectorMetricsExporter
+
+    def close(self) -> None:
+        if self.cache:
+            self.cache.close()
+            self.cache = None
+
+    def _get_block_layout(self) -> KVCacheBlockLayout:
+
+        if self.attn_backend.get_name() in \
+            OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS:
+            return KVCacheBlockLayout(
+                OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS[
+                    self.attn_backend.get_name()])
+        raise NotImplementedError(
+            f"Only support attn backends in "
+            f"{list(OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS.keys())}. "
+            f"{self.attn_backend.get_name()} is used.")
+
+    def _prepare_request_cache(
+        self,
+        model_input: "ModelInputForGPUWithSamplingMetadata",
+    ) -> None:
+        # remove finished requests
+        for req_id in model_input.finished_requests_ids:
+            if req_id in self._connector_cache:
+                self._connector_cache.pop(req_id)
+
+        # remove decode requests
+        seq_lens = model_input.seq_lens
+        num_prefills = model_input.attn_metadata.num_prefills
+        request_ids = list(model_input.request_ids_to_seq_ids.keys())
+        for seq_idx, _ in enumerate(seq_lens):
+            if seq_idx >= num_prefills:
+                seq_request_id = request_ids[seq_idx]
+                if seq_request_id in self._connector_cache:
+                    self._connector_cache.pop(seq_request_id)
+
+        # add new requests
+        for req_id in model_input.request_ids_to_seq_ids:
+            if req_id not in self._connector_cache:
+                self._connector_cache[req_id] = None
+
+    def _update_request_cache(
+        self,
+        seq_request_id: str,
+        seq_len: int,
+        seq_input_tokens: list[int],
+        seq_slot_mapping: torch.Tensor,
+        kv_transfer_context_tokens: list[int],
+    ) -> None:
+        if self._connector_cache[seq_request_id] is None:
+            self._connector_cache[
+                seq_request_id] = AIBrixOffloadingConnectorCachedMeta(seq_len)
+        seq_request_cache = self._connector_cache[seq_request_id]
+        kv_transfer_context_tokens = kv_transfer_context_tokens or []
+        if seq_len == len(kv_transfer_context_tokens):
+            # when using prefix caching, if all tokens are cached,
+            # we will leave a token in `seq_input_tokens` to avoid
+            # erroneous behavior. In this case, we need to ignore
+            # the token in `seq_input_tokens`.
+            # See `_compute_for_prefix_cache_hit` for more details.
+            seq_input_tokens = []
+        if seq_len != len(seq_request_cache.get_context_tokens()) + len(
+                kv_transfer_context_tokens) + len(seq_input_tokens):
+            # this is a preempted request to be recomputed, let's drop
+            # the context tokens
+            seq_request_cache.clear_context_tokens()
+        if len(kv_transfer_context_tokens) > 0:
+            assert seq_request_cache.context_tokens_offset == 0
+            seq_request_cache.extend_context_tokens(kv_transfer_context_tokens)
+        seq_request_cache.extend_context_tokens(seq_input_tokens)
+        last_block_len = min(self.cache_block_ntokens,
+                             seq_slot_mapping.shape[0])
+        seq_request_cache.last_block_slot_mapping = seq_slot_mapping[
+            -last_block_len:]
+
+    def _get_chunk_slot_mapping(
+        self,
+        seq_request_id: str,
+        seq_slot_mapping: torch.Tensor,
+        offset: int,
+        length: int,
+    ) -> torch.Tensor:
+        if offset < 0:
+            chunk_slot_mapping = seq_slot_mapping[:offset + length]
+            # attach more to slot mapping
+            seq_last_block_slot_mapping = self._connector_cache[
+                seq_request_id].last_block_slot_mapping
+            assert seq_last_block_slot_mapping is not None
+            assert -offset <= seq_last_block_slot_mapping.shape[0]
+            prepend_slot_mapping = seq_last_block_slot_mapping[offset:]
+            chunk_slot_mapping = torch.cat(
+                (prepend_slot_mapping, chunk_slot_mapping))
+        else:
+            chunk_slot_mapping = seq_slot_mapping[offset:offset + length]
+
+        assert chunk_slot_mapping.shape[0] == length
+        return chunk_slot_mapping
+
+    def _ensure_kv_scales(self, model_executable: torch.nn.Module) -> None:
+        if self.k_scales is None:
+            start_layer = model_executable.model.start_layer
+            end_layer = model_executable.model.end_layer
+            layers = model_executable.model.layers[start_layer:end_layer]
+            self.k_scales = [layer.self_attn.attn._k_scale for layer in layers]
+            self.v_scales = [layer.self_attn.attn._v_scale for layer in layers]
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnector",
+        "func": "send_kv_caches_and_hidden_states"
+    })
+    def send_kv_caches_and_hidden_states(
+        self,
+        model_executable: torch.nn.Module,
+        model_input: "ModelInputForGPUWithSamplingMetadata",
+        kv_caches: list[torch.Tensor],
+        hidden_or_intermediate_states: Union[torch.Tensor,
+                                             "IntermediateTensors"],
+    ) -> None:
+        attn_metadata = model_input.attn_metadata
+        kv_transfer_metadata = model_input.kv_transfer_metadata
+
+        if kv_transfer_metadata is None:
+            return
+
+        assert attn_metadata is not None
+
+        self._prepare_request_cache(model_input)
+
+        # only send prompt KV caches
+        if attn_metadata.num_prefills <= 0:
+            return
+
+        self._ensure_kv_scales(model_executable)
+        num_prefills = attn_metadata.num_prefills
+        seq_lens = model_input.seq_lens[:num_prefills]
+        query_lens = model_input.query_lens[:num_prefills]
+        slot_mapping = model_input.attn_metadata.slot_mapping.flatten()
+        start_layer = model_executable.model.start_layer
+        end_layer = model_executable.model.end_layer
+        request_ids = list(model_input.request_ids_to_seq_ids.keys())
+
+        if self._metrics.time_measurement_enabled:
+            self._compute_end_event.record()
+            self._compute_end_event.synchronize()
+            compute_lat_ms = self._compute_start_event.elapsed_time(
+                self._compute_end_event)
+            self._metrics._compute_metrics.add(self._compute_total,
+                                               compute_lat_ms)
+
+        # query_lens contains new KV caches that need to be offloaded
+        for seq_idx, query_len in enumerate(query_lens):
+            start_pos = sum(query_lens[:seq_idx])
+            end_pos = start_pos + query_len
+            seq_request_id = request_ids[seq_idx]
+            seq_context_len = seq_lens[seq_idx] - query_len
+            seq_slot_mapping = slot_mapping[start_pos:end_pos]
+            seq_cached_meta = self._connector_cache[seq_request_id]
+            assert seq_cached_meta is not None
+            seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+            assert seq_all_tokens is not None
+            assert len(seq_all_tokens) == seq_lens[
+                seq_idx], f"{len(seq_all_tokens)}!={seq_lens[seq_idx]}"
+            prompt_len = kv_transfer_metadata.seq_groups[seq_idx].prompt_len
+
+            # align to block boundary
+            aligned_context_len = round_down(seq_context_len,
+                                             self.cache_block_ntokens)
+            actual_query_len = seq_context_len + query_len - aligned_context_len
+            aligned_query_len = round_down(actual_query_len,
+                                           self.cache_block_ntokens)
+
+            # skip if there are not enough tokens to send after alignment
+            if prompt_len == seq_lens[seq_idx]:
+                # If chunked prefill is not enabled or this is the last
+                # chunk, we use a larger skip threshold
+                skip_threshold = OFFLOADING_CONNECTOR_SKIP_THRESHOLD
+            else:
+                # This is an intermediate chunk, only skip if this is not
+                # a full block
+                skip_threshold = 1
+            if aligned_query_len <= skip_threshold * self.engine_block_ntokens:
+                continue
+
+            assert len(
+                seq_all_tokens) >= aligned_context_len + aligned_query_len
+
+            prefix = seq_all_tokens[:aligned_context_len]
+            tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                    aligned_query_len]
+
+            if self._metrics.time_measurement_enabled:
+                start = torch.cuda.Event(enable_timing=True)
+                end = torch.cuda.Event(enable_timing=True)
+                start.record()
+
+            total_sent = 0
+            for (
+                    chunk_prefix,
+                    chunk_tokens,
+                    _,
+                    all,
+            ) in self.cache.cache_chunk_keys(prefix, tokens):
+                # |----------- seq_context_len ---------|- offset -|
+                # |- aligned_context_len -|- shift_len -|
+                #                         |--- n * chunk_size -----|
+                # |----------------- chunk_prefix -----------------|- tokens -|
+                #                         |-------- (n + 1) * chunk_size -----|
+                chunk_size = len(chunk_tokens)
+                offset = len(chunk_prefix) - seq_context_len
+                length = chunk_size
+
+                exists_status = self.cache.exists(chunk_prefix, chunk_tokens)
+                if exists_status.is_ok():
+                    num_existing_tokens = exists_status.value
+                    logger.info(
+                        "Request[id=%s] send(%d) encounters %d existing tokens",
+                        seq_request_id, length, num_existing_tokens)
+                    if chunk_size - num_existing_tokens < \
+                        self.cache_block_ntokens:
+                        continue
+                    else:
+                        # partially exists
+                        offset += num_existing_tokens
+                        length -= num_existing_tokens
+                        new_chunk_prefix_len = (len(chunk_prefix) +
+                                                num_existing_tokens)
+                        chunk_prefix = all[:new_chunk_prefix_len]
+                        chunk_tokens = all[
+                            new_chunk_prefix_len:new_chunk_prefix_len + length]
+
+                # allocate space for KV caches
+                status = self.cache.allocate_for(chunk_prefix, chunk_tokens)
+                if not status.is_ok():
+                    log_every_n_seconds(logger, logging.ERROR,
+                                        "Failed to allocate : %s", 3,
+                                        str(status))
+                    break
+                handle = status.value
+                tensors = handle.to_tensors()
+                length = len(tensors) * self.cache_block_ntokens
+
+                chunk_slot_mapping = self._get_chunk_slot_mapping(
+                    seq_request_id, seq_slot_mapping, offset, length)
+
+                with perf_timer() as get_kernel_offload_dur_ms:
+                    reshape_and_offload_multi_layer(
+                        tensors,
+                        kv_caches[start_layer:end_layer],
+                        chunk_slot_mapping,
+                        self.engine_block_ntokens,
+                        self.kv_cache_dtype,
+                        self.k_scales,
+                        self.v_scales,
+                        self.block_layout.name,
+                    )
+
+                logger.info("Request[id=%s] offloads %d tokens in %.4f ms",
+                            seq_request_id, length,
+                            get_kernel_offload_dur_ms())
+
+                # put KV caches to offloading service
+                status = self.cache.put(chunk_prefix, chunk_tokens[:length],
+                                        handle)
+                if not status.is_ok():
+                    # TODO: notify other ranks in the group to stop sending
+                    # if this is a fatal error
+                    log_every_n_seconds(
+                        logger, logging.ERROR,
+                        "Failed to put to offloading service: %s", 3,
+                        str(status))
+                    break
+
+                put_ntokens = status.get()
+                total_sent += put_ntokens
+                if put_ntokens != length:
+                    break
+
+            log_if(
+                logger,
+                logging.INFO,
+                "Request[id=%s, prompt_len=%d, context_len=%d] sent %d tokens",
+                total_sent > 0,
+                seq_request_id,
+                prompt_len,
+                seq_context_len,
+                total_sent,
+            )
+
+            if self._metrics.time_measurement_enabled:
+                end.record()
+                end.synchronize()
+                lat_ms = start.elapsed_time(end)
+                self._metrics._send_metrics.add(aligned_context_len,
+                                                aligned_query_len, total_sent,
+                                                lat_ms)
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnector",
+        "func": "recv_kv_caches_and_hidden_states"
+    })
+    def recv_kv_caches_and_hidden_states(
+        self,
+        model_executable: torch.nn.Module,
+        model_input: "ModelInputForGPUWithSamplingMetadata",
+        kv_caches: list[torch.Tensor],
+    ) -> tuple[
+            Union[torch.Tensor, "IntermediateTensors"],
+            bool,
+            "ModelInputForGPUWithSamplingMetadata",
+    ]:
+        attn_metadata = model_input.attn_metadata
+        kv_transfer_metadata = model_input.kv_transfer_metadata
+
+        hidden_or_intermediate_states = None
+
+        # TODO:
+        # When using KV cache offloading with chunk-prefill enabled, for
+        # the first `n` chunks (except the last chunk before the decode
+        # phase), we can bypass the model execution if there is a full
+        # hit in the KV cache.
+        bypass_model_exec = False
+
+        if kv_transfer_metadata is None:
+            return hidden_or_intermediate_states, bypass_model_exec, model_input
+
+        assert attn_metadata is not None
+
+        self._compute_total = 0
+        self._prepare_request_cache(model_input)
+
+        # only recv prompt KV caches
+        if attn_metadata.num_prefills <= 0:
+            return hidden_or_intermediate_states, bypass_model_exec, model_input
+
+        self._ensure_kv_scales(model_executable)
+        num_prefills = attn_metadata.num_prefills
+        input_tokens_tensor = model_input.input_tokens
+        seq_lens = model_input.seq_lens[:num_prefills]
+        query_lens = model_input.query_lens[:num_prefills]
+        slot_mapping = model_input.attn_metadata.slot_mapping.flatten()
+        start_layer = model_executable.model.start_layer
+        end_layer = model_executable.model.end_layer
+        request_ids = list(model_input.request_ids_to_seq_ids.keys())
+
+        if kv_transfer_metadata.seq_group_metadata_list is not None:
+            assert len(request_ids) == len(
+                kv_transfer_metadata.seq_group_metadata_list)
+
+        model_config = model_executable.model.config
+        num_kv_heads = int(model_config.num_key_value_heads / self.tp_size)
+        hidden_size = model_config.hidden_size
+        num_attention_heads = model_config.num_attention_heads
+        head_size = int(hidden_size / num_attention_heads)
+
+        assert num_kv_heads == self.num_kv_heads
+        assert head_size == self.head_size
+
+        reused_lens = []
+        rebuild_model_input = False
+        # query_lens contains new KV caches to be received
+        for seq_idx, query_len in enumerate(query_lens):
+            start_pos = sum(query_lens[:seq_idx])
+            end_pos = start_pos + query_len
+            seq_request_id = request_ids[seq_idx]
+            seq_context_len = seq_lens[seq_idx] - query_len
+            seq_input_tokens = (
+                input_tokens_tensor[start_pos:end_pos].cpu().tolist())
+            prompt_len = kv_transfer_metadata.seq_groups[seq_idx].prompt_len
+
+            seq_slot_mapping = slot_mapping[start_pos:end_pos]
+            # will update these lens later if needed
+            reused_lens.append(0)
+
+            # align to block boundary
+            aligned_context_len = round_down(seq_context_len,
+                                             self.cache_block_ntokens)
+            actual_query_len = seq_context_len + query_len - aligned_context_len
+            aligned_query_len = round_down(actual_query_len,
+                                           self.cache_block_ntokens)
+            shift_len = seq_context_len - aligned_context_len
+
+            self._update_request_cache(
+                seq_request_id,
+                seq_lens[seq_idx],
+                seq_input_tokens,
+                seq_slot_mapping,
+                kv_transfer_metadata.seq_groups[seq_idx].context_tokens,
+            )
+
+            # skip if there are not enough tokens to receive after alignment
+            if prompt_len == seq_lens[seq_idx]:
+                # If chunked prefill is not enabled or this is the last
+                # chunk, we use a larger skip threshold
+                skip_threshold = OFFLOADING_CONNECTOR_SKIP_THRESHOLD
+            else:
+                # This is an intermediate chunk, only skip if this is not
+                # a full block
+                skip_threshold = 1
+            if aligned_query_len <= skip_threshold * self.engine_block_ntokens:
+                continue
+
+            seq_cached_meta = self._connector_cache[seq_request_id]
+            assert seq_cached_meta is not None
+            seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+            assert seq_all_tokens is not None
+            assert len(seq_all_tokens) == seq_lens[
+                seq_idx], f"{len(seq_all_tokens)}!={seq_lens[seq_idx]}"
+
+            assert len(
+                seq_all_tokens) >= aligned_context_len + aligned_query_len
+
+            prefix = seq_all_tokens[:aligned_context_len]
+            tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                    aligned_query_len]
+
+            if self._metrics.time_measurement_enabled:
+                start = torch.cuda.Event(enable_timing=True)
+                end = torch.cuda.Event(enable_timing=True)
+                start.record()
+
+            seq_recv_len = 0
+            for (
+                    chunk_prefix,
+                    chunk_tokens,
+                    next_tokens,
+                    _,
+            ) in self.cache.cache_chunk_keys(prefix, tokens):
+                if next_tokens and len(next_tokens) > 0:
+                    # prefetch
+                    self.cache.prefetch(chunk_prefix + chunk_tokens,
+                                        next_tokens)
+
+                # get KV caches from offloading service
+                status = self.cache.acquire(chunk_prefix, chunk_tokens)
+
+                if not status.is_ok():
+                    if not status.is_not_found():
+                        log_every_n_seconds(
+                            logger, logging.ERROR,
+                            "Failed to get from offloading service: %s", 3,
+                            str(status))
+                    break
+
+                num_fetched_tokens, handle = status.value
+                kv_blocks = handle.to_tensors()
+
+                offset = len(chunk_prefix) - seq_context_len
+                length = num_fetched_tokens
+
+                chunk_slot_mapping = self._get_chunk_slot_mapping(
+                    seq_request_id, seq_slot_mapping, offset, length)
+
+                with perf_timer() as get_kernel_onload_dur_ms:
+                    reshape_and_cache_multi_layer(
+                        kv_blocks,
+                        kv_caches[start_layer:end_layer],
+                        chunk_slot_mapping,
+                        self.engine_block_ntokens,
+                        self.kv_cache_dtype,
+                        self.k_scales,
+                        self.v_scales,
+                        self.block_layout.name,
+                    )
+
+                logger.info("Request[id=%s] onloads %d tokens in %.4f ms",
+                            seq_request_id, length, get_kernel_onload_dur_ms())
+
+                # update recv_len
+                seq_recv_len += num_fetched_tokens - shift_len
+                rebuild_model_input = True
+                # reset shift_len
+                shift_len = 0
+
+                # release handle
+                handle.release()
+
+                if num_fetched_tokens < len(chunk_tokens):
+                    # didn't receive all tokens for current chunk, break
+                    break
+
+            reused_lens[-1] = seq_recv_len
+            log_if(
+                logger,
+                logging.INFO,
+                ("Request[id=%s, prompt_len=%d, context_len=%d] "
+                 "reused %d tokens"),
+                seq_recv_len > 0,
+                seq_request_id,
+                prompt_len,
+                seq_context_len,
+                seq_recv_len,
+            )
+
+            if self._metrics.time_measurement_enabled:
+                end.record()
+                end.synchronize()
+                lat_ms = start.elapsed_time(end)
+                self._metrics._recv_metrics.add(aligned_context_len,
+                                                aligned_query_len,
+                                                seq_recv_len, lat_ms)
+
+        if rebuild_model_input:
+            if self._metrics.time_measurement_enabled:
+                start = time.perf_counter()
+            model_input = self._rebuild_model_input(model_input, reused_lens)
+            if self._metrics.time_measurement_enabled:
+                end = time.perf_counter()
+                lat_ms = (end - start) * 1000
+                self._metrics._recv_metrics.record_rebuild_latency(lat_ms)
+
+        if self._metrics.time_measurement_enabled:
+            self._compute_start_event.record()
+            self._compute_total = sum(query_lens) - sum(reused_lens)
+
+        return hidden_or_intermediate_states, bypass_model_exec, model_input
+
+    def _rebuild_model_input(
+        self,
+        model_input: "ModelInputForGPUWithSamplingMetadata",
+        reused_lens: list[int],
+    ) -> "ModelInputForGPUWithSamplingMetadata":
+        seq_group_metadata_list = (
+            model_input.kv_transfer_metadata.seq_group_metadata_list)
+        if seq_group_metadata_list:
+            new_model_input = self._driver_rebuild_model_input(
+                model_input, reused_lens)
+            if self.tp_size > 1:
+                broadcast_data = new_model_input.as_broadcastable_tensor_dict()
+                broadcast_tensor_dict(broadcast_data, src=0)
+        else:
+            # worker
+            runner = model_input.kv_transfer_metadata.runner
+            broadcast_data = broadcast_tensor_dict(src=0)
+            new_model_input = (
+                runner.make_model_input_from_broadcasted_tensor_dict(
+                    broadcast_data))
+
+        # replace fields
+        return dataclasses.replace(
+            model_input,
+            input_tokens=new_model_input.input_tokens,
+            input_positions=new_model_input.input_positions,
+            token_types=new_model_input.token_types,
+            attn_metadata=new_model_input.attn_metadata,
+            seq_lens=new_model_input.seq_lens,
+            query_lens=new_model_input.query_lens,
+            lora_mapping=new_model_input.lora_mapping,
+            lora_requests=new_model_input.lora_requests,
+            multi_modal_kwargs=new_model_input.multi_modal_kwargs,
+            prompt_adapter_mapping=new_model_input.prompt_adapter_mapping,
+            prompt_adapter_requests=new_model_input.prompt_adapter_requests,
+            sampling_metadata=new_model_input.sampling_metadata,
+        )
+
+    def _driver_rebuild_model_input(
+        self,
+        model_input: "ModelInputForGPUWithSamplingMetadata",
+        reused_lens: list[int],
+    ) -> "ModelInputForGPUWithSamplingMetadata":
+        seq_group_metadata_list = (
+            model_input.kv_transfer_metadata.seq_group_metadata_list)
+        runner = model_input.kv_transfer_metadata.runner
+        finished_requests_ids = model_input.finished_requests_ids
+
+        request_ids = list(model_input.request_ids_to_seq_ids.keys())
+        assert len(request_ids) == len(seq_group_metadata_list)
+
+        seq_lens = model_input.seq_lens
+        query_lens = model_input.query_lens
+
+        backup = [None] * len(seq_group_metadata_list)
+        for offset, seq_group_metadata in enumerate(seq_group_metadata_list):
+            if not seq_group_metadata.is_prompt:
+                break
+            if reused_lens[offset] == 0:
+                continue
+            # Prefill has only 1 sequence
+            backup[offset] = seq_group_metadata.computed_block_nums
+            context_len = seq_lens[offset] - query_lens[offset]
+            context_len += reused_lens[offset]
+            num_blocks = context_len // self.engine_block_ntokens
+            seq_group_metadata.computed_block_nums = copy.deepcopy(
+                seq_group_metadata.computed_block_nums)
+            seq_group_metadata.computed_block_nums.extend(
+                itertools.repeat(0, num_blocks))
+
+        new_model_input = runner._prepare_model_input_tensors(
+            seq_group_metadata_list, finished_requests_ids)
+
+        if model_input.sampling_metadata is not None:
+            generators = runner.get_generators(finished_requests_ids)
+            sampling_metadata = SamplingMetadata.prepare(
+                seq_group_metadata_list,
+                new_model_input.seq_lens,
+                new_model_input.query_lens,
+                runner.device,
+                runner.pin_memory,
+                generators,
+                runner.sampling_metadata_cache,
+            )
+            new_model_input = dataclasses.replace(
+                new_model_input, sampling_metadata=sampling_metadata)
+
+        # revert changes on seq_group_metadata_list
+        for offset, seq_group_metadata in enumerate(seq_group_metadata_list):
+            if not seq_group_metadata.is_prompt:
+                break
+            if reused_lens[offset] == 0:
+                continue
+            seq_group_metadata.computed_block_nums = backup[offset]
+
+        return new_model_input
diff --git a/vllm/distributed/kv_transfer/kv_connector/base.py b/vllm/distributed/kv_transfer/kv_connector/base.py
index 181c33925..be54e4186 100644
--- a/vllm/distributed/kv_transfer/kv_connector/base.py
+++ b/vllm/distributed/kv_transfer/kv_connector/base.py
@@ -20,6 +20,9 @@ if TYPE_CHECKING:
     from vllm.config import VllmConfig
     from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
 
+    from ..kv_transfer_metrics import (KVTransferMetrics,
+                                       KVTransferMetricsExporter)
+
 
 class KVConnectorBase(ABC):
     """
@@ -51,6 +54,21 @@ class KVConnectorBase(ABC):
         """
         raise NotImplementedError
 
+    @property
+    def metrics(self) -> 'KVTransferMetrics':
+        """
+        Get the metrics object associated with the connector.
+        Returns:
+            KVTransferMetrics: The metrics object.
+        """
+        return None
+
+    def get_metrics_exporter_cls(self) -> 'KVTransferMetricsExporter':
+        """
+        Get the metrics exporter class associated with the connector.
+        """
+        return None
+
     @abstractmethod
     def send_kv_caches_and_hidden_states(
         self,
diff --git a/vllm/distributed/kv_transfer/kv_connector/factory.py b/vllm/distributed/kv_transfer/kv_connector/factory.py
index 58dfa251c..40f86449f 100644
--- a/vllm/distributed/kv_transfer/kv_connector/factory.py
+++ b/vllm/distributed/kv_transfer/kv_connector/factory.py
@@ -126,3 +126,23 @@ KVConnectorFactory.register_connector(
     "MultiConnector",
     "vllm.distributed.kv_transfer.kv_connector.v1.multi_connector",
     "MultiConnector")
+
+KVConnectorFactory.register_connector(
+    "AIBrixOffloadingConnector",
+    "vllm.distributed.kv_transfer.kv_connector.aibrix_offloading_connector",
+    "AIBrixOffloadingConnector")
+
+KVConnectorFactory.register_connector(
+    "AIBrixOffloadingConnectorV1Type1",
+    "vllm.distributed.kv_transfer.kv_connector.v1"
+    ".aibrix_offloading_connector_type1", "AIBrixOffloadingConnector")
+
+KVConnectorFactory.register_connector(
+    "AIBrixOffloadingConnectorV1Type2",
+    "vllm.distributed.kv_transfer.kv_connector.v1"
+    ".aibrix_offloading_connector_type2", "AIBrixOffloadingConnector")
+
+KVConnectorFactory.register_connector(
+    "AIBrixOffloadingConnectorV1Type3",
+    "vllm.distributed.kv_transfer.kv_connector.v1"
+    ".aibrix_offloading_connector_type3", "AIBrixOffloadingConnector")
diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type1.py b/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type1.py
new file mode 100644
index 000000000..9cce14e18
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type1.py
@@ -0,0 +1,1406 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+import enum
+import logging
+from dataclasses import dataclass, field
+from functools import wraps
+from typing import TYPE_CHECKING, Any, Callable, Optional, TypeVar
+
+import numpy as np
+import torch
+import torch.distributed as dist
+from aibrix_kvcache import (BaseKVCacheManager, GroupAwareKVCacheManager,
+                            KVCacheBlockLayout, KVCacheBlockSpec,
+                            KVCacheConfig, KVCacheMetrics, KVCacheTensorSpec,
+                            ModelSpec, TokenListView)
+from aibrix_kvcache._custom_ops import (reshape_and_cache_multi_layer,
+                                        reshape_and_offload_multi_layer)
+from aibrix_kvcache.common.absl_logging import (getLogger, log_every_n_seconds,
+                                                log_if)
+from aibrix_kvcache.common.cached_pyobject import CachedPyObjectBase
+from aibrix_kvcache.metrics import (MS_BUCKETS, TOKEN_BUCKETS,
+                                    BaseMetricsExporter,
+                                    KVCacheMetricsExporter, Metrics)
+from aibrix_kvcache.profiling import tag_wrapper
+from aibrix_kvcache.utils import perf_timer
+
+import vllm.envs
+from vllm.attention import get_attn_backend
+# from vllm.v1.attention.backends.flashinfer import FlashInferBackend
+# from vllm.v1.attention.backends.flex_attention import FlexAttentionBackend
+# from vllm.v1.attention.backends.triton_attn import TritonAttentionBackend
+from vllm.distributed import (get_tp_group, get_world_group,
+                              init_model_parallel_group)
+from vllm.distributed.kv_transfer.kv_connector.v1.base import (
+    KVConnectorBase_V1, KVConnectorMetadata, KVConnectorRole)
+from vllm.distributed.kv_transfer.kv_transfer_metrics import (
+    KVTransferMetrics, KVTransferMetricsExporter)
+from vllm.utils import get_kv_cache_torch_dtype, round_down, round_up
+from vllm.v1.attention.backends.flash_attn import FlashAttentionBackend
+
+if TYPE_CHECKING:
+    from vllm.attention.backends.abstract import AttentionMetadata
+    from vllm.config import VllmConfig
+    from vllm.forward_context import ForwardContext
+    from vllm.v1.core.kv_cache_manager import KVCacheBlocks
+    from vllm.v1.core.sched.output import SchedulerOutput
+    from vllm.v1.request import Request
+
+logger = getLogger(__name__)
+
+OFFLOADING_CONNECTOR_SKIP_THRESHOLD = 8
+OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS = {
+    FlashAttentionBackend.get_name(): KVCacheBlockLayout.LCND,
+}
+
+T = TypeVar('T')
+
+
+def delegate_to(
+        member_name: str) -> Callable[[Callable[..., T]], Callable[..., T]]:
+    """
+    Decorator that delegates a method call to a member object.
+    
+    Args:
+        member_name: The name of the member attribute to delegate to.
+    """
+
+    def decorator(method: Callable[..., T]) -> Callable[..., T]:
+
+        @wraps(method)
+        def wrapper(self, *args, **kwargs) -> T:
+            member = getattr(self, member_name)
+            assert member is not None, f"{member_name} is not set"
+            member_method = getattr(member, method.__name__)
+            return member_method(*args, **kwargs)
+
+        return wrapper
+
+    return decorator
+
+
+class AIBrixOffloadingConnectorSyncGranularity(enum.Enum):
+    """The sync granularity used by AIBrix offloading connectors.
+    NONE: no synchronization among TP participants.
+    PER_OP: sync up the min num. of tokens fetched by each TP participant after
+            each GET/ACUQUIRE operation.
+    PER_BATCH: sync up the min num. of tokens fetched by each TP participant
+               after each batch.
+    """
+    NONE = enum.auto()
+    PER_OP = enum.auto()
+    PER_BATCH = enum.auto()
+
+
+class AIBrixOffloadingConnectorOpMetrics(Metrics):
+    """Op metrics."""
+
+    class OP(enum.Enum):
+        SEND = enum.auto()
+        RECV = enum.auto()
+
+    def __init__(
+        self,
+        op: OP,
+        enable_time_measurement: bool = True,
+    ) -> None:
+        self.num_ops: int = 0
+        self.total_tokens: int = 0
+        self.total_sent_or_recved_tokens: int = 0
+        self.num_prefixes: list[int] = []
+        self.num_tokens: list[int] = []
+        self.num_sent_or_recved_tokens: list[int] = []
+        self.op_lat_ms: Optional[list[int]] = None
+
+        self._op = op
+        self._enable_time_measurement = enable_time_measurement
+        self._init_optionals()
+
+    def _init_optionals(self) -> None:
+        if self._enable_time_measurement:
+            self.op_lat_ms = []
+
+    def add(
+        self,
+        num_prefix: int,
+        num_tokens: int,
+        num_sent_or_recved_tokens: int,
+        lat_ms: int,
+    ) -> None:
+        self.num_ops += 1
+        self.total_tokens += num_tokens
+        self.total_sent_or_recved_tokens += num_sent_or_recved_tokens
+        self.num_sent_or_recved_tokens.append(num_sent_or_recved_tokens)
+        self.num_prefixes.append(num_prefix)
+        self.num_tokens.append(num_tokens)
+        if self._enable_time_measurement:
+            self.op_lat_ms.append(lat_ms)
+
+    def reset(self) -> None:
+        self.num_prefixes = []
+        self.num_tokens = []
+        self.num_sent_or_recved_tokens = []
+        self._init_optionals()
+
+    def summary(self) -> str:
+        iter_len = len(self.num_prefixes)
+        total_prefixes = sum(self.num_prefixes)
+        avg_prefixes = total_prefixes / iter_len if iter_len > 0 else 0
+        total_tokens = sum(self.num_tokens)
+        avg_tokens = total_tokens / iter_len if iter_len > 0 else 0
+        total_sent_or_recved_tokens = sum(self.num_sent_or_recved_tokens)
+        avg_sent_or_recved_tokens = (total_sent_or_recved_tokens /
+                                     iter_len if iter_len > 0 else 0)
+        summary = f"{self._op.name}: Num. of ops: {self.num_ops}, " \
+                  f"Total num. of tokens: {self.total_tokens}, "
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            summary += f"Total num. of sent tokens: " \
+                       f"{self.total_sent_or_recved_tokens}, "
+        else:
+            summary += f"Total num. of received tokens: " \
+                       f"{self.total_sent_or_recved_tokens}, "
+        summary += f"Num. of prefixes (iter): total={total_prefixes}, " \
+                   f"avg={avg_prefixes:.2f}, " \
+                   f"Num. of tokens (iter): total={total_tokens}, " \
+                   f"avg={avg_tokens:.2f}"
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            summary += f", Num. of sent tokens (iter): " \
+                       f"total={total_sent_or_recved_tokens}, " \
+                       f"avg={avg_sent_or_recved_tokens:.2f}"
+        else:
+            summary += f", Num. of received tokens (iter): " \
+                       f"total={total_sent_or_recved_tokens}, " \
+                       f"avg={avg_sent_or_recved_tokens:.2f}"
+        if self._enable_time_measurement:
+            total_lat_ms = sum(self.op_lat_ms)
+            avg_lat_ms = total_lat_ms / iter_len if iter_len > 0 else 0
+            summary += f", Latency (iter, ms): total={total_lat_ms:.2f}, " \
+                       f"avg={avg_lat_ms:.2f}"
+        if self._op is AIBrixOffloadingConnectorOpMetrics.OP.RECV:
+            hit_rate = (self.total_sent_or_recved_tokens * 100 /
+                        self.total_tokens) if self.total_tokens > 0 else 0
+            summary += f", Hit rate: {hit_rate:.2f}%"
+        return summary
+
+
+class AIBrixOffloadingConnectorOpMetricsExporter(BaseMetricsExporter):
+    OP_TYPE_LABELNAME = "op_type"
+
+    def __init__(self, *, prefix, labelnames, counter_cls, gauge_cls,
+                 histogram_cls) -> None:
+        labelnames = labelnames or []
+        labelnames.append(self.OP_TYPE_LABELNAME)
+
+        super().__init__(
+            prefix=f"{prefix}ol_connector_",
+            labelnames=labelnames,
+            counter_cls=counter_cls,
+            gauge_cls=gauge_cls,
+            histogram_cls=histogram_cls,
+        )
+
+        self._init_exporter_fields()
+
+    def _init_exporter_fields(self) -> None:
+        self.counter_num_ops = self._counter_cls(
+            name=f"{self._prefix}num_ops",
+            documentation="Cumulative number of operations.",
+            labelnames=self._labelnames)
+        self.histogram_iteration_prefixes = self._histogram_cls(
+            name=f"{self._prefix}iteration_prefixes",
+            documentation="Histogram of number of prefixes per iteration.",
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_tokens",
+            documentation="Histogram of number of tokens per iteration.",
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_sent_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_sent_tokens",
+            documentation=("Histogram of number of sent tokens "
+                           "per iteration."),
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_received_tokens = self._histogram_cls(
+            name=f"{self._prefix}iteration_received_tokens",
+            documentation=("Histogram of number of received tokens "
+                           "per iteration."),
+            labelnames=self._labelnames,
+            buckets=TOKEN_BUCKETS)
+        self.histogram_iteration_op_lat_ms = self._histogram_cls(
+            name=f"{self._prefix}iteration_op_lat_ms",
+            documentation=("Histogram of operation latencies "
+                           "per iteration in ms."),
+            labelnames=self._labelnames,
+            buckets=MS_BUCKETS)
+
+    def export(
+        self,
+        labels: dict[str, str],
+        metrics: AIBrixOffloadingConnectorOpMetrics,
+    ) -> None:
+        labels = labels.copy()
+
+        labels[self.OP_TYPE_LABELNAME] = metrics._op.name.lower()
+        self._export_op_metrics(labels, metrics)
+
+    def _export_op_metrics(
+        self,
+        labels: dict[str, str],
+        metrics: AIBrixOffloadingConnectorOpMetrics,
+    ) -> None:
+        self._export_counter(self.counter_num_ops, labels,
+                             len(metrics.num_prefixes))
+        self._export_histogram(self.histogram_iteration_prefixes, labels,
+                               metrics.num_prefixes)
+        self._export_histogram(self.histogram_iteration_tokens, labels,
+                               metrics.num_tokens)
+        if metrics._op is AIBrixOffloadingConnectorOpMetrics.OP.SEND:
+            self._export_histogram(self.histogram_iteration_sent_tokens,
+                                   labels, metrics.num_sent_or_recved_tokens)
+        else:
+            self._export_histogram(self.histogram_iteration_received_tokens,
+                                   labels, metrics.num_sent_or_recved_tokens)
+        if metrics._enable_time_measurement:
+            self._export_histogram(self.histogram_iteration_op_lat_ms, labels,
+                                   metrics.op_lat_ms)
+
+
+class AIBrixOffloadingConnectorMetrics(KVTransferMetrics):
+
+    def __init__(self, metrics: KVCacheMetrics) -> None:
+        self._cache_metrics = metrics
+        self._time_measurement_enabled = (
+            self._cache_metrics.time_measurement_enabled)
+        self._send_metrics = AIBrixOffloadingConnectorOpMetrics(
+            AIBrixOffloadingConnectorOpMetrics.OP.SEND,
+            enable_time_measurement=self._time_measurement_enabled,
+        )
+        self._recv_metrics = AIBrixOffloadingConnectorOpMetrics(
+            AIBrixOffloadingConnectorOpMetrics.OP.RECV,
+            enable_time_measurement=self._time_measurement_enabled,
+        )
+
+    @property
+    def time_measurement_enabled(self) -> bool:
+        return self._time_measurement_enabled
+
+    def reset(self) -> None:
+        self._cache_metrics.reset()
+        self._send_metrics.reset()
+        self._recv_metrics.reset()
+
+    def __str__(self) -> str:
+        return f"AIBrixOffloadingConnector metrics: " \
+               f"{self._send_metrics.summary()}" \
+               f"\n\t{self._recv_metrics.summary()}" \
+               f"\n\t{self._cache_metrics.summary()}"
+
+    # NOTE: Functions `log` and `findCaller` are used as a workaround to
+    # log metrics on the worker side, will be removed once the worker is
+    # able to transfer metrics to the scheduler.
+    def log(self, level, msg, *args) -> None:
+        logger.log(level, str(self))
+        self.reset()
+
+    def findCaller(self) -> tuple[str, int, str, str | None]:
+        return logger.findCaller()
+
+
+class AIBrixOffloadingConnectorMetricsExporter(KVTransferMetricsExporter):
+    """Metrics for AIBrixOffloadingConnector."""
+
+    def __init__(self, *, prefix, labelnames, gauge_cls, counter_cls,
+                 histogram_cls):
+        self.kv_cache_metrics_exporter = KVCacheMetricsExporter(
+            prefix=prefix,
+            labelnames=labelnames,
+            gauge_cls=gauge_cls,
+            counter_cls=counter_cls,
+            histogram_cls=histogram_cls,
+        )
+        self.ol_connector_op_metrics_exporter = \
+            AIBrixOffloadingConnectorOpMetricsExporter(
+            prefix=prefix,
+            labelnames=labelnames,
+            gauge_cls=gauge_cls,
+            counter_cls=counter_cls,
+            histogram_cls=histogram_cls,
+        )
+
+    def export(
+        self,
+        *,
+        metrics: KVTransferMetrics,
+        labels: dict[str, str],
+    ):
+        self.kv_cache_metrics_exporter.export(
+            metrics=metrics._cache_metrics,
+            labels=labels,
+        )
+        self.ol_connector_op_metrics_exporter.export(
+            metrics=metrics._send_metrics,
+            labels=labels,
+        )
+        self.ol_connector_op_metrics_exporter.export(
+            metrics=metrics._recv_metrics,
+            labels=labels,
+        )
+
+
+@dataclass
+class AIBrixOffloadingConnectorCachedMeta:
+    context_tokens: np.ndarray = field(default_factory=lambda: np.array([]))
+    context_tokens_offset: int = 0
+    context_tokens_view: Optional[TokenListView] = None
+
+    context_slot_mapping: Optional[torch.Tensor] = None
+    context_slot_mapping_offset: int = 0
+
+    def __init__(self, prompt_len: int) -> None:
+        self.context_tokens = np.empty(prompt_len, dtype=np.int32)
+        self.context_slot_mapping = torch.empty(
+            prompt_len,
+            dtype=torch.long,
+            device="cuda",
+        )
+
+    def get_context_tokens(self) -> list[int]:
+        return self.context_tokens[:self.context_tokens_offset]
+
+    def get_context_tokens_view(self) -> TokenListView:
+        if self.context_tokens_view is None:
+            self.context_tokens_view = TokenListView(self.get_context_tokens())
+        return self.context_tokens_view
+
+    def extend(
+        self,
+        tokens: tuple[int, list[int]],
+        slot_mapping: Optional[tuple[int, torch.Tensor]],
+    ):
+        if tokens:
+            offset = tokens[0]
+            length = len(tokens[1])
+            self.context_tokens[offset:offset + length] = tokens[1]
+            self.context_tokens_offset = offset + length
+            self.context_tokens_view = None
+
+        if slot_mapping is None:
+            return
+        offset = slot_mapping[0]
+        length = min(
+            slot_mapping[1].shape[0],
+            self.context_slot_mapping.shape[0] - offset,
+        )
+        self.context_slot_mapping[offset : offset + length] = \
+            slot_mapping[1][:length]
+        self.context_slot_mapping_offset = offset + length
+
+
+class AIBrixOffloadingConnectorRequestState(enum.IntEnum):
+    INIT = enum.auto()
+    WAITING_FOR_ALLOC = enum.auto()
+    WAITING_FOR_SEND = enum.auto()
+    WAITING_FOR_RECV = enum.auto()
+    SENDING = enum.auto()
+    RECEIVING = enum.auto()
+
+
+@dataclass
+class AIBrixOffloadingConnectorRequestMetadata(CachedPyObjectBase):
+    req_id: str = ""
+    prompt_len: int = -1
+    context_len: int = -1
+    query_len: int = -1  # num of tokens to send/recv
+    load_len: int = -1  # num of tokens to load
+    # a tuple of offset and
+    # 1. all token ids for a new request
+    # 2. new token ids for a cached request
+    seq_token_ids: tuple[int, list[int]] = field(default_factory=tuple)
+    # a tuple of offset and
+    # 1. all slot mapping for a new request
+    # 2. new slot mapping for a cached request
+    seq_slot_mapping: Optional[tuple[int, torch.Tensor]] = None
+    state: AIBrixOffloadingConnectorRequestState = (
+        AIBrixOffloadingConnectorRequestState.INIT)
+    resumed_from_preemption: bool = False
+
+    def __str__(self) -> str:
+        seq_slot_mapping_off = (
+            "None" if self.seq_slot_mapping is None else \
+                str(self.seq_slot_mapping[0])
+        )
+        seq_slot_mapping_len = (
+            "None" if self.seq_slot_mapping is None else \
+                str(self.seq_slot_mapping[1].shape[0])
+        )
+        seq_token_ids_off = (
+            "None" if not self.seq_token_ids else \
+                str(self.seq_token_ids[0])
+        )
+        seq_token_ids_len = ("None" if not self.seq_token_ids
+                             or len(self.seq_token_ids) < 2 else str(
+                                 len(self.seq_token_ids[1])))
+        return (f"AIBrixOffloadingConnectorRequestMetadata["
+                f"req_id={self.req_id}, "
+                f"prompt_len={self.prompt_len}, "
+                f"context_len={self.context_len}, "
+                f"query_len={self.query_len}, "
+                f"load_len={self.load_len}, "
+                f"offset(seq_token_ids)={seq_token_ids_off}, "
+                f"len(seq_token_ids)={seq_token_ids_len}, "
+                f"offset(seq_slot_mapping)={seq_slot_mapping_off}, "
+                f"len(seq_slot_mapping)={seq_slot_mapping_len}, "
+                f"state={self.state.name}, "
+                f"resumed_from_preemption={self.resumed_from_preemption}]")
+
+    def __repr__(self):
+        return self.__str__()
+
+
+class AIBrixOffloadingConnectorMetadata(KVConnectorMetadata):
+
+    def __init__(self,
+                 requests: dict[str,
+                                AIBrixOffloadingConnectorRequestMetadata]):
+        # Requests that need to load from external kvcache.
+        self.requests = requests
+        self.finished_requests_ids: set[str] = set()
+        self.total_num_scheduled_tokens: int = 0
+        self.side_channel_host: str = ""
+        self.side_channel_port: int = -1
+
+    def __getitem__(self,
+                    key: str) -> AIBrixOffloadingConnectorRequestMetadata:
+        return self.requests[key]
+
+    def __contains__(self, key: str) -> bool:
+        return key in self.requests
+
+    def __iter__(self):
+        return iter(self.requests)
+
+    def __next__(self):
+        return next(self.requests)
+
+    def __len__(self):
+        return len(self.requests)
+
+    def items(self):
+        return self.requests.items()
+
+    def upsert_request(
+        self,
+        request_id: str,
+        **kwargs,
+    ) -> None:
+        self.requests.setdefault(
+            request_id,
+            AIBrixOffloadingConnectorRequestMetadata()).__dict__.update(
+                req_id=request_id, **kwargs)
+
+    def pop_request(
+        self,
+        request_id: str,
+    ) -> AIBrixOffloadingConnectorRequestMetadata | None:
+        return self.requests.pop(request_id, None)
+
+    def finish_request(self, request_id: str) -> None:
+        self.finished_requests_ids.add(request_id)
+        self.pop_request(request_id)
+
+    def get(self, predicate: Callable) -> "AIBrixOffloadingConnectorMetadata":
+        requests = {k: v for k, v in self.requests.items() if predicate(v)}
+        return AIBrixOffloadingConnectorMetadata(requests=requests)
+
+    def filter_requests(self, predicate: Callable) -> None:
+        self.requests = {
+            k: v
+            for k, v in self.requests.items() if not predicate(v)
+        }
+
+    def extend(self, other: "AIBrixOffloadingConnectorMetadata") -> None:
+        self.requests.update(other.requests)
+
+    def clear(self) -> None:
+        self.requests.clear()
+        self.finished_requests_ids.clear()
+
+    def __str__(self) -> str:
+        return f"AIBrixOffloadingConnectorMetadata: {self.__dict__}"
+
+
+class AIBrixOffloadingConnectorScheduler:
+
+    def __init__(self, config: "VllmConfig"):
+        self.kv_role = config.kv_transfer_config.kv_role
+        self.engine_block_ntokens = config.cache_config.block_size
+
+        self._scheduler_meta = AIBrixOffloadingConnectorMetadata({})
+
+    def build_connector_meta(
+            self, scheduler_output: "SchedulerOutput") -> KVConnectorMetadata:
+        # 1. remove finished requests
+        for req_id in scheduler_output.finished_req_ids:
+            self._scheduler_meta.pop_request(req_id)
+
+        # 2. new requests
+        for req in scheduler_output.scheduled_new_reqs:
+            req_id = req.req_id
+
+            prompt_len = len(req.prompt_token_ids)
+            context_len = req.num_computed_tokens
+            query_len = scheduler_output.num_scheduled_tokens[req_id]
+            seq_len = min(context_len + query_len, prompt_len)
+
+            if context_len >= prompt_len:
+                continue
+
+            (block_ids, ) = req.block_ids
+            slot_mapping = self._block_ids_to_slot_mapping(block_ids)
+
+            self._scheduler_meta.upsert_request(
+                req_id,
+                prompt_len=prompt_len,
+                context_len=context_len,
+                query_len=query_len,
+                seq_token_ids=(0, req.prompt_token_ids[:seq_len]),
+                seq_slot_mapping=(0, slot_mapping),
+                state=AIBrixOffloadingConnectorRequestState.WAITING_FOR_RECV,
+            )
+
+        # 3. cached requests
+        for req in scheduler_output.scheduled_cached_reqs:
+            req_id = req.req_id
+
+            if req_id not in self._scheduler_meta:
+                continue
+
+            req_meta = self._scheduler_meta[req_id]
+
+            prompt_len = req_meta.prompt_len
+            context_len = min(req.num_computed_tokens, prompt_len)
+            query_len = min(
+                scheduler_output.num_scheduled_tokens[req_id],
+                prompt_len - context_len,
+            )
+            seq_len = min(context_len + query_len, prompt_len)
+
+            if context_len >= prompt_len:
+                continue
+
+            if req.resumed_from_preemption:
+                logger.debug(
+                    "Got preempt Request[id=%s, context_len=%d, query_len=%d]",
+                    req_id,
+                    context_len,
+                    query_len,
+                )
+                (block_ids, ) = req.new_block_ids
+                seq_token_ids = (
+                    context_len,
+                    req.new_token_ids[:seq_len - context_len],
+                )
+                seq_slot_mapping = (0,
+                                    self._block_ids_to_slot_mapping(block_ids))
+            else:
+                (block_ids, ) = req.new_block_ids
+                seq_token_ids = (context_len, req.new_token_ids)
+                seq_slot_mapping = (
+                    round_up(context_len, self.engine_block_ntokens),
+                    self._block_ids_to_slot_mapping(block_ids),
+                )
+
+            self._scheduler_meta.upsert_request(
+                req_id,
+                prompt_len=prompt_len,
+                context_len=context_len,
+                query_len=query_len,
+                seq_token_ids=seq_token_ids,
+                seq_slot_mapping=seq_slot_mapping,
+                state=AIBrixOffloadingConnectorRequestState.WAITING_FOR_RECV,
+                resumed_from_preemption=req.resumed_from_preemption,
+            )
+
+        # 4. keep requests that are in the WAITING_FOR_RECV state
+        meta = self._scheduler_meta.get(lambda req: req.state == \
+                AIBrixOffloadingConnectorRequestState.WAITING_FOR_RECV
+            )
+
+        logger.debug("SCHEDULER: build_connector_meta, meta=%s", meta.__dict__)
+
+        # 5. update scheduled requests
+        for req_id in meta:
+            self._scheduler_meta.upsert_request(
+                req_id,
+                state=AIBrixOffloadingConnectorRequestState.RECEIVING,
+            )
+
+        # 6. attach finished requests
+        meta.finished_requests_ids = self._scheduler_meta.finished_requests_ids
+        self._scheduler_meta.finished_requests_ids = set()
+
+        logger.debug(
+            "Num. of scheduled requests: %s",
+            len(
+                self._scheduler_meta.get(lambda req: req.state == \
+                    AIBrixOffloadingConnectorRequestState.RECEIVING
+                )),
+        )
+        return meta
+
+    def request_finished(
+        self,
+        request: "Request",
+        block_ids: list[int],
+    ) -> tuple[bool, Optional[dict[str, Any]]]:
+        req_id = request.request_id
+        logger.debug("SCHEDULER: Request[id=%s] finished", req_id)
+
+        self._scheduler_meta.finish_request(req_id)
+        return False, None
+
+    def _block_ids_to_slot_mapping(self, block_ids: list[int]) -> torch.Tensor:
+        block_ids_tensor = torch.tensor(block_ids)
+        num_blocks = block_ids_tensor.shape[0]
+        block_offsets = torch.arange(0, self.engine_block_ntokens)
+        slot_mapping = block_offsets.reshape((1, self.engine_block_ntokens)) + \
+                block_ids_tensor.reshape((num_blocks, 1)) * \
+                    self.engine_block_ntokens
+        return slot_mapping.flatten()
+
+
+class AIBrixOffloadingConnectorWorker:
+    """AIBrixOffloadingConnectorWorker carries out the data-plane operations.
+    """
+
+    def __init__(self, config: "VllmConfig"):
+        self._init_worker(config)
+
+    def _init_worker(
+        self,
+        config: "VllmConfig",
+        max_num_batched_tokens: int = -1,
+        tp_aware: bool = True,
+        multi_threaded: bool = False,
+    ):
+        cache_config = config.cache_config
+        model_config = config.model_config
+        parallel_config = config.parallel_config
+
+        tp_size = parallel_config.tensor_parallel_size
+        num_kv_heads = model_config.get_num_kv_heads(parallel_config)
+        head_size = model_config.get_head_size()
+
+        rank = get_tp_group().rank_in_group
+        kv_head_ids = list(
+            range(num_kv_heads * rank, num_kv_heads * (rank + 1)))
+        layer_ids = list(
+            range(*model_config.get_layers_start_end_indices(parallel_config)))
+        num_layers = len(layer_ids)
+
+        block_ntokens = cache_config.block_size
+        block_dtype = get_kv_cache_torch_dtype(cache_config.cache_dtype,
+                                               model_config.dtype)
+
+        kv_cache_dtype = cache_config.cache_dtype
+
+        self.attn_backend = get_attn_backend(
+            model_config.get_head_size(),
+            model_config.dtype,
+            kv_cache_dtype,
+            block_ntokens,
+            model_config.is_attention_free,
+            use_mla=model_config.use_mla,
+        )
+
+        block_spec = KVCacheBlockSpec(
+            block_ntokens=block_ntokens,
+            block_dtype=block_dtype,
+            block_layout=self._get_block_layout(),
+            tensor_spec=KVCacheTensorSpec(
+                heads=kv_head_ids,
+                layers=layer_ids,
+                head_size=head_size,
+            ),
+        )
+
+        kv_config = KVCacheConfig(
+            block_spec=block_spec,
+            model_spec=ModelSpec(
+                model_config.max_model_len,
+                max_num_batched_tokens,
+            ),
+            multi_threaded=multi_threaded,
+        )
+
+        self.kv_group: dist.ProcessGroup | None = None
+        if parallel_config.tensor_parallel_size == 1 or not tp_aware:
+            self.cache = BaseKVCacheManager(config=kv_config)
+        else:
+            backend = torch.distributed.get_backend(get_world_group()\
+                .device_group)
+            world_size = parallel_config.world_size
+            dp_size = parallel_config.data_parallel_size
+            pp_size = parallel_config.pipeline_parallel_size
+            # the layout order is: ExternalDP x DP x PP x TP
+            # ExternalDP is the data parallel group that is not part of the
+            # model, every dp rank can generate independently (in verl
+            # integration).
+            # DP is the data parallel group that is part of the model,
+            # all the ranks in the same DP group should generate simultaneously,
+            # i.e. the `generate` call in the same DP group should be called
+            # together, otherwise it will cause deadlock.
+            # to get group_ranks for each dimension, transpose that dimension to
+            # the last dimension, then reshape to 2D, then unbind the last
+            # dimension
+            all_ranks = torch.arange(world_size).reshape(
+                -1, dp_size, pp_size, tp_size)
+
+            # Build the kv model-parallel groups.
+            group_ranks = all_ranks.view(-1, tp_size).unbind(0)
+            group_ranks = [x.tolist() for x in group_ranks]
+
+            kv_group = init_model_parallel_group(
+                group_ranks,
+                get_world_group().local_rank,
+                backend,
+                group_name="kvcache",
+            )
+            assert rank == kv_group.rank_in_group
+
+            sync_granularity = vllm.envs.VLLM_AIBRIX_SYNC_GRANULARITY
+            if (AIBrixOffloadingConnectorSyncGranularity.NONE.name ==
+                    sync_granularity):
+                self.cache = BaseKVCacheManager(config=kv_config)
+            elif (AIBrixOffloadingConnectorSyncGranularity.PER_OP.name ==
+                  sync_granularity):
+                self.cache = GroupAwareKVCacheManager(
+                    config=kv_config, process_group=kv_group.cpu_group)
+            elif (AIBrixOffloadingConnectorSyncGranularity.PER_BATCH.name ==
+                  sync_granularity):
+                self.cache = BaseKVCacheManager(config=kv_config)
+                self.kv_group = kv_group.cpu_group
+                self._coll_tensor = torch.empty(
+                    (config.scheduler_config.max_num_seqs * 3),
+                    dtype=torch.int32,
+                )
+            else:
+                raise ValueError(
+                    f"Unknown sync granularity {sync_granularity}")
+
+        self.rank = rank
+        self.head_size = head_size
+        self.tp_size = tp_size
+        self.num_kv_heads = num_kv_heads
+        self.num_layers = num_layers
+        self.kv_head_ids = kv_head_ids
+        self.layer_ids = layer_ids
+        self.engine_block_ntokens = block_ntokens
+        self.cache_block_ntokens = self.cache.block_size
+        self.block_dtype = block_dtype
+        self.block_shape = block_spec.block_shape
+        self.block_spec = block_spec
+        self.block_layout = block_spec.block_layout
+        self.chunk_size = self.cache.chunk_size
+        self.cache_feature = self.cache.feature
+        self.kv_cache_dtype = kv_cache_dtype
+
+        # KV caches and kv scales will be init'ed later
+        self.no_compile_layers = config.compilation_config.\
+            static_forward_context
+        self.kv_caches: dict[str, torch.Tensor] | None = None
+        self.layers_kv_caches: list[torch.Tensor] | None = None
+        self.k_scales: list[torch.Tensor] | None = None
+        self.v_scales: list[torch.Tensor] | None = None
+
+        self._meta_cache: dict[str, AIBrixOffloadingConnectorCachedMeta] = {}
+        # metrics
+        self._metrics = AIBrixOffloadingConnectorMetrics(self.cache.metrics)
+        logger.info(
+            "AIBrixOffloadingConnector is initialized, "
+            "engine_block_ntokens=%d, cache_block_ntokens=%d",
+            self.engine_block_ntokens,
+            self.cache_block_ntokens,
+        )
+
+    def __del__(self) -> None:
+        if getattr(self, "cache", None) is not None:
+            self.cache.close()
+            self.cache = None
+
+    def _get_block_layout(self) -> KVCacheBlockLayout:
+
+        if self.attn_backend.get_name() in \
+            OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS:
+            return KVCacheBlockLayout(
+                OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS[
+                    self.attn_backend.get_name()])
+        raise NotImplementedError(
+            f"Only support attn backends in "
+            f"{list(OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS.keys())}. "
+            f"{self.attn_backend.get_name()} is used.")
+
+    def _update_meta_cache(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+    ) -> None:
+        # remove finished requests
+        for req_id in metadata.finished_requests_ids:
+            if req_id in self._meta_cache:
+                self._meta_cache.pop(req_id)
+
+        for req_id, meta in metadata.items():
+            if req_id not in self._meta_cache or meta.resumed_from_preemption:
+                self._meta_cache[req_id] = AIBrixOffloadingConnectorCachedMeta(
+                    meta.prompt_len)
+
+            seq_meta_cache = self._meta_cache[req_id]
+            seq_meta_cache.extend(meta.seq_token_ids, meta.seq_slot_mapping)
+
+    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]) -> None:
+        self.kv_caches = kv_caches
+        layer_names = self.no_compile_layers.keys()
+        self.layers_kv_caches = [
+            self.kv_caches[layer_name] for layer_name in layer_names
+        ]
+        self.layer_name_idx_mapping = {
+            layer_name: idx
+            for idx, layer_name in enumerate(layer_names)
+        }
+        layers = self.no_compile_layers.values()
+        self.k_scales = [layer._k_scale for layer in layers]
+        self.v_scales = [layer._v_scale for layer in layers]
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnectorV1Type1",
+        "func": "start_load_kv_before_update"
+    })
+    def start_load_kv_before_update(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+    ) -> dict[str, int]:
+        self._update_meta_cache(metadata)
+
+        stats = {}
+        for seq_request_id, seq_request_meta in metadata.items():
+            num_fetched_tokens = self._recv_kv_sync_impl(seq_request_meta)
+            stats[seq_request_id] = num_fetched_tokens
+
+        if len(stats) > 0 and self.kv_group is not None:
+            for idx, (seq_request_id, seq_request_meta) in \
+                enumerate(metadata.items()):
+                self._coll_tensor[idx] = stats.get(seq_request_id, 0)
+            dist.all_reduce(self._coll_tensor[:idx], dist.ReduceOp.MIN,
+                            self.kv_group)
+
+            for idx, (seq_request_id, seq_request_meta) in \
+                enumerate(metadata.items()):
+                if self._coll_tensor[idx] > 0:
+                    stats[seq_request_id] = self._coll_tensor[idx].item()
+                else:
+                    stats.pop(seq_request_id, None)
+
+        for seq_request_id, num_fetched_tokens in stats.items():
+            seq_request_meta = metadata[seq_request_id]
+            # update seq_request_meta
+            seq_request_meta.query_len -= num_fetched_tokens
+            seq_request_meta.context_len += num_fetched_tokens
+
+            seq_request_meta.state = \
+                AIBrixOffloadingConnectorRequestState.WAITING_FOR_SEND
+
+        return stats
+
+    def _recv_kv_sync_impl(
+        self,
+        seq_request_meta: AIBrixOffloadingConnectorRequestMetadata,
+    ) -> int:
+        logger.debug("_recv_kv_sync_impl: %s", seq_request_meta)
+        seq_request_id = seq_request_meta.req_id
+        seq_cached_meta = self._meta_cache[seq_request_id]
+        seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+        assert seq_all_tokens is not None, "seq_all_tokens is None"
+        seq_context_len = seq_request_meta.context_len
+
+        prompt_len = seq_request_meta.prompt_len
+        query_len = seq_request_meta.query_len
+
+        # align to block boundary
+        aligned_context_len = round_down(seq_context_len,
+                                         self.cache_block_ntokens)
+        actual_query_len = seq_context_len + query_len - aligned_context_len
+        aligned_query_len = round_down(actual_query_len,
+                                       self.cache_block_ntokens)
+        shift_len = seq_context_len - aligned_context_len
+
+        assert prompt_len >= aligned_context_len + aligned_query_len, \
+            f"{prompt_len}<{aligned_context_len}+{aligned_query_len}"
+
+        if aligned_query_len < OFFLOADING_CONNECTOR_SKIP_THRESHOLD * \
+                self.engine_block_ntokens:
+            logger.debug(
+                "Skip Request[id=%s, context_len=%d, query_len=%d]",
+                seq_request_id,
+                aligned_context_len,
+                aligned_query_len,
+            )
+            return 0
+
+        prefix = seq_all_tokens[:aligned_context_len]
+        tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                aligned_query_len]
+
+        if self._metrics.time_measurement_enabled:
+            start = torch.cuda.Event(enable_timing=True)
+            end = torch.cuda.Event(enable_timing=True)
+            start.record()
+
+        seq_recv_len = 0
+        for (
+                chunk_prefix,
+                chunk_tokens,
+                next_tokens,
+                _,
+        ) in self.cache.cache_chunk_keys(prefix, tokens):
+            if next_tokens and len(next_tokens) > 0:
+                # prefetch
+                self.cache.prefetch(chunk_prefix + chunk_tokens, next_tokens)
+
+            # get KV caches from offloading service
+            status = self.cache.acquire(chunk_prefix, chunk_tokens)
+
+            if not status.is_ok():
+                if not status.is_not_found():
+                    log_every_n_seconds(
+                        logger,
+                        logging.ERROR,
+                        "Failed to get from offloading service: %s",
+                        3,
+                        str(status),
+                    )
+                break
+
+            num_fetched_tokens, handle = status.value
+            kv_blocks = handle.to_tensors()
+
+            offset = len(chunk_prefix)
+            length = num_fetched_tokens
+
+            chunk_slot_mapping = seq_cached_meta.context_slot_mapping[
+                offset:offset + length]
+
+            with perf_timer() as get_kernel_onload_dur_ms:
+                reshape_and_cache_multi_layer(
+                    kv_blocks,
+                    self.layers_kv_caches,
+                    chunk_slot_mapping,
+                    self.engine_block_ntokens,
+                    self.kv_cache_dtype,
+                    self.k_scales,
+                    self.v_scales,
+                    self.block_layout.name,
+                )
+
+            logger.info(
+                "Request[id=%s] onloads %d tokens in %.4f ms",
+                seq_request_id,
+                length,
+                get_kernel_onload_dur_ms(),
+            )
+
+            # update recv_len
+            seq_recv_len += num_fetched_tokens - shift_len
+            # reset shift_len
+            shift_len = 0
+
+            # release handle
+            handle.release()
+
+            if num_fetched_tokens < len(chunk_tokens):
+                # didn't receive all tokens for current chunk, break
+                break
+
+        log_if(
+            logger,
+            logging.INFO,
+            "Request[id=%s, prompt_len=%d, context_len=%d] reused %d tokens",
+            seq_recv_len > 0,
+            seq_request_id,
+            prompt_len,
+            seq_context_len,
+            seq_recv_len,
+        )
+
+        if self._metrics.time_measurement_enabled:
+            end.record()
+            end.synchronize()
+            lat_ms = start.elapsed_time(end)
+            self._metrics._recv_metrics.add(aligned_context_len,
+                                            aligned_query_len, seq_recv_len,
+                                            lat_ms)
+
+        return seq_recv_len
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnectorV1Type1",
+        "func": "wait_for_save"
+    })
+    def wait_for_save(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+    ) -> None:
+        assert self.layers_kv_caches is not None, "layers_kv_caches is None"
+
+        for seq_request_id, seq_request_meta in metadata.items():
+            if seq_request_meta.query_len == 0:
+                continue
+            self._send_kv_sync_impl(seq_request_meta)
+
+        if self._metrics.time_measurement_enabled:
+            log_every_n_seconds(self._metrics, logging.INFO, "UNUSED", 10)
+
+    def _send_kv_sync_impl(
+        self,
+        seq_request_meta: AIBrixOffloadingConnectorRequestMetadata,
+    ) -> None:
+        logger.debug("_send_kv_sync_impl: %s", seq_request_meta)
+        seq_request_id = seq_request_meta.req_id
+        seq_context_len = seq_request_meta.context_len
+        seq_cached_meta = self._meta_cache[seq_request_id]
+        seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+        assert seq_all_tokens is not None, "seq_all_tokens is None"
+        prompt_len = seq_request_meta.prompt_len
+        query_len = seq_request_meta.query_len
+
+        # align to block boundary
+        aligned_context_len = round_down(seq_context_len,
+                                         self.cache_block_ntokens)
+        actual_query_len = seq_context_len + query_len - aligned_context_len
+        aligned_query_len = round_down(actual_query_len,
+                                       self.cache_block_ntokens)
+
+        assert prompt_len >= aligned_context_len + aligned_query_len, \
+            f"{prompt_len}<{aligned_context_len}+{aligned_query_len}"
+
+        prefix = seq_all_tokens[:aligned_context_len]
+        tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                aligned_query_len]
+
+        if self._metrics.time_measurement_enabled:
+            start = torch.cuda.Event(enable_timing=True)
+            end = torch.cuda.Event(enable_timing=True)
+            start.record()
+
+        total_sent = 0
+        for (
+                chunk_prefix,
+                chunk_tokens,
+                _,
+                all,
+        ) in self.cache.cache_chunk_keys(prefix, tokens):
+            # |----------- seq_context_len ---------|- offset -|
+            # |- aligned_context_len -|- shift_len -|
+            #                         |--- n * chunk_size -----|
+            # |----------------- chunk_prefix -----------------|- tokens -|
+            #                         |-------- (n + 1) * chunk_size -----|
+            chunk_size = len(chunk_tokens)
+            offset = len(chunk_prefix)
+            length = chunk_size
+
+            exists_status = self.cache.exists(chunk_prefix, chunk_tokens)
+            if exists_status.is_ok():
+                num_existing_tokens = exists_status.value
+                logger.info(
+                    "Request[id=%s] send(%d) encounters %d existing tokens",
+                    seq_request_id, length, num_existing_tokens)
+                if chunk_size - num_existing_tokens < self.cache_block_ntokens:
+                    continue
+                else:
+                    # partially exists
+                    offset += num_existing_tokens
+                    length -= num_existing_tokens
+                    new_chunk_prefix_len = len(
+                        chunk_prefix) + num_existing_tokens
+                    chunk_prefix = all[:new_chunk_prefix_len]
+                    chunk_tokens = all[
+                        new_chunk_prefix_len:new_chunk_prefix_len + length]
+
+            # allocate space for KV caches
+            status = self.cache.allocate_for(chunk_prefix, chunk_tokens)
+            if not status.is_ok():
+                log_every_n_seconds(logger, logging.ERROR,
+                                    "Failed to allocate : %s", 3, str(status))
+                break
+            handle = status.value
+            tensors = handle.to_tensors()
+            length = len(tensors) * self.cache_block_ntokens
+
+            chunk_slot_mapping = seq_cached_meta.context_slot_mapping[
+                offset:offset + length]
+
+            with perf_timer() as get_kernel_offload_dur_ms:
+                reshape_and_offload_multi_layer(
+                    tensors,
+                    self.layers_kv_caches,
+                    chunk_slot_mapping,
+                    self.engine_block_ntokens,
+                    self.kv_cache_dtype,
+                    self.k_scales,
+                    self.v_scales,
+                    self.block_layout.name,
+                )
+
+            logger.info("Request[id=%s] offloads %d tokens in %.4f ms",
+                        seq_request_id, length, get_kernel_offload_dur_ms())
+
+            # put KV caches to offloading service
+            status = self.cache.put(chunk_prefix, chunk_tokens[:length],
+                                    handle)
+            if not status.is_ok():
+                # TODO: notify other ranks in the group to stop sending
+                # if this is a fatal error
+                log_every_n_seconds(logger, logging.ERROR,
+                                    "Failed to put to offloading service: %s",
+                                    3, str(status))
+                break
+
+            put_ntokens = status.get()
+            total_sent += put_ntokens
+            if put_ntokens != length:
+                break
+
+        log_if(
+            logger,
+            logging.INFO,
+            "Request[id=%s, prompt_len=%d, context_len=%d] sent %d tokens",
+            total_sent > 0,
+            seq_request_id,
+            prompt_len,
+            seq_context_len,
+            total_sent,
+        )
+
+        if self._metrics.time_measurement_enabled:
+            end.record()
+            end.synchronize()
+            lat_ms = start.elapsed_time(end)
+            self._metrics._send_metrics.add(aligned_context_len,
+                                            aligned_query_len, total_sent,
+                                            lat_ms)
+
+
+class AIBrixOffloadingConnector(KVConnectorBase_V1):
+    """AIBrixOffloadingConnector is a KVConnector that offloads KV caches
+    to the kv cache offloading service.
+    """
+
+    def __init__(self, config: "VllmConfig", role: KVConnectorRole):
+        super().__init__(vllm_config=config, role=role)
+
+        self.connector_scheduler: Optional[
+            AIBrixOffloadingConnectorScheduler] = None
+
+        self.connector_worker: Optional[AIBrixOffloadingConnectorWorker] = None
+        if role == KVConnectorRole.SCHEDULER:
+            self.connector_scheduler = AIBrixOffloadingConnectorScheduler(
+                config)
+        elif role == KVConnectorRole.WORKER:
+            self.connector_worker = AIBrixOffloadingConnectorWorker(config)
+
+    # ==============================
+    # Worker-side methods
+    # ==============================
+
+    @delegate_to("connector_worker")
+    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):
+        """
+        Initialize with the KV caches. Useful for pre-registering the
+        KV Caches in the KVConnector (e.g. for NIXL).
+
+        Args: kv_caches:
+            dictionary of layer names, kv cache
+        """
+        pass
+
+    def start_load_kv_before_update(self, **kwargs) -> dict[str, int]:
+        """
+        Start loading the KV cache from the connector to vLLM's paged
+        KV buffer before gpu runner updating its states.
+
+        Args:
+            **kwargs: additional arguments for the load operation
+
+        Returns:
+            dict[str, int]: a dictionary of request ids and the number of
+            tokens loaded for each request.
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        return self.connector_worker.start_load_kv_before_update(
+            self._connector_metadata)
+
+    def start_load_kv(self, forward_context: "ForwardContext",
+                      **kwargs) -> None:
+        """
+        Start loading the KV cache from the connector to vLLM's paged
+        KV buffer. This is called from the forward context before the
+        forward pass to enable async loading during model execution.
+
+        Args:
+            forward_context (ForwardContext): the forward context.
+            **kwargs: additional arguments for the load operation
+
+        Note:
+            The number of elements in kv_caches and layer_names should be 
+            the same.
+            
+        """
+        return
+
+    def wait_for_layer_load(self, layer_name: str) -> None:
+        """
+        Block until the KV for a specific layer is loaded into vLLM's
+        paged buffer. This is called from within attention layer to ensure
+        async copying from start_load_kv is complete.
+        
+        This interface will be useful for layer-by-layer pipelining.
+
+        Args:
+            layer_name: the name of that layer
+        """
+        """Not supported yet"""
+        pass
+
+    def save_kv_layer(self, layer_name: str, kv_layer: torch.Tensor,
+                      attn_metadata: "AttentionMetadata", **kwargs) -> None:
+        """
+        Start saving a layer of KV cache from vLLM's paged buffer 
+        to the connector. This is called from within attention layer to
+        enable async copying during execution.
+
+        Args:
+            layer_name (str): the name of the layer.
+            kv_layer (torch.Tensor): the paged KV buffer of the current 
+                layer in vLLM.
+            attn_metadata (AttentionMetadata): the attention metadata.
+            **kwargs: additional arguments for the save operation.
+        """
+        """Not supported yet"""
+        pass
+
+    def wait_for_save(self) -> None:
+        """
+        Block until all the save operations is done. This is called
+        as the forward context exits to ensure that the async saving
+        from save_kv_layer is complete before finishing the forward.
+
+        This prevents overwrites of paged KV buffer before saving done.
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        self.connector_worker.wait_for_save(self._connector_metadata)
+
+    def get_finished(
+        self, finished_req_ids: set[str]
+    ) -> tuple[Optional[set[str]], Optional[set[str | tuple[str, int]]]]:
+        """
+        Notifies worker-side connector ids of requests that have
+        finished generating tokens.
+
+        Returns:
+            ids of requests that have finished asynchronous transfer
+            (requests that previously returned True from request_finished()),
+            tuple of (sending/saving ids, recving/loading ids or
+            (recving/loading id, num. of recv'ed/loaded tokens) pairs).
+            The finished saves/sends req ids must belong to a set provided in a
+            call to this method (this call or a prior one).
+        """
+        return None, None
+
+    # ==============================
+    # Scheduler-side methods
+    # ==============================
+
+    def get_num_new_matched_tokens(
+        self,
+        request: "Request",
+        num_computed_tokens: int,
+    ) -> tuple[int, bool]:
+        """
+        Get number of new tokens that can be loaded from the
+        external KV cache beyond the num_computed_tokens.
+        
+        Args:
+            request (Request): the request object.
+            num_computed_tokens (int): the number of locally
+                computed tokens for this request
+
+        Returns:
+            A tuple with the following elements:
+                - The number of tokens that can be loaded from the 
+                  external KV cache beyond what is already computed.
+                - `True` if external KV cache tokens will be loaded
+                  asynchronously (between scheduler steps). Must be
+                  'False' if the first element is 0.
+        """
+        return 0, False
+
+    def update_state_after_alloc(self, request: "Request",
+                                 blocks: "KVCacheBlocks",
+                                 num_external_tokens: int):
+        """
+        Update KVConnector state after block allocation.
+
+        If get_num_new_matched_tokens previously returned True for a
+        request, this function may be called twice for that same request -
+        first when blocks are allocated for the connector tokens to be
+        asynchronously loaded into, and second when any additional blocks
+        are allocated, after the load/transfer is complete.
+
+        Args:
+            request (Request): the request object.
+            blocks (KVCacheBlocks): the blocks allocated for the request.
+            num_external_tokens (int): the number of tokens that will be
+                loaded from the external KV cache.
+        """
+        return
+
+    @delegate_to("connector_scheduler")
+    def build_connector_meta(
+            self, scheduler_output: "SchedulerOutput") -> KVConnectorMetadata:
+        """
+        Build the connector metadata for this step.
+
+        This function should NOT modify fields in the scheduler_output.
+        Also, calling this function will reset the state of the connector.
+
+        Args:
+            scheduler_output (SchedulerOutput): the scheduler output object.
+        """
+        pass
+
+    @delegate_to("connector_scheduler")
+    def request_finished(
+        self,
+        request: "Request",
+        block_ids: list[int],
+    ) -> tuple[bool, Optional[dict[str, Any]]]:
+        """
+        Called when a request has finished, before its blocks are freed.
+
+        Returns:
+            True if the request is being saved/sent asynchronously and blocks
+            should not be freed until the request_id is returned from
+            get_finished().
+            Optional KVTransferParams to be included in the request outputs
+            returned by the engine.
+        """
+        pass
diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type2.py b/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type2.py
new file mode 100644
index 000000000..3a6f2dddf
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type2.py
@@ -0,0 +1,741 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+import logging
+import time
+from typing import TYPE_CHECKING, Any, Optional
+
+import torch
+import torch.distributed as dist
+from aibrix_kvcache import KVCacheBlockLayout, KVCacheHandle
+from aibrix_kvcache._custom_ops import (reshape_and_cache_multi_layer,
+                                        reshape_and_offload_multi_layer)
+from aibrix_kvcache.common.absl_logging import (getLogger, log_every_n_seconds,
+                                                log_if)
+from aibrix_kvcache.profiling import tag_wrapper
+
+# from vllm.v1.attention.backends.flashinfer import FlashInferBackend
+# from vllm.v1.attention.backends.flex_attention import FlexAttentionBackend
+# from vllm.v1.attention.backends.triton_attn import TritonAttentionBackend
+from vllm.distributed.kv_transfer.kv_connector.v1.base import (
+    KVConnectorBase_V1, KVConnectorMetadata, KVConnectorRole)
+from vllm.utils import round_down
+from vllm.v1.attention.backends.flash_attn import FlashAttentionBackend
+
+from .aibrix_offloading_connector_type1 import (
+    AIBrixOffloadingConnectorMetadata,
+    AIBrixOffloadingConnectorRequestMetadata,
+    AIBrixOffloadingConnectorRequestState)
+from .aibrix_offloading_connector_type1 import (
+    AIBrixOffloadingConnectorScheduler as AIBrixOffloadingConnectorSchedulerType1)
+from .aibrix_offloading_connector_type1 import (
+    AIBrixOffloadingConnectorWorker as AIBrixOffloadingConnectorWorkerType1)
+from .aibrix_offloading_connector_type1 import delegate_to
+
+if TYPE_CHECKING:
+    from vllm.attention.backends.abstract import AttentionMetadata
+    from vllm.config import VllmConfig
+    from vllm.forward_context import ForwardContext
+    from vllm.v1.core.kv_cache_manager import KVCacheBlocks
+    from vllm.v1.core.sched.output import SchedulerOutput
+    from vllm.v1.request import Request
+
+logger = getLogger(__name__)
+
+OFFLOADING_CONNECTOR_SKIP_THRESHOLD = 8
+OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS = {
+    FlashAttentionBackend.get_name(): KVCacheBlockLayout.LCND,
+}
+
+
+class AIBrixOffloadingConnectorScheduler(
+        AIBrixOffloadingConnectorSchedulerType1):
+
+    def __init__(self, config: "VllmConfig"):
+        super().__init__(config)
+
+
+class AIBrixOffloadingConnectorWorker(AIBrixOffloadingConnectorWorkerType1):
+    """AIBrixOffloadingConnectorWorker carries out the data-plane operations.
+    """
+
+    def __init__(self, config: "VllmConfig"):
+        self._init_worker(
+            config,
+            max_num_batched_tokens=config.scheduler_config.
+            max_num_batched_tokens,
+        )
+        # create streams on current device that are dedicated for sends and
+        # recvs
+        self._send_stream = torch.cuda.Stream()
+        self._recv_stream = torch.cuda.Stream()
+        self._send_slot_mapping = torch.empty(
+            config.scheduler_config.max_num_batched_tokens,
+            dtype=torch.long,
+            device="cuda",
+        )
+        self._recv_slot_mapping = torch.empty(
+            config.scheduler_config.max_num_batched_tokens,
+            dtype=torch.long,
+            device="cuda",
+        )
+
+        self._allocated_kvcache_handles: dict[str, KVCacheHandle] = {}
+        self._acquired_kvcache_handles: dict[str, KVCacheHandle] = {}
+        self._send_lengths: dict[str, tuple[int, int]] = {}
+
+    def _get_block_layout(self) -> KVCacheBlockLayout:
+
+        if self.attn_backend.get_name() in \
+            OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS:
+            return KVCacheBlockLayout(
+                OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS[
+                    self.attn_backend.get_name()])
+        raise NotImplementedError(
+            f"Only support attn backends in "
+            f"{list(OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS.keys())}. "
+            f"{self.attn_backend.get_name()} is used.")
+
+    def _release_allocated_kvcache_handles(self):
+        for seq_request_id in self._allocated_kvcache_handles:
+            self._allocated_kvcache_handles[seq_request_id].release()
+        self._allocated_kvcache_handles.clear()
+
+    def _release_acquired_kvcache_handles(self):
+        for seq_request_id in self._acquired_kvcache_handles:
+            self._acquired_kvcache_handles[seq_request_id].release()
+        self._acquired_kvcache_handles.clear()
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnectorV1Type2",
+        "func": "start_load_kv_before_update"
+    })
+    def start_load_kv_before_update(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+    ) -> dict[str, int]:
+        self._update_meta_cache(metadata)
+
+        stats = {}
+        if self.kv_group is not None:
+            for seq_request_id, seq_request_meta in metadata.items():
+                num_fetched_tokens = self._recv_kv_impl(seq_request_meta)
+                if num_fetched_tokens > 0:
+                    stats[seq_request_id] = num_fetched_tokens
+
+            if len(stats) > 0:
+                for idx, (seq_request_id, seq_request_meta) in \
+                    enumerate(metadata.items()):
+                    self._coll_tensor[idx] = stats.get(seq_request_id, 0)
+                dist.all_reduce(self._coll_tensor[:idx], dist.ReduceOp.MIN,
+                                self.kv_group)
+
+                for idx, (seq_request_id, seq_request_meta) in \
+                    enumerate(metadata.items()):
+                    if self._coll_tensor[idx] > 0:
+                        stats[seq_request_id] = self._coll_tensor[idx].item()
+                    else:
+                        stats.pop(seq_request_id, None)
+
+        for seq_request_id, seq_request_meta in metadata.items():
+            if self.kv_group is None:
+                num_fetched_tokens = self._recv_kv_impl(seq_request_meta)
+            else:
+                num_fetched_tokens = stats.get(seq_request_id, 0)
+
+            if num_fetched_tokens > 0:
+                stats[seq_request_id] = num_fetched_tokens
+                self._send_lengths[seq_request_id] = (
+                    seq_request_meta.context_len + num_fetched_tokens,
+                    seq_request_meta.query_len - num_fetched_tokens)
+            else:
+                self._send_lengths[seq_request_id] = (
+                    seq_request_meta.context_len, seq_request_meta.query_len)
+
+            seq_request_meta.state = \
+                AIBrixOffloadingConnectorRequestState.WAITING_FOR_SEND
+
+            if seq_request_id not in self._acquired_kvcache_handles:
+                continue
+
+        # load the first layer
+        layer_name = list(self.no_compile_layers.keys())[0]
+        self._start_load_kv(metadata, layer_name)
+
+        return stats
+
+    def _recv_kv_impl(
+        self,
+        seq_request_meta: AIBrixOffloadingConnectorRequestMetadata,
+    ) -> int:
+        logger.debug("_recv_kv_impl: %s", seq_request_meta)
+        seq_request_id = seq_request_meta.req_id
+        seq_cached_meta = self._meta_cache[seq_request_id]
+        seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+        assert seq_all_tokens is not None, "seq_all_tokens is None"
+        seq_context_len = seq_request_meta.context_len
+
+        prompt_len = seq_request_meta.prompt_len
+        query_len = seq_request_meta.query_len
+
+        # align to block boundary
+        aligned_context_len = round_down(seq_context_len,
+                                         self.cache_block_ntokens)
+        actual_query_len = seq_context_len + query_len - aligned_context_len
+        aligned_query_len = round_down(actual_query_len,
+                                       self.cache_block_ntokens)
+        shift_len = seq_context_len - aligned_context_len
+
+        assert prompt_len >= aligned_context_len + aligned_query_len, \
+            f"{prompt_len}<{aligned_context_len}+{aligned_query_len}"
+
+        if aligned_query_len < OFFLOADING_CONNECTOR_SKIP_THRESHOLD * \
+                self.engine_block_ntokens:
+            logger.debug(
+                "Skip Request[id=%s, context_len=%d, query_len=%d]",
+                seq_request_id,
+                aligned_context_len,
+                aligned_query_len,
+            )
+            return 0
+
+        prefix = seq_all_tokens[:aligned_context_len]
+        tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                aligned_query_len]
+
+        if self._metrics.time_measurement_enabled:
+            start = time.perf_counter()
+
+        seq_recv_len = 0
+
+        # get KV caches from offloading service
+        status = self.cache.acquire(prefix, tokens)
+
+        if not status.is_ok():
+            if not status.is_not_found():
+                log_every_n_seconds(
+                    logger,
+                    logging.ERROR,
+                    "Failed to get from offloading service: %s",
+                    3,
+                    str(status),
+                )
+            if self._metrics.time_measurement_enabled:
+                end = time.perf_counter()
+                lat_ms = (end - start) * 1000
+                self._metrics._recv_metrics.add(aligned_context_len,
+                                                aligned_query_len, 0, lat_ms)
+            return 0
+
+        num_fetched_tokens, handle = status.value
+        self._acquired_kvcache_handles[seq_request_id] = handle
+
+        # update recv_len
+        seq_recv_len += num_fetched_tokens - shift_len
+
+        log_if(
+            logger,
+            logging.INFO,
+            "Request[id=%s, prompt_len=%d, context_len=%d] reused %d tokens",
+            seq_recv_len > 0,
+            seq_request_id,
+            prompt_len,
+            seq_context_len,
+            seq_recv_len,
+        )
+
+        if self._metrics.time_measurement_enabled:
+            end = time.perf_counter()
+            lat_ms = (end - start) * 1000
+            self._metrics._recv_metrics.add(aligned_context_len,
+                                            aligned_query_len, seq_recv_len,
+                                            lat_ms)
+
+        return seq_recv_len
+
+    def _start_load_kv(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+        layer_name: str,
+    ):
+        lid = self.layer_name_idx_mapping[layer_name]
+        layer_tensors: list[torch.Tensor] = []
+        slot_mapping_offset = 0
+
+        for seq_request_id in self._acquired_kvcache_handles:
+            if self._acquired_kvcache_handles[seq_request_id] is None:
+                continue
+            handle = self._acquired_kvcache_handles[seq_request_id]
+            seq_cached_meta = self._meta_cache[seq_request_id]
+            seq_context_len = metadata[seq_request_id].context_len
+            aligned_context_len = round_down(seq_context_len,
+                                             self.cache_block_ntokens)
+
+            kv_blocks = handle.to_tensors()
+            num_fetched_tokens = len(kv_blocks) * self.cache_block_ntokens
+
+            offset = aligned_context_len
+            length = num_fetched_tokens
+
+            self._recv_slot_mapping[slot_mapping_offset:slot_mapping_offset +
+                                    length].copy_(
+                                        seq_cached_meta.
+                                        context_slot_mapping[offset:offset +
+                                                             length])
+            slot_mapping_offset += length
+            # We are using LCND layout, so the tensors can be split by layers
+            layer_tensors.extend([t[lid:lid + 1] for t in kv_blocks])
+
+        if slot_mapping_offset == 0:
+            return
+
+        with torch.cuda.stream(self._recv_stream):
+            reshape_and_cache_multi_layer(
+                layer_tensors,
+                [self.layers_kv_caches[lid]],
+                self._recv_slot_mapping[:slot_mapping_offset],
+                self.engine_block_ntokens,
+                self.kv_cache_dtype,
+                [self.k_scales[lid]],
+                [self.v_scales[lid]],
+                self.block_layout.name,
+            )
+
+    def wait_for_layer_load(self, metadata: AIBrixOffloadingConnectorMetadata,
+                            layer_name: str) -> None:
+        # wait until kv layer is loaded
+        self._recv_stream.synchronize()
+        curr_layer_idx = self.layer_name_idx_mapping[layer_name]
+        if curr_layer_idx == self.num_layers - 1:
+            # release acquired handles once we finished onloading the last
+            # layer
+            self._release_acquired_kvcache_handles()
+        else:
+            # start to load next layer
+            next_layer_idx = curr_layer_idx + 1
+            next_layer = list(self.no_compile_layers.keys())[next_layer_idx]
+            self._start_load_kv(metadata, next_layer)
+
+    def save_kv_layer(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+        layer_name: str,
+        kv_layer: torch.Tensor,
+        attn_metadata: "AttentionMetadata",
+    ) -> None:
+
+        is_first_layer = self.layer_name_idx_mapping[layer_name] == 0
+        lid = self.layer_name_idx_mapping[layer_name]
+        layer_tensors: list[torch.Tensor] = []
+        slot_mapping_offset = 0
+
+        if is_first_layer:
+            for seq_req_id in self._send_lengths:
+                seq_context_len, seq_query_len = \
+                    self._send_lengths[seq_req_id]
+                if seq_query_len == 0:
+                    continue
+                # allocate staging buffers that can hold kvcache for all layers
+                seq_request_meta = metadata[seq_req_id]
+                handle = self._allocate_for_request(seq_request_meta)
+                if handle is None:
+                    continue
+                else:
+                    self._allocated_kvcache_handles[seq_req_id] = handle
+
+        for seq_req_id, _ in self._allocated_kvcache_handles.items():
+            if seq_req_id not in self._allocated_kvcache_handles or \
+                self._allocated_kvcache_handles[seq_req_id] is None:
+                return
+
+            seq_cached_meta = self._meta_cache[seq_req_id]
+            seq_context_len, _ = self._send_lengths[seq_req_id]
+            aligned_context_len = round_down(seq_context_len,
+                                             self.cache_block_ntokens)
+
+            seq_allocated_handle = self._allocated_kvcache_handles[seq_req_id]
+            seq_allocated_tensors = seq_allocated_handle.to_tensors()
+            seq_num_tokens = len(
+                seq_allocated_tensors) * self.cache_block_ntokens
+            self._send_slot_mapping[
+                slot_mapping_offset:slot_mapping_offset +
+                seq_num_tokens].copy_(seq_cached_meta.context_slot_mapping[
+                    aligned_context_len:aligned_context_len + seq_num_tokens])
+            slot_mapping_offset += seq_num_tokens
+
+            # We are using LCND layout, so the tensors can be split by layers
+            layer_tensors.extend(
+                [t[lid:lid + 1] for t in seq_allocated_tensors])
+
+        if slot_mapping_offset == 0:
+            return
+
+        # wait for compute on current stream
+        curr_stream = torch.cuda.current_stream()
+        self._send_stream.wait_stream(curr_stream)
+
+        # use send stream to carry out async copy from HBM to DRAM
+        with torch.cuda.stream(self._send_stream):
+            reshape_and_offload_multi_layer(
+                layer_tensors,
+                [kv_layer],
+                self._send_slot_mapping[:slot_mapping_offset],
+                self.engine_block_ntokens,
+                self.kv_cache_dtype,
+                [self.k_scales[lid]],
+                [self.v_scales[lid]],
+                self.block_layout.name,
+            )
+
+    def _allocate_for_request(
+        self,
+        seq_request_meta: AIBrixOffloadingConnectorRequestMetadata,
+    ) -> Optional[KVCacheHandle]:
+        seq_request_id = seq_request_meta.req_id
+        seq_context_len, query_len = self._send_lengths[seq_request_id]
+        seq_cached_meta = self._meta_cache[seq_request_id]
+        seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+        assert seq_all_tokens is not None, "seq_all_tokens is None"
+        prompt_len = seq_request_meta.prompt_len
+
+        # align to block boundary
+        aligned_context_len = round_down(seq_context_len,
+                                         self.cache_block_ntokens)
+        actual_query_len = seq_context_len + query_len - aligned_context_len
+        aligned_query_len = round_down(actual_query_len,
+                                       self.cache_block_ntokens)
+
+        assert prompt_len >= aligned_context_len + aligned_query_len, \
+            f"{prompt_len}<{aligned_context_len}+{aligned_query_len}"
+
+        if aligned_query_len < self.cache_block_ntokens:
+            return None
+
+        prefix = seq_all_tokens[:aligned_context_len]
+        tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                aligned_query_len]
+        status = self.cache.allocate_for(prefix, tokens)
+        if not status.is_ok():
+            log_every_n_seconds(logger, logging.ERROR,
+                                "Failed to allocate : %s", 3, str(status))
+            return None
+        return status.get()
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnectorV1Type2",
+        "func": "wait_for_save"
+    })
+    def wait_for_save(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+    ) -> None:
+        # wait for send stream to finish
+        self._send_stream.synchronize()
+
+        for seq_request_id in self._send_lengths:
+            seq_context_len, seq_query_len = self._send_lengths[seq_request_id]
+            if seq_query_len == 0:
+                continue
+            if (seq_request_id not in self._allocated_kvcache_handles or
+                    self._allocated_kvcache_handles[seq_request_id] is None):
+                continue
+            seq_request_meta = metadata[seq_request_id]
+            self._send_kv_impl(seq_request_meta)
+
+        # release all allocated handles
+        self._release_allocated_kvcache_handles()
+        self._send_lengths.clear()
+
+        if self._metrics.time_measurement_enabled:
+            log_every_n_seconds(self._metrics, logging.INFO, "UNUSED", 10)
+
+    def _send_kv_impl(
+        self,
+        seq_request_meta: AIBrixOffloadingConnectorRequestMetadata,
+    ) -> None:
+        logger.debug("_send_kv_impl: %s", seq_request_meta)
+        seq_request_id = seq_request_meta.req_id
+        seq_context_len, query_len = self._send_lengths[seq_request_id]
+        seq_cached_meta = self._meta_cache[seq_request_id]
+        seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+        assert seq_all_tokens is not None, "seq_all_tokens is None"
+        seq_allocated_handle = self._allocated_kvcache_handles.pop(
+            seq_request_id)
+        seq_allocated_tensors = seq_allocated_handle.to_tensors()
+        prompt_len = seq_request_meta.prompt_len
+        query_len = min(
+            query_len,
+            len(seq_allocated_tensors) * self.cache_block_ntokens,
+        )
+
+        # align to block boundary
+        aligned_context_len = round_down(seq_context_len,
+                                         self.cache_block_ntokens)
+        actual_query_len = seq_context_len + query_len - aligned_context_len
+        aligned_query_len = round_down(actual_query_len,
+                                       self.cache_block_ntokens)
+
+        assert prompt_len >= aligned_context_len + aligned_query_len, \
+            f"{prompt_len}<{aligned_context_len}+{aligned_query_len}"
+
+        prefix = seq_all_tokens[:aligned_context_len]
+        tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                aligned_query_len]
+
+        if self._metrics.time_measurement_enabled:
+            start = time.perf_counter()
+
+        total_sent = 0
+        length = aligned_query_len
+
+        # put KV caches to offloading service
+        status = self.cache.put(prefix, tokens, seq_allocated_handle)
+        if not status.is_ok():
+            log_every_n_seconds(logger, logging.ERROR,
+                                "Failed to put to offloading service: %s", 3,
+                                str(status))
+        else:
+            total_sent += length
+
+            log_if(
+                logger,
+                logging.INFO,
+                "Request[id=%s, prompt_len=%d, context_len=%d] sent %d tokens",
+                total_sent > 0,
+                seq_request_id,
+                prompt_len,
+                seq_context_len,
+                total_sent,
+            )
+
+        if self._metrics.time_measurement_enabled:
+            end = time.perf_counter()
+            lat_ms = (end - start) * 1000
+            self._metrics._send_metrics.add(aligned_context_len,
+                                            aligned_query_len, total_sent,
+                                            lat_ms)
+
+
+class AIBrixOffloadingConnector(KVConnectorBase_V1):
+    """AIBrixOffloadingConnector is a KVConnector that offloads KV caches
+    to the kv cache offloading service.
+    """
+
+    def __init__(self, config: "VllmConfig", role: KVConnectorRole):
+        super().__init__(vllm_config=config, role=role)
+
+        self.connector_scheduler: Optional[
+            AIBrixOffloadingConnectorScheduler] = None
+
+        self.connector_worker: Optional[AIBrixOffloadingConnectorWorker] = None
+        if role == KVConnectorRole.SCHEDULER:
+            self.connector_scheduler = AIBrixOffloadingConnectorScheduler(
+                config)
+        elif role == KVConnectorRole.WORKER:
+            self.connector_worker = AIBrixOffloadingConnectorWorker(config)
+
+    # ==============================
+    # Worker-side methods
+    # ==============================
+
+    @delegate_to("connector_worker")
+    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):
+        """
+        Initialize with the KV caches. Useful for pre-registering the
+        KV Caches in the KVConnector (e.g. for NIXL).
+
+        Args: kv_caches:
+            dictionary of layer names, kv cache
+        """
+        pass
+
+    def start_load_kv_before_update(self, **kwargs) -> dict[str, int]:
+        """
+        Start loading the KV cache from the connector to vLLM's paged
+        KV buffer before gpu runner updating its states.
+
+        Args:
+            **kwargs: additional arguments for the load operation
+
+        Returns:
+            dict[str, int]: a dictionary of request ids and the number of
+            tokens loaded for each request.
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        return self.connector_worker.start_load_kv_before_update(
+            self._connector_metadata)
+
+    def start_load_kv(self, forward_context: "ForwardContext",
+                      **kwargs) -> None:
+        """
+        Start loading the KV cache from the connector to vLLM's paged
+        KV buffer. This is called from the forward context before the
+        forward pass to enable async loading during model execution.
+
+        Args:
+            forward_context (ForwardContext): the forward context.
+            **kwargs: additional arguments for the load operation
+
+        Note:
+            The number of elements in kv_caches and layer_names should be 
+            the same.
+            
+        """
+        pass
+
+    def wait_for_layer_load(self, layer_name: str) -> None:
+        """
+        Block until the KV for a specific layer is loaded into vLLM's
+        paged buffer. This is called from within attention layer to ensure
+        async copying from start_load_kv is complete.
+        
+        This interface will be useful for layer-by-layer pipelining.
+
+        Args:
+            layer_name: the name of that layer
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        return self.connector_worker.wait_for_layer_load(
+            self._connector_metadata, layer_name)
+
+    def save_kv_layer(self, layer_name: str, kv_layer: torch.Tensor,
+                      attn_metadata: "AttentionMetadata", **kwargs) -> None:
+        """
+        Start saving a layer of KV cache from vLLM's paged buffer 
+        to the connector. This is called from within attention layer to
+        enable async copying during execution.
+
+        Args:
+            layer_name (str): the name of the layer.
+            kv_layer (torch.Tensor): the paged KV buffer of the current 
+                layer in vLLM.
+            attn_metadata (AttentionMetadata): the attention metadata.
+            **kwargs: additional arguments for the save operation.
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        self.connector_worker.save_kv_layer(self._connector_metadata,
+                                            layer_name, kv_layer,
+                                            attn_metadata)
+
+    def wait_for_save(self) -> None:
+        """
+        Block until all the save operations is done. This is called
+        as the forward context exits to ensure that the async saving
+        from save_kv_layer is complete before finishing the forward.
+
+        This prevents overwrites of paged KV buffer before saving done.
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        self.connector_worker.wait_for_save(self._connector_metadata)
+
+    def get_finished(
+        self, finished_req_ids: set[str]
+    ) -> tuple[Optional[set[str]], Optional[set[str | tuple[str, int]]]]:
+        """
+        Notifies worker-side connector ids of requests that have
+        finished generating tokens.
+
+        Returns:
+            ids of requests that have finished asynchronous transfer
+            (requests that previously returned True from request_finished()),
+            tuple of (sending/saving ids, recving/loading ids or
+            (recving/loading id, num. of recv'ed/loaded tokens) pairs).
+            The finished saves/sends req ids must belong to a set provided in a
+            call to this method (this call or a prior one).
+        """
+        return None, None
+
+    # ==============================
+    # Scheduler-side methods
+    # ==============================
+
+    def get_num_new_matched_tokens(
+        self,
+        request: "Request",
+        num_computed_tokens: int,
+    ) -> tuple[int, bool]:
+        """
+        Get number of new tokens that can be loaded from the
+        external KV cache beyond the num_computed_tokens.
+        
+        Args:
+            request (Request): the request object.
+            num_computed_tokens (int): the number of locally
+                computed tokens for this request
+
+        Returns:
+            A tuple with the following elements:
+                - The number of tokens that can be loaded from the 
+                  external KV cache beyond what is already computed.
+                - `True` if external KV cache tokens will be loaded
+                  asynchronously (between scheduler steps). Must be
+                  'False' if the first element is 0.
+        """
+        return 0, False
+
+    def update_state_after_alloc(self, request: "Request",
+                                 blocks: "KVCacheBlocks",
+                                 num_external_tokens: int):
+        """
+        Update KVConnector state after block allocation.
+
+        If get_num_new_matched_tokens previously returned True for a
+        request, this function may be called twice for that same request -
+        first when blocks are allocated for the connector tokens to be
+        asynchronously loaded into, and second when any additional blocks
+        are allocated, after the load/transfer is complete.
+
+        Args:
+            request (Request): the request object.
+            blocks (KVCacheBlocks): the blocks allocated for the request.
+            num_external_tokens (int): the number of tokens that will be
+                loaded from the external KV cache.
+        """
+        return
+
+    @delegate_to("connector_scheduler")
+    def build_connector_meta(
+            self, scheduler_output: "SchedulerOutput") -> KVConnectorMetadata:
+        """
+        Build the connector metadata for this step.
+
+        This function should NOT modify fields in the scheduler_output.
+        Also, calling this function will reset the state of the connector.
+
+        Args:
+            scheduler_output (SchedulerOutput): the scheduler output object.
+        """
+        pass
+
+    @delegate_to("connector_scheduler")
+    def request_finished(
+        self,
+        request: "Request",
+        block_ids: list[int],
+    ) -> tuple[bool, Optional[dict[str, Any]]]:
+        """
+        Called when a request has finished, before its blocks are freed.
+
+        Returns:
+            True if the request is being saved/sent asynchronously and blocks
+            should not be freed until the request_id is returned from
+            get_finished().
+            Optional KVTransferParams to be included in the request outputs
+            returned by the engine.
+        """
+        pass
diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type3.py b/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type3.py
new file mode 100644
index 000000000..45d600930
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/aibrix_offloading_connector_type3.py
@@ -0,0 +1,1615 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+import asyncio
+import contextlib
+import logging
+import queue
+import threading
+import time
+from typing import TYPE_CHECKING, Any, Optional
+
+import aibrix_kvcache.envs
+import msgspec
+import torch
+import torch.distributed as dist
+import zmq
+from aibrix_kvcache import KVCacheBlockLayout, KVCacheHandle, TokenListView
+from aibrix_kvcache._custom_ops import (reshape_and_cache_multi_layer,
+                                        reshape_and_offload_multi_layer)
+from aibrix_kvcache.common.absl_logging import (getLogger, log_every_n_seconds,
+                                                log_if)
+from aibrix_kvcache.profiling import tag_wrapper
+from aibrix_kvcache.utils import perf_timer
+
+import vllm.envs
+# from vllm.v1.attention.backends.flashinfer import FlashInferBackend
+# from vllm.v1.attention.backends.flex_attention import FlexAttentionBackend
+# from vllm.v1.attention.backends.triton_attn import TritonAttentionBackend
+from vllm.distributed.kv_transfer.kv_connector.v1.base import (
+    KVConnectorBase_V1, KVConnectorMetadata, KVConnectorRole)
+from vllm.utils import (get_ip, make_zmq_path, make_zmq_socket, round_down,
+                        round_up)
+from vllm.v1.attention.backends.flash_attn import FlashAttentionBackend
+
+from .aibrix_offloading_connector_type1 import (
+    AIBrixOffloadingConnectorMetadata,
+    AIBrixOffloadingConnectorRequestMetadata,
+    AIBrixOffloadingConnectorRequestState)
+from .aibrix_offloading_connector_type1 import (
+    AIBrixOffloadingConnectorScheduler as AIBrixOffloadingConnectorSchedulerType1)
+from .aibrix_offloading_connector_type1 import (
+    AIBrixOffloadingConnectorWorker as AIBrixOffloadingConnectorWorkerType1)
+from .aibrix_offloading_connector_type1 import delegate_to
+
+if TYPE_CHECKING:
+    from vllm.attention.backends.abstract import AttentionMetadata
+    from vllm.config import VllmConfig
+    from vllm.forward_context import ForwardContext
+    from vllm.v1.core.kv_cache_manager import KVCacheBlocks
+    from vllm.v1.core.sched.output import SchedulerOutput
+    from vllm.v1.request import Request
+
+logger = getLogger(__name__)
+
+OFFLOADING_CONNECTOR_SKIP_THRESHOLD = 8
+OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS = {
+    FlashAttentionBackend.get_name(): KVCacheBlockLayout.LCND,
+}
+OFFLOADING_CONNECTOR_TIMEOUT_S = 0.1
+
+
+class AIBrixOffloadingConnectorWorkerMeta(
+        msgspec.Struct,
+        omit_defaults=True,  # type: ignore[call-arg]
+        # required for @cached_property.
+        dict=True):
+    tp_rank: int
+
+
+class AIBrixOffloadingConnectorTestRequest(
+        msgspec.Struct,
+        omit_defaults=True,  # type: ignore[call-arg]
+        # required for @cached_property.
+        dict=True):
+    req_id: str
+    seq_token_ids: list[int]
+
+
+class AIBrixOffloadingConnectorTestResponse(
+        msgspec.Struct,
+        omit_defaults=True,  # type: ignore[call-arg]
+        # required for @cached_property.
+        dict=True):
+    req_id: str
+    seq_existing_len: int
+
+
+class AIBrixOffloadingConnectorScheduler(
+        AIBrixOffloadingConnectorSchedulerType1):
+
+    def __init__(self, config: "VllmConfig"):
+        super().__init__(config)
+
+        if config.parallel_config.data_parallel_backend == "ray":
+            sched_ip = get_ip()
+        else:
+            sched_ip = config.parallel_config.data_parallel_master_ip
+
+        self.tp_size = config.parallel_config.tensor_parallel_size
+        self.side_channel_host = sched_ip
+        self.side_channel_port = (vllm.envs.VLLM_AIBRIX_SIDE_CHANNEL_PORT +
+                                  config.parallel_config.data_parallel_rank)
+        self.dp_rank = config.parallel_config.data_parallel_rank
+
+        self._alloc_block_ids: dict[str, list[int]] = {}
+        self.cache_block_ntokens = \
+            aibrix_kvcache.envs.AIBRIX_KV_CACHE_OL_BLOCK_SIZE
+        if self.cache_block_ntokens < 0:
+            self.cache_block_ntokens = self.engine_block_ntokens
+
+        self._testing_requests: dict[str, int] = {}
+        self._pending_requests: list[Request] = []
+        self._testing_result_queue = queue.Queue()
+
+        # init side channels
+        self._zmq_ctx = zmq.Context()
+
+        self._sidechannel_path = make_zmq_path(
+            "tcp",
+            self.side_channel_host,
+            self.side_channel_port,
+        )
+        self._sidechannel_sock = make_zmq_socket(
+            ctx=self._zmq_ctx,
+            path=self._sidechannel_path,
+            socket_type=zmq.ROUTER,
+            bind=True,
+        )
+
+        self._worker_identities: dict[int, Any] = {}
+        self._handshake_listener_thread = threading.Thread(
+            target=self._handshake_listener,
+            args=(),
+            daemon=True,
+            name="sched_sidechannel")
+        self._handshake_listener_thread.start()
+        self._sidechannels_ready = False
+
+    def __del__(self):
+        if hasattr(self, "_zmq_ctx") and self._zmq_ctx is not None:
+            self._zmq_ctx.destroy(linger=0)
+
+    def _handshake_listener(self):
+        logger.debug("Starting listening on path: %s", self._sidechannel_path)
+        decoder = msgspec.msgpack.Decoder(AIBrixOffloadingConnectorWorkerMeta)
+        while True:
+            identity, _, msg = self._sidechannel_sock.recv_multipart()
+            metadata = decoder.decode(msg)
+            worker_tp_rank = metadata.tp_rank
+            if 0 <= worker_tp_rank < self.tp_size:
+                if worker_tp_rank in self._worker_identities:
+                    logger.warning("Got duplicated worker tp rank: %s",
+                                   worker_tp_rank)
+                else:
+                    logger.info(
+                        "Worker %d in DP group %d has established side "
+                        "channel",
+                        worker_tp_rank,
+                        self.dp_rank,
+                    )
+                self._worker_identities[worker_tp_rank] = identity
+            else:
+                logger.error("Got invalid worker tp rank: %s", worker_tp_rank)
+
+            if len(self._worker_identities) == self.tp_size:
+                logger.info(
+                    "Side channels for all workers in DP group %d have "
+                    "been established",
+                    self.dp_rank,
+                )
+                break
+            else:
+                logger.info(
+                    "Established %d side channels for DP group %d",
+                    len(self._worker_identities),
+                    self.dp_rank,
+                )
+
+        # start polling
+        self._sidechannel_run()
+
+    def _sidechannel_run(self):
+        logger.debug(
+            "Starting scheduler side channel on path: %s",
+            self._sidechannel_path,
+        )
+        decoder = msgspec.msgpack.Decoder(
+            list[AIBrixOffloadingConnectorTestResponse])
+        poller = zmq.Poller()
+        poller.register(self._sidechannel_sock, zmq.POLLIN)
+        while True:
+            for sock, _ in poller.poll():
+                id, _, msg = sock.recv_multipart()
+                responses = decoder.decode(msg)
+                for response in responses:
+                    seq_existing_len = response.seq_existing_len
+                    logger.debug(
+                        "Test Request[id=%s] %s returns %d",
+                        response.req_id,
+                        id.hex(),
+                        seq_existing_len,
+                    )
+                    self._testing_result_queue.put(
+                        (response.req_id, seq_existing_len))
+
+    def update_state_after_alloc(self, request: "Request",
+                                 blocks: "KVCacheBlocks",
+                                 num_external_tokens: int):
+        req_id = request.request_id
+        prompt_len = len(request.prompt_token_ids)
+
+        if prompt_len < OFFLOADING_CONNECTOR_SKIP_THRESHOLD * \
+                self.engine_block_ntokens:
+            return
+
+        (block_ids, ) = blocks.get_block_ids()
+        self._alloc_block_ids.setdefault(req_id, []).extend(block_ids)
+
+    def load_and_prefetch(
+        self,
+        request: "Request",
+        num_to_load_tokens: int,
+        num_to_prefetch_tokens: int,
+    ) -> int:
+        req_id = request.request_id
+
+        prompt_len = len(request.prompt_token_ids)
+        if prompt_len < OFFLOADING_CONNECTOR_SKIP_THRESHOLD * \
+                self.engine_block_ntokens:
+            logger.debug(
+                "Skip Request[id=%s, prompt_len=%d]: prompt len too short",
+                req_id,
+                prompt_len,
+            )
+            return 0
+
+        is_new_request = req_id not in self._scheduler_meta
+
+        loaded_len = request.num_computed_tokens
+        block_ids = self._alloc_block_ids.get(req_id, None)
+
+        if block_ids is not None:
+            aligned_loaded_len = round_up(loaded_len,
+                                          self.engine_block_ntokens)
+            num_loaded_blocks = aligned_loaded_len // self.engine_block_ntokens
+            if len(block_ids) > num_loaded_blocks:
+                seq_slot_mapping = (
+                    aligned_loaded_len,
+                    self._block_ids_to_slot_mapping(block_ids)\
+                        [aligned_loaded_len:],
+                )
+            else:
+                # this is a hybrid request, the block ids in _alloc_block_ids
+                # are for compute not for loading
+                seq_slot_mapping = None
+        else:
+            seq_slot_mapping = None
+
+        context_len = request.num_computed_tokens + num_to_load_tokens
+        query_len = round_down(num_to_prefetch_tokens,
+                               self.cache_block_ntokens)
+
+        if num_to_load_tokens == 0 and query_len == 0:
+            logger.debug(
+                "Skip Request[id=%s, prompt_len=%d, computed_len=%d]: no tokens"
+                " to load or prefetch",
+                req_id,
+                prompt_len,
+                request.num_computed_tokens,
+            )
+            return 0
+
+        seq_len = min(context_len + query_len, prompt_len)
+
+        if not is_new_request:
+            seq_token_ids = (
+                context_len,
+                request.prompt_token_ids[context_len:seq_len],
+            )
+        else:
+            seq_token_ids = (0, request.prompt_token_ids[:seq_len])
+
+        self._scheduler_meta.upsert_request(
+            req_id,
+            prompt_len=prompt_len,
+            context_len=context_len,
+            query_len=query_len,
+            load_len=num_to_load_tokens,
+            seq_token_ids=seq_token_ids,
+            seq_slot_mapping=seq_slot_mapping,
+            state=AIBrixOffloadingConnectorRequestState.WAITING_FOR_RECV,
+        )
+        return query_len
+
+    def add_requests(
+        self,
+        requests: list["Request"],
+    ) -> None:
+        self._pending_requests.extend(requests)
+        self._send_test_requests()
+
+    def test_request(
+        self,
+        request: "Request",
+    ) -> bool:
+        if not self._sidechannels_ready:
+            return False
+        if len(self._pending_requests) > 0:
+            self._send_test_requests()
+
+        req_id = request.request_id
+        if req_id not in self._testing_requests:
+            return False
+
+        while self._testing_requests[req_id] == -1:
+            try:
+                (seq_req_id,
+                 seq_existing_len) = self._testing_result_queue.get(
+                     timeout=OFFLOADING_CONNECTOR_TIMEOUT_S)
+            except Exception:
+                self._testing_requests.pop(req_id, None)
+                return False
+            if seq_req_id in self._testing_requests:
+                self._testing_requests[seq_req_id] = seq_existing_len
+
+        return self._testing_requests[req_id] > 0
+
+    def _send_test_requests(self) -> None:
+        # side channels are not ready, return
+        if not self._sidechannels_ready:
+            return
+
+        num_to_test_tokens = (OFFLOADING_CONNECTOR_SKIP_THRESHOLD *
+                              self.engine_block_ntokens)
+
+        test_requests: list[AIBrixOffloadingConnectorTestRequest] = []
+        for request in self._pending_requests:
+            req_id = request.request_id
+            if req_id in self._testing_requests:
+                continue
+
+            prompt_len = len(request.prompt_token_ids)
+            if prompt_len < num_to_test_tokens:
+                logger.debug(
+                    "Skip testing Request[id=%s, prompt_len=%d]: too short",
+                    req_id,
+                    prompt_len,
+                )
+                self._testing_requests[req_id] = 0
+                continue
+
+            logger.debug(
+                "Test Request[id=%s, prompt_len=%d]",
+                req_id,
+                prompt_len,
+            )
+
+            seq_token_ids = request.prompt_token_ids[:num_to_test_tokens]
+            test = AIBrixOffloadingConnectorTestRequest(
+                req_id,
+                seq_token_ids=seq_token_ids,
+            )
+            self._testing_requests[req_id] = -1
+            test_requests.append(test)
+
+        encoder = msgspec.msgpack.Encoder()
+        encoded_data = encoder.encode(test_requests)
+        # only test requests on worker-0 to alleviate comm overhead
+        id0 = self._worker_identities[0]
+        self._sidechannel_sock.send_multipart((id0, b"", encoded_data))
+
+        # clear pending requests
+        self._pending_requests.clear()
+
+    def build_connector_meta(
+            self, scheduler_output: "SchedulerOutput") -> KVConnectorMetadata:
+        # 1. new requests
+        for req in scheduler_output.scheduled_new_reqs:
+            req_id = req.req_id
+
+            prompt_len = len(req.prompt_token_ids)
+            context_len = req.num_computed_tokens
+            query_len = scheduler_output.num_scheduled_tokens[req_id]
+            seq_len = min(context_len + query_len, prompt_len)
+
+            if context_len >= prompt_len:
+                continue
+
+            seq_token_ids = (0, req.prompt_token_ids[:seq_len])
+            (block_ids, ) = req.block_ids
+            seq_slot_mapping = (0, self._block_ids_to_slot_mapping(block_ids))
+
+            self._scheduler_meta.upsert_request(
+                req_id,
+                prompt_len=prompt_len,
+                context_len=context_len,
+                load_len=0,
+                query_len=query_len,
+                seq_token_ids=seq_token_ids,
+                seq_slot_mapping=seq_slot_mapping,
+                state=AIBrixOffloadingConnectorRequestState.WAITING_FOR_SEND,
+            )
+
+        # 2. cached requests
+        for req in scheduler_output.scheduled_cached_reqs:
+            req_id = req.req_id
+
+            if (req_id not in self._scheduler_meta
+                    or req_id not in scheduler_output.num_scheduled_tokens):
+                continue
+
+            req_meta = self._scheduler_meta[req_id]
+
+            prompt_len = req_meta.prompt_len
+            context_len = min(req.num_computed_tokens, prompt_len)
+            query_len = min(
+                scheduler_output.num_scheduled_tokens[req_id],
+                prompt_len - context_len,
+            )
+            seq_len = min(context_len + query_len, prompt_len)
+
+            if context_len >= prompt_len:
+                continue
+
+            if req.resumed_from_preemption:
+                logger.debug(
+                    "Got preempt Request[id=%s, context_len=%d, query_len=%d]",
+                    req_id,
+                    context_len,
+                    query_len,
+                )
+                (block_ids, ) = req.new_block_ids
+                seq_token_ids = (
+                    context_len,
+                    req.new_token_ids[:seq_len - context_len],
+                )
+                seq_slot_mapping = (0,
+                                    self._block_ids_to_slot_mapping(block_ids))
+            else:
+                (block_ids, ) = req.new_block_ids
+                seq_token_ids = (context_len, req.new_token_ids[-query_len:])
+                nblocks = round_down(query_len, self.engine_block_ntokens)
+                if nblocks > 0:
+                    seq_slot_mapping = (
+                        round_up(context_len, self.engine_block_ntokens),
+                        self._block_ids_to_slot_mapping(block_ids[-nblocks:]),
+                    )
+                else:
+                    seq_slot_mapping = None
+
+            self._scheduler_meta.upsert_request(
+                req_id,
+                prompt_len=prompt_len,
+                context_len=context_len,
+                load_len=0,
+                query_len=query_len,
+                seq_token_ids=seq_token_ids,
+                seq_slot_mapping=seq_slot_mapping,
+                state=AIBrixOffloadingConnectorRequestState.WAITING_FOR_SEND,
+                resumed_from_preemption=req.resumed_from_preemption,
+            )
+
+        # 3. keep requests that are in the WAITING_FOR_RECV/SEND state
+        meta = self._scheduler_meta.get(lambda req: req.state in [
+            AIBrixOffloadingConnectorRequestState.WAITING_FOR_RECV,
+            AIBrixOffloadingConnectorRequestState.WAITING_FOR_SEND,
+        ])
+
+        # 4. update scheduled requests
+        for req_id in meta:
+            if (self._scheduler_meta[req_id].state ==
+                    AIBrixOffloadingConnectorRequestState.WAITING_FOR_RECV):
+                self._scheduler_meta.upsert_request(
+                    req_id,
+                    state=AIBrixOffloadingConnectorRequestState.RECEIVING,
+                )
+            else:
+                self._scheduler_meta.upsert_request(
+                    req_id,
+                    state=AIBrixOffloadingConnectorRequestState.SENDING,
+                )
+
+        # 5. attach finished requests
+        meta.finished_requests_ids = self._scheduler_meta.finished_requests_ids
+        self._scheduler_meta.finished_requests_ids = set()
+
+        # 6. attach total_num_scheduled_tokens
+        meta.total_num_scheduled_tokens = \
+            scheduler_output.total_num_scheduled_tokens
+
+        # 7. side channel host and port
+        if not self._sidechannels_ready:
+            meta.side_channel_host = self.side_channel_host
+            meta.side_channel_port = self.side_channel_port
+            self._sidechannels_ready = True
+        else:
+            meta.side_channel_host = ""
+
+        logger.debug("SCHEDULER: build_connector_meta, meta=%s", meta)
+        if len(meta.requests) > 0 or len(meta.finished_requests_ids) > 0:
+            logger.debug(
+                "Num. of scheduled requests: %s",
+                len(
+                    self._scheduler_meta.get(lambda req: req.state in [
+                        AIBrixOffloadingConnectorRequestState.RECEIVING,
+                        AIBrixOffloadingConnectorRequestState.SENDING,
+                    ])),
+            )
+        return meta
+
+    def request_finished(
+        self,
+        request: "Request",
+        block_ids: list[int],
+    ) -> tuple[bool, Optional[dict[str, Any]]]:
+        req_id = request.request_id
+        self._alloc_block_ids.pop(req_id, None)
+        self._testing_requests.pop(req_id, None)
+        return super().request_finished(request, block_ids)
+
+    def request_preempted(
+        self,
+        request: "Request",
+    ) -> None:
+        req_id = request.request_id
+        self._alloc_block_ids.pop(req_id, None)
+        logger.debug("SCHEDULER: Request[id=%s] preempted", req_id)
+
+        self._scheduler_meta.finish_request(req_id)
+
+
+class AIBrixOffloadingConnectorWorker(AIBrixOffloadingConnectorWorkerType1):
+    """AIBrixOffloadingConnectorWorker carries out the data-plane operations.
+    """
+
+    def __init__(self, config: "VllmConfig"):
+        self._init_worker(
+            config,
+            max_num_batched_tokens=config.scheduler_config.
+            max_num_batched_tokens,
+            multi_threaded=True,
+        )
+        self.dp_rank = config.parallel_config.data_parallel_rank
+        # create streams on current device that are dedicated for sends and
+        # recvs
+        self._send_stream = torch.cuda.Stream()
+        self._load_stream = torch.cuda.Stream()
+        self._send_slot_mapping = torch.empty(
+            config.scheduler_config.max_num_batched_tokens,
+            dtype=torch.long,
+            device="cuda",
+        )
+        self._recv_slot_mapping = torch.empty(
+            config.scheduler_config.max_num_batched_tokens,
+            dtype=torch.long,
+            device="cuda",
+        )
+
+        self._kv_event_loop, self._kv_thread = self._create_event_loop()
+
+        self._prefetch_reqs: list[
+            AIBrixOffloadingConnectorRequestMetadata] = []
+        self._load_reqs: list[AIBrixOffloadingConnectorRequestMetadata] = []
+        self._send_reqs: list[AIBrixOffloadingConnectorRequestMetadata] = []
+
+        self._allocated_kvcache_handles: dict[str, KVCacheHandle] = {}
+        self._acquired_kvcache_handles: dict[str, list[KVCacheHandle]] = {}
+        self._prefetch_future: asyncio.Future | None = None
+
+        # side channel
+        self._zmq_ctx = zmq.Context()
+        self._sidechannel_path: Optional[str] = None
+        self._sidechannel_sock: Optional[zmq.Socket
+                                         | zmq.asyncio.Socket] = None
+        self._sidechannel_thread: Optional[threading.Thread] = None
+
+    def __del__(self):
+        self._destroy_event_loop(
+            self._kv_event_loop,
+            self._kv_thread,
+        )
+        del self._kv_event_loop
+        del self._kv_thread
+
+        if (hasattr(self, "_sidechannel_thread")
+                and self._sidechannel_thread is not None
+                and self._sidechannel_thread.is_alive()):
+            self._sidechannel_thread.join()
+
+        if hasattr(self, "_zmq_ctx") and self._zmq_ctx is not None:
+            self._zmq_ctx.destroy(linger=0)
+
+    def _create_event_loop(self) \
+        -> tuple[asyncio.AbstractEventLoop, threading.Thread]:
+        event_loop = asyncio.new_event_loop()
+        thread = threading.Thread(target=event_loop.run_forever, daemon=True)
+        thread.start()
+        return event_loop, thread
+
+    def _destroy_event_loop(self, event_loop: asyncio.AbstractEventLoop,
+                            thread: threading.Thread):
+        # terminate event loop and thread
+        if event_loop is not None and event_loop.is_running():
+            with contextlib.suppress(Exception):
+                # ignore the exception
+                event_loop.call_soon_threadsafe(event_loop.stop)
+
+        if thread is not None and thread.is_alive():
+            thread.join()
+
+    def _get_block_layout(self) -> KVCacheBlockLayout:
+
+        if self.attn_backend.get_name() in \
+            OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS:
+            return KVCacheBlockLayout(
+                OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS[
+                    self.attn_backend.get_name()])
+        raise NotImplementedError(
+            f"Only support attn backends in "
+            f"{list(OFFLOADING_CONNECTOR_SUPPORTED_ATTN_BACKENDS.keys())}. "
+            f"{self.attn_backend.get_name()} is used.")
+
+    def _release_allocated_kvcache_handles(self):
+        for seq_request_id in self._allocated_kvcache_handles:
+            self._allocated_kvcache_handles[seq_request_id].release()
+        self._allocated_kvcache_handles.clear()
+
+    def _release_allocated_kvcache_handle(self, req_id):
+        handle = self._allocated_kvcache_handles.pop(req_id, None)
+        if handle is not None:
+            handle.release()
+
+    def _pop_acquired_kvcache_handle(self, req_id: str):
+        handles = self._acquired_kvcache_handles.get(req_id, None)
+        if handles is None or len(handles) == 0:
+            return
+
+        handles[0].release()
+        self._acquired_kvcache_handles[req_id] = handles[1:]
+
+    def _release_acquired_kvcache_handles(self, req_id: str):
+        handles = self._acquired_kvcache_handles.pop(req_id, None)
+        if handles is None:
+            return
+
+        for handle in handles:
+            handle.release()
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnectorV1Type3",
+        "func": "start_load_kv"
+    })
+    def start_load_kv(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+    ) -> None:
+        if (metadata.side_channel_host is not None
+                and len(metadata.side_channel_host) > 0):
+            self._handshake(metadata)
+
+        if len(metadata) == 0 and len(metadata.finished_requests_ids) == 0:
+            return
+
+        logger.debug("start_load_kv %s", metadata)
+        self._update_meta_cache(metadata)
+
+        # release kvcache handles for finished/preempted reqs
+        for req_id in metadata.finished_requests_ids:
+            self._release_acquired_kvcache_handles(req_id)
+            self._release_allocated_kvcache_handle(req_id)
+
+        # split prefetch, load and send reqs
+        for seq_request_id, seq_request_meta in metadata.items():
+            if seq_request_meta.load_len > 0:
+                req_id = seq_request_meta.req_id
+                handles = self._acquired_kvcache_handles.get(req_id, None)
+                if handles is None or len(handles) == 0:
+                    logger.debug("Skip loading %s", req_id)
+                self._load_reqs.append(seq_request_meta)
+
+            if (seq_request_meta.query_len > 0 and seq_request_meta.state
+                    == AIBrixOffloadingConnectorRequestState.RECEIVING):
+                self._prefetch_reqs.append(seq_request_meta)
+
+            if (seq_request_meta.query_len > 0 and seq_request_meta.state
+                    == AIBrixOffloadingConnectorRequestState.SENDING):
+                prompt_len = seq_request_meta.prompt_len
+                context_len = seq_request_meta.context_len
+                query_len = seq_request_meta.query_len
+                # align to block boundary
+                aligned_context_len = round_down(context_len,
+                                                 self.cache_block_ntokens)
+                actual_query_len = context_len + query_len - \
+                    aligned_context_len
+                aligned_query_len = round_down(actual_query_len,
+                                               self.cache_block_ntokens)
+                aligned_prompt_len = round_down(prompt_len,
+                                                self.cache_block_ntokens)
+
+                if aligned_query_len == 0 or not self._need_to_process(
+                        aligned_prompt_len,
+                        aligned_context_len,
+                ):
+                    logger.debug("Skip sending %s", seq_request_meta.req_id)
+                else:
+                    self._send_reqs.append(seq_request_meta)
+
+        if len(self._prefetch_reqs) > 0:
+            self._prefetch_future = asyncio.run_coroutine_threadsafe(
+                self._start_prefetch_kv_async(self._prefetch_reqs),
+                self._kv_event_loop,
+            )
+
+        if len(self._load_reqs) > 0:
+            self._start_load_kv(self._load_reqs)
+
+    def _handshake(self, metadata: AIBrixOffloadingConnectorMetadata):
+        self._sidechannel_path = make_zmq_path("tcp",
+                                               metadata.side_channel_host,
+                                               metadata.side_channel_port)
+        self._sidechannel_sock = make_zmq_socket(
+            ctx=self._zmq_ctx,
+            path=self._sidechannel_path,
+            socket_type=zmq.REQ,
+            bind=False,
+        )
+        metadata = AIBrixOffloadingConnectorWorkerMeta(tp_rank=self.rank, )
+        encoder = msgspec.msgpack.Encoder()
+        encoded_data = encoder.encode(metadata)
+        self._sidechannel_sock.send(encoded_data)
+        logger.info(
+            "Worker %d in DP group %d is establishing side channel",
+            self.rank,
+            self.dp_rank,
+        )
+
+        # init side channel
+        self._sidechannel_thread = threading.Thread(
+            target=self._sidechannel_run,
+            args=(),
+            daemon=True,
+            name="worker_sidechannel_run")
+        self._sidechannel_thread.start()
+
+    def _sidechannel_run(self):
+        logger.debug(
+            "Starting worker side channel on path: %s",
+            self._sidechannel_path,
+        )
+        encoder = msgspec.msgpack.Encoder()
+        decoder = msgspec.msgpack.Decoder(
+            list[AIBrixOffloadingConnectorTestRequest])
+        poller = zmq.Poller()
+        poller.register(self._sidechannel_sock, zmq.POLLIN)
+        while True:
+            for sock, _ in poller.poll():
+                msg = sock.recv()
+                requests = decoder.decode(msg)
+                responses: list[AIBrixOffloadingConnectorTestResponse] = []
+                for request in requests:
+                    seq_existing_len = self._test_kv_impl(request)
+                    response = AIBrixOffloadingConnectorTestResponse(
+                        req_id=request.req_id,
+                        seq_existing_len=seq_existing_len,
+                    )
+                    responses.append(response)
+                encoded_data = encoder.encode(responses)
+                sock.send(encoded_data)
+
+    async def _start_prefetch_kv_async(
+        self, prefetch_reqs: list[AIBrixOffloadingConnectorRequestMetadata]
+    ) -> dict[str, tuple[int, KVCacheHandle]]:
+        logger.debug("Start prefetching %s", prefetch_reqs)
+        stats: dict[str, tuple[int, KVCacheHandle]] = {}
+        if self.kv_group is not None:
+            for seq_request_meta in prefetch_reqs:
+                num_fetched_tokens, handle = \
+                    self._recv_kv_impl(seq_request_meta)
+                if num_fetched_tokens > 0:
+                    seq_request_id = seq_request_meta.req_id
+                    stats[seq_request_id] = num_fetched_tokens, handle
+
+            if len(stats) > 0:
+                for idx, seq_request_meta in enumerate(prefetch_reqs):
+                    seq_request_id = seq_request_meta.req_id
+                    self._coll_tensor[idx], _ = stats.get(seq_request_id, \
+                        (0, None))
+                dist.all_reduce(self._coll_tensor[:idx], dist.ReduceOp.MIN,
+                                self.kv_group)
+
+                for idx, seq_request_meta in enumerate(prefetch_reqs):
+                    seq_request_id = seq_request_meta.req_id
+                    if self._coll_tensor[idx] > 0:
+                        num_fetched_tokens = self._coll_tensor[idx].item()
+                        num_handles = round_up(
+                            num_fetched_tokens,
+                            self.cache_block_ntokens,
+                        )
+                        stats[seq_request_id][0] = num_fetched_tokens
+                        stats[seq_request_id][1].truncate(num_handles)
+                    else:
+                        stats[seq_request_id][1].release()
+                        stats.pop(seq_request_id, None)
+
+        else:
+            for seq_request_meta in prefetch_reqs:
+                seq_request_id = seq_request_meta.req_id
+                num_fetched_tokens, handle = \
+                    self._recv_kv_impl(seq_request_meta)
+                if num_fetched_tokens > 0:
+                    stats[seq_request_id] = num_fetched_tokens, handle
+        return stats
+
+    def _recv_kv_impl(
+        self,
+        seq_request_meta: AIBrixOffloadingConnectorRequestMetadata,
+    ) -> tuple[int, Optional[KVCacheHandle]]:
+        logger.debug("_recv_kv_impl: %s", seq_request_meta)
+        seq_request_id = seq_request_meta.req_id
+        seq_cached_meta = self._meta_cache[seq_request_id]
+        seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+        assert seq_all_tokens is not None, "seq_all_tokens is None"
+        seq_context_len = seq_request_meta.context_len
+
+        prompt_len = seq_request_meta.prompt_len
+        query_len = seq_request_meta.query_len
+
+        if query_len < OFFLOADING_CONNECTOR_SKIP_THRESHOLD * \
+                self.engine_block_ntokens:
+            logger.debug(
+                "Skip Request[id=%s, context_len=%d, query_len=%d]",
+                seq_request_id,
+                seq_context_len,
+                query_len,
+            )
+            return 0, None
+
+        prefix = seq_all_tokens[:seq_context_len]
+        tokens = seq_all_tokens[seq_context_len:seq_context_len + query_len]
+
+        if self._metrics.time_measurement_enabled:
+            start = time.perf_counter()
+
+        seq_recv_len = 0
+
+        # get KV caches from offloading service
+        status = self.cache.acquire(prefix, tokens)
+
+        if not status.is_ok():
+            if not status.is_not_found():
+                log_every_n_seconds(
+                    logger,
+                    logging.ERROR,
+                    "Failed to get from offloading service: %s",
+                    3,
+                    str(status),
+                )
+            if self._metrics.time_measurement_enabled:
+                end = time.perf_counter()
+                lat_ms = (end - start) * 1000
+                self._metrics._recv_metrics.add(seq_context_len, query_len, 0,
+                                                lat_ms)
+            return 0, None
+
+        num_fetched_tokens, handle = status.value
+
+        # update recv_len
+        seq_recv_len += num_fetched_tokens
+
+        log_if(
+            logger,
+            logging.INFO,
+            "Request[id=%s, prompt_len=%d, context_len=%d] reused %d tokens",
+            seq_recv_len > 0,
+            seq_request_id,
+            prompt_len,
+            seq_context_len,
+            seq_recv_len,
+        )
+
+        if self._metrics.time_measurement_enabled:
+            end = time.perf_counter()
+            lat_ms = (end - start) * 1000
+            self._metrics._recv_metrics.add(seq_context_len, query_len,
+                                            seq_recv_len, lat_ms)
+
+        return seq_recv_len, handle
+
+    def _test_kv_impl(
+        self,
+        request: AIBrixOffloadingConnectorTestRequest,
+    ) -> int:
+        logger.debug("_test_kv_impl: %s", request)
+        seq_request_id = request.req_id
+        if (request.seq_token_ids is None or len(request.seq_token_ids) == 0):
+            return 0
+
+        prefix = None
+        tokens = TokenListView(request.seq_token_ids)
+
+        status = self.cache.exists(prefix, tokens)
+
+        if not status.is_ok():
+            return 0
+
+        seq_existing_len = status.value
+
+        log_if(
+            logger,
+            logging.DEBUG,
+            "Request[id=%s] exists %d tokens",
+            seq_existing_len > 0,
+            seq_request_id,
+            seq_existing_len,
+        )
+
+        return seq_existing_len
+
+    def _start_load_kv(
+        self,
+        load_reqs: list[AIBrixOffloadingConnectorRequestMetadata],
+    ):
+        logger.debug("Start non-layer-wise loading %s", load_reqs)
+        self._start_load_kv_impl(load_reqs)
+
+    def _start_load_kv_impl(
+        self,
+        load_reqs: list[AIBrixOffloadingConnectorRequestMetadata],
+    ):
+        tensors: list[torch.Tensor] = []
+        slot_mapping_offset = 0
+
+        for req in load_reqs:
+            seq_request_id = req.req_id
+            handles = self._acquired_kvcache_handles.get(seq_request_id, None)
+            if handles is None or len(handles) == 0:
+                continue
+
+            seq_cached_meta = self._meta_cache[seq_request_id]
+            seq_context_len = req.context_len - req.load_len
+            seq_load_len = round_up(req.load_len, self.cache_block_ntokens)
+
+            assert seq_context_len >= 0
+            logger.debug(
+                "Request[id=%s, context_len=%d, load_len=%d]",
+                seq_request_id,
+                seq_context_len,
+                seq_load_len,
+            )
+
+            handle = handles[0]
+            kv_blocks = handle.to_tensors()
+            num_fetched_tokens = len(kv_blocks) * self.cache_block_ntokens
+            assert num_fetched_tokens == seq_load_len, \
+                f"{num_fetched_tokens}!={seq_load_len}, handles={len(handles)}"
+
+            offset = seq_context_len
+            length = num_fetched_tokens
+
+            if slot_mapping_offset + length >= len(self._recv_slot_mapping):
+                new_recv_slot_mapping = torch.empty(
+                    len(self._recv_slot_mapping) * 2,
+                    dtype=torch.long,
+                    device="cuda",
+                )
+                new_recv_slot_mapping[:slot_mapping_offset].copy_(
+                    self._recv_slot_mapping[:slot_mapping_offset])
+                self._recv_slot_mapping = new_recv_slot_mapping
+
+            self._recv_slot_mapping[slot_mapping_offset:slot_mapping_offset +
+                                    length].copy_(
+                                        seq_cached_meta.
+                                        context_slot_mapping[offset:offset +
+                                                             length])
+            slot_mapping_offset += length
+            tensors.extend(kv_blocks)
+
+        if slot_mapping_offset == 0:
+            return
+
+        with torch.cuda.stream(self._load_stream):
+            reshape_and_cache_multi_layer(
+                tensors,
+                self.layers_kv_caches,
+                self._recv_slot_mapping[:slot_mapping_offset],
+                self.engine_block_ntokens,
+                self.kv_cache_dtype,
+                self.k_scales,
+                self.v_scales,
+                self.block_layout.name,
+            )
+
+    def save_kv_layer(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+        layer_name: str,
+        kv_layer: torch.Tensor,
+        attn_metadata: "AttentionMetadata",
+    ) -> None:
+        if len(self._send_reqs) == 0:
+            return
+
+        if not vllm.envs.VLLM_AIBRIX_TYPE3_USE_LAYER_WISE_SAVE:
+            return
+
+        logger.debug("save_kv_layer %s %s", layer_name, self._send_reqs)
+
+        is_first_layer = self.layer_name_idx_mapping[layer_name] == 0
+        lid = self.layer_name_idx_mapping[layer_name]
+        layer_tensors: list[torch.Tensor] = []
+        slot_mapping_offset = 0
+
+        if is_first_layer:
+            for req in self._send_reqs:
+                seq_req_id = req.req_id
+                # align to block boundary
+                aligned_context_len = round_down(req.context_len,
+                                                 self.cache_block_ntokens)
+                actual_query_len = req.context_len + req.query_len - \
+                    aligned_context_len
+                aligned_query_len = round_down(actual_query_len,
+                                               self.cache_block_ntokens)
+                aligned_prompt_len = round_down(req.prompt_len,
+                                                self.cache_block_ntokens)
+
+                if aligned_query_len == 0 or not self._need_to_process(
+                        aligned_prompt_len,
+                        aligned_context_len,
+                ):
+                    continue
+
+                seq_cached_meta = self._meta_cache[seq_req_id]
+                seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+                assert seq_all_tokens is not None, "seq_all_tokens is None"
+
+                prefix = seq_all_tokens[:aligned_context_len]
+                tokens = seq_all_tokens[
+                    aligned_context_len:aligned_context_len +
+                    aligned_query_len + self.cache_block_ntokens]
+                exists_status = self.cache.exists(prefix, tokens)
+                if exists_status.is_ok():
+                    num_existing_tokens = exists_status.value
+                    logger.debug(
+                        "Request[id=%s] send(%d) encounters %d existing tokens",
+                        seq_req_id, aligned_query_len, num_existing_tokens)
+                    if (aligned_query_len <= num_existing_tokens
+                            or not self._need_to_process(
+                                aligned_prompt_len,
+                                aligned_context_len + num_existing_tokens,
+                            )):
+                        continue
+                    else:
+                        # partially exists
+                        aligned_context_len += num_existing_tokens
+                        aligned_query_len -= num_existing_tokens
+
+                # update lengths
+                metadata[seq_req_id].context_len = aligned_context_len
+                metadata[seq_req_id].query_len = aligned_query_len
+
+                # allocate staging buffers that can hold kvcache for all layers
+                handle = self._allocate_for_request(seq_req_id,
+                                                    aligned_context_len,
+                                                    aligned_query_len)
+                if handle is None:
+                    continue
+                else:
+                    self._allocated_kvcache_handles[seq_req_id] = handle
+
+        for seq_req_id, _ in self._allocated_kvcache_handles.items():
+            if seq_req_id not in self._allocated_kvcache_handles or \
+                self._allocated_kvcache_handles[seq_req_id] is None:
+                return
+
+            seq_cached_meta = self._meta_cache[seq_req_id]
+            seq_context_len = metadata[seq_req_id].context_len
+
+            seq_allocated_handle = self._allocated_kvcache_handles[seq_req_id]
+            seq_allocated_tensors = seq_allocated_handle.to_tensors()
+            seq_num_tokens = len(
+                seq_allocated_tensors) * self.cache_block_ntokens
+            self._send_slot_mapping[slot_mapping_offset:slot_mapping_offset +
+                                    seq_num_tokens].copy_(
+                                        seq_cached_meta.context_slot_mapping[
+                                            seq_context_len:seq_context_len +
+                                            seq_num_tokens])
+            slot_mapping_offset += seq_num_tokens
+
+            # We are using LCND layout, so the tensors can be split by layers
+            layer_tensors.extend(
+                [t[lid:lid + 1] for t in seq_allocated_tensors])
+
+        if slot_mapping_offset == 0:
+            return
+
+        # wait for compute on current stream
+        curr_stream = torch.cuda.current_stream()
+        self._send_stream.wait_stream(curr_stream)
+
+        # use send stream to carry out async copy from HBM to DRAM
+        with torch.cuda.stream(self._send_stream):
+            reshape_and_offload_multi_layer(
+                layer_tensors,
+                [kv_layer],
+                self._send_slot_mapping[:slot_mapping_offset],
+                self.engine_block_ntokens,
+                self.kv_cache_dtype,
+                [self.k_scales[lid]],
+                [self.v_scales[lid]],
+                self.block_layout.name,
+            )
+
+    def _allocate_for_request(
+        self,
+        seq_request_id: str,
+        aligned_context_len: int,
+        aligned_query_len: int,
+    ) -> Optional[KVCacheHandle]:
+        logger.debug("_allocate_for_request %s", seq_request_id)
+
+        seq_cached_meta = self._meta_cache[seq_request_id]
+        seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+        assert seq_all_tokens is not None, "seq_all_tokens is None"
+
+        prefix = seq_all_tokens[:aligned_context_len]
+        tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                aligned_query_len]
+        status = self.cache.allocate_for(prefix, tokens)
+        if not status.is_ok():
+            log_every_n_seconds(logger, logging.ERROR,
+                                "Failed to allocate : %s", 3, str(status))
+            return None
+        return status.get()
+
+    @tag_wrapper({
+        "connector": "AIBrixOffloadingConnectorV1Type3",
+        "func": "wait_for_save"
+    })
+    def wait_for_save(
+        self,
+        metadata: AIBrixOffloadingConnectorMetadata,
+    ) -> None:
+        if len(self._send_reqs) == 0:
+            return
+
+        logger.debug("wait_for_save %s", metadata)
+        if vllm.envs.VLLM_AIBRIX_TYPE3_USE_LAYER_WISE_SAVE:
+            # wait for send stream to finish
+            self._send_stream.synchronize()
+
+        for req in self._send_reqs:
+            self._send_kv_impl(req)
+
+        self._send_reqs.clear()
+
+        # release all allocated handles
+        self._release_allocated_kvcache_handles()
+
+    def _send_kv_impl(
+        self,
+        seq_request_meta: AIBrixOffloadingConnectorRequestMetadata,
+    ) -> None:
+        logger.debug("_send_kv_impl: %s", seq_request_meta)
+        seq_request_id = seq_request_meta.req_id
+        seq_context_len = seq_request_meta.context_len
+        query_len = seq_request_meta.query_len
+        prompt_len = seq_request_meta.prompt_len
+
+        # align to block boundary
+        aligned_context_len = round_down(seq_context_len,
+                                         self.cache_block_ntokens)
+        actual_query_len = seq_context_len + query_len - aligned_context_len
+        aligned_query_len = round_down(actual_query_len,
+                                       self.cache_block_ntokens)
+        aligned_prompt_len = round_down(prompt_len, self.cache_block_ntokens)
+
+        if aligned_query_len == 0 or not self._need_to_process(
+                aligned_prompt_len,
+                aligned_context_len,
+        ):
+            return
+
+        assert prompt_len >= aligned_context_len + aligned_query_len, \
+            f"{prompt_len}<{aligned_context_len}+{aligned_query_len}"
+
+        seq_cached_meta = self._meta_cache[seq_request_id]
+        seq_all_tokens = seq_cached_meta.get_context_tokens_view()
+        assert seq_all_tokens is not None, "seq_all_tokens is None"
+
+        if not vllm.envs.VLLM_AIBRIX_TYPE3_USE_LAYER_WISE_SAVE:
+            prefix = seq_all_tokens[:aligned_context_len]
+            tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                    aligned_query_len +
+                                    self.cache_block_ntokens]
+            exists_status = self.cache.exists(prefix, tokens)
+            if exists_status.is_ok():
+                num_existing_tokens = exists_status.value
+                logger.debug(
+                    "Request[id=%s] send(%d) encounters %d existing tokens",
+                    seq_request_id, aligned_query_len, num_existing_tokens)
+                aligned_prompt_len = round_down(prompt_len,
+                                                self.cache_block_ntokens)
+                if (aligned_query_len <= num_existing_tokens
+                        or not self._need_to_process(
+                            aligned_prompt_len,
+                            aligned_context_len + num_existing_tokens,
+                        )):
+                    return
+                else:
+                    # partially exists
+                    aligned_context_len += num_existing_tokens
+                    aligned_query_len -= num_existing_tokens
+            seq_allocated_handle = self._allocate_for_request(
+                seq_request_id, aligned_context_len, aligned_query_len)
+        else:
+            seq_allocated_handle = self._allocated_kvcache_handles.pop(
+                seq_request_id, None)
+
+        if seq_allocated_handle is None:
+            return
+
+        seq_allocated_tensors = seq_allocated_handle.to_tensors()
+        aligned_query_len = min(
+            aligned_query_len,
+            len(seq_allocated_tensors) * self.cache_block_ntokens,
+        )
+
+        prefix = seq_all_tokens[:aligned_context_len]
+        tokens = seq_all_tokens[aligned_context_len:aligned_context_len +
+                                aligned_query_len]
+
+        if self._metrics.time_measurement_enabled:
+            start = time.perf_counter()
+
+        total_sent = 0
+        length = aligned_query_len
+
+        if not vllm.envs.VLLM_AIBRIX_TYPE3_USE_LAYER_WISE_SAVE:
+            offset = len(prefix)
+            slot_mapping = seq_cached_meta.context_slot_mapping[offset:offset +
+                                                                length]
+
+            # wait for compute on current stream
+            curr_stream = torch.cuda.current_stream()
+            self._send_stream.wait_stream(curr_stream)
+
+            with perf_timer() as get_kernel_offload_dur_ms:
+                with torch.cuda.stream(self._send_stream):
+                    reshape_and_offload_multi_layer(
+                        seq_allocated_tensors,
+                        self.layers_kv_caches,
+                        slot_mapping,
+                        self.engine_block_ntokens,
+                        self.kv_cache_dtype,
+                        self.k_scales,
+                        self.v_scales,
+                        self.block_layout.name,
+                    )
+                self._send_stream.synchronize()
+
+            logger.info("Request[id=%s] offloads %d tokens in %.4f ms",
+                        seq_request_id, length, get_kernel_offload_dur_ms())
+
+        # put KV caches to offloading service
+        status = self.cache.put(prefix, tokens, seq_allocated_handle)
+        if not status.is_ok():
+            log_every_n_seconds(logger, logging.ERROR,
+                                "Failed to put to offloading service: %s", 3,
+                                str(status))
+        else:
+            total_sent += length
+
+            log_if(
+                logger,
+                logging.INFO,
+                "Request[id=%s, prompt_len=%d, context_len=%d] sent %d tokens",
+                total_sent > 0,
+                seq_request_id,
+                prompt_len,
+                seq_context_len,
+                total_sent,
+            )
+
+        if self._metrics.time_measurement_enabled:
+            end = time.perf_counter()
+            lat_ms = (end - start) * 1000
+            self._metrics._send_metrics.add(seq_context_len, query_len,
+                                            total_sent, lat_ms)
+
+    def _need_to_process(self, aligned_prompt_len: int,
+                         aligned_context_len: int) -> bool:
+        return aligned_prompt_len - aligned_context_len >= max(
+            OFFLOADING_CONNECTOR_SKIP_THRESHOLD * self.engine_block_ntokens,
+            self.cache_block_ntokens,
+        )
+
+    def get_finished(
+        self, metadata: AIBrixOffloadingConnectorMetadata,
+        finished_req_ids: set[str]
+    ) -> tuple[Optional[set[str]], Optional[set[str | tuple[str, int]]]]:
+        if len(metadata) == 0:
+            return None, None
+
+        # wait until prefetching is done
+        prefetch_output: set[tuple[str, int]] = set()
+        req_ids: set[str] = set()
+        for req in self._prefetch_reqs:
+            req_ids.add(req.req_id)
+        for req in self._load_reqs:
+            req_ids.add(req.req_id)
+
+        if self._prefetch_future is not None:
+            stats = self._prefetch_future.result()
+            for req_id in req_ids:
+                (num_fetched_tokens, handle) = stats.get(req_id, (0, None))
+                if handle is not None:
+                    self._acquired_kvcache_handles.setdefault(req_id, [])\
+                        .append(handle)
+                    prefetch_output.add((req_id, num_fetched_tokens))
+                elif num_fetched_tokens > 0:
+                    prefetch_output.add((req_id, num_fetched_tokens))
+                else:
+                    prefetch_output.add((req_id, 0))
+            self._prefetch_future = None
+        else:
+            for req_id in req_ids:
+                prefetch_output.add((req_id, 0))
+
+        # wait until loading is done
+        if len(self._load_reqs) > 0:
+            self._load_stream.synchronize()
+            # release curr handle once loading is complete
+            for req in self._load_reqs:
+                seq_request_id = req.req_id
+                self._pop_acquired_kvcache_handle(seq_request_id)
+
+        self._prefetch_reqs.clear()
+        self._load_reqs.clear()
+
+        logger.debug("get_finished: %s", prefetch_output)
+
+        if self._metrics.time_measurement_enabled:
+            log_every_n_seconds(self._metrics, logging.INFO, "UNUSED", 10)
+
+        return None, prefetch_output
+
+
+class AIBrixOffloadingConnector(KVConnectorBase_V1):
+    """AIBrixOffloadingConnector is a KVConnector that offloads KV caches
+    to the kv cache offloading service.
+    """
+
+    def __init__(self, config: "VllmConfig", role: KVConnectorRole):
+        super().__init__(vllm_config=config, role=role)
+
+        self.connector_scheduler: Optional[
+            AIBrixOffloadingConnectorScheduler] = None
+
+        self.connector_worker: Optional[AIBrixOffloadingConnectorWorker] = None
+        if role == KVConnectorRole.SCHEDULER:
+            self.connector_scheduler = AIBrixOffloadingConnectorScheduler(
+                config)
+        elif role == KVConnectorRole.WORKER:
+            self.connector_worker = AIBrixOffloadingConnectorWorker(config)
+
+    # ==============================
+    # Worker-side methods
+    # ==============================
+
+    @delegate_to("connector_worker")
+    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):
+        """
+        Initialize with the KV caches. Useful for pre-registering the
+        KV Caches in the KVConnector (e.g. for NIXL).
+
+        Args: kv_caches:
+            dictionary of layer names, kv cache
+        """
+        pass
+
+    def start_load_kv(self, forward_context: "ForwardContext",
+                      **kwargs) -> None:
+        """
+        Start loading the KV cache from the connector to vLLM's paged
+        KV buffer. This is called from the forward context before the
+        forward pass to enable async loading during model execution.
+
+        Args:
+            forward_context (ForwardContext): the forward context.
+            **kwargs: additional arguments for the load operation
+
+        Note:
+            The number of elements in kv_caches and layer_names should be
+            the same.
+
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        return self.connector_worker.start_load_kv(self._connector_metadata)
+
+    def wait_for_layer_load(self, layer_name: str) -> None:
+        """
+        Block until the KV for a specific layer is loaded into vLLM's
+        paged buffer. This is called from within attention layer to ensure
+        async copying from start_load_kv is complete.
+
+        This interface will be useful for layer-by-layer pipelining.
+
+        Args:
+            layer_name: the name of that layer
+        """
+        pass
+
+    def save_kv_layer(self, layer_name: str, kv_layer: torch.Tensor,
+                      attn_metadata: "AttentionMetadata", **kwargs) -> None:
+        """
+        Start saving a layer of KV cache from vLLM's paged buffer
+        to the connector. This is called from within attention layer to
+        enable async copying during execution.
+
+        Args:
+            layer_name (str): the name of the layer.
+            kv_layer (torch.Tensor): the paged KV buffer of the current
+                layer in vLLM.
+            attn_metadata (AttentionMetadata): the attention metadata.
+            **kwargs: additional arguments for the save operation.
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        self.connector_worker.save_kv_layer(self._connector_metadata,
+                                            layer_name, kv_layer,
+                                            attn_metadata)
+
+    def wait_for_save(self) -> None:
+        """
+        Block until all the save operations is done. This is called
+        as the forward context exits to ensure that the async saving
+        from save_kv_layer is complete before finishing the forward.
+
+        This prevents overwrites of paged KV buffer before saving done.
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        self.connector_worker.wait_for_save(self._connector_metadata)
+
+    def get_finished(
+        self, finished_req_ids: set[str]
+    ) -> tuple[Optional[set[str]], Optional[set[str | tuple[str, int]]]]:
+        """
+        Notifies worker-side connector ids of requests that have
+        finished generating tokens.
+
+        Returns:
+            ids of requests that have finished asynchronous transfer
+            (requests that previously returned True from request_finished()),
+            tuple of (sending/saving ids, recving/loading ids or
+            (recving/loading id, num. of recv'ed/loaded tokens) pairs).
+            The finished saves/sends req ids must belong to a set provided in a
+            call to this method (this call or a prior one).
+        """
+        assert self.connector_worker is not None
+        assert isinstance(
+            self._connector_metadata,
+            AIBrixOffloadingConnectorMetadata,
+        )
+        return self.connector_worker.get_finished(self._connector_metadata,
+                                                  finished_req_ids)
+
+    # ==============================
+    # Scheduler-side methods
+    # ==============================
+
+    def get_num_new_matched_tokens(
+        self,
+        request: "Request",
+        num_computed_tokens: int,
+    ) -> tuple[int, bool]:
+        """
+        Get number of new tokens that can be loaded from the
+        external KV cache beyond the num_computed_tokens.
+
+        Args:
+            request (Request): the request object.
+            num_computed_tokens (int): the number of locally
+                computed tokens for this request
+
+        Returns:
+            A tuple with the following elements:
+                - The number of tokens that can be loaded from the
+                  external KV cache beyond what is already computed.
+                - `True` if external KV cache tokens will be loaded
+                  asynchronously (between scheduler steps). Must be
+                  'False' if the first element is 0.
+        """
+        return 0, False
+
+    @delegate_to("connector_scheduler")
+    def update_state_after_alloc(self, request: "Request",
+                                 blocks: "KVCacheBlocks",
+                                 num_external_tokens: int):
+        """
+        Update KVConnector state after block allocation.
+
+        If get_num_new_matched_tokens previously returned True for a
+        request, this function may be called twice for that same request -
+        first when blocks are allocated for the connector tokens to be
+        asynchronously loaded into, and second when any additional blocks
+        are allocated, after the load/transfer is complete.
+
+        Args:
+            request (Request): the request object.
+            blocks (KVCacheBlocks): the blocks allocated for the request.
+            num_external_tokens (int): the number of tokens that will be
+                loaded from the external KV cache.
+        """
+        return
+
+    @delegate_to("connector_scheduler")
+    def load_and_prefetch(
+        self,
+        request: "Request",
+        num_to_load_tokens: int,
+        num_to_prefetch_tokens: int,
+    ) -> int:
+        """
+        Get num of tokens that can be scheduled for prefetching.
+
+        Args:
+            request (Request): the request object.
+            num_to_load_tokens (int): the number of tokens that will be
+                scheduled to load to GPU memory.
+            num_to_prefetch_tokens (int): the number of tokens that will be
+                scheduled to prefetch from the external KV cache.
+        Returns:
+            Num of tokens to prefetch.
+        """
+        return 0
+
+    @delegate_to("connector_scheduler")
+    def add_requests(
+        self,
+        requests: list["Request"],
+    ) -> None:
+        """
+        Add requests.
+
+        Args:
+            requests (list[Request]): list of request objects.
+        """
+        return
+
+    @delegate_to("connector_scheduler")
+    def test_request(
+        self,
+        request: "Request",
+    ) -> bool:
+        """
+        Test if request has matched tokens.
+
+        Args:
+            request (Request): the request object.
+        Returns:
+            True if the request has matched tokens.
+        """
+        return False
+
+    @delegate_to("connector_scheduler")
+    def build_connector_meta(
+            self, scheduler_output: "SchedulerOutput") -> KVConnectorMetadata:
+        """
+        Build the connector metadata for this step.
+
+        This function should NOT modify fields in the scheduler_output.
+        Also, calling this function will reset the state of the connector.
+
+        Args:
+            scheduler_output (SchedulerOutput): the scheduler output object.
+        """
+        pass
+
+    @delegate_to("connector_scheduler")
+    def request_finished(
+        self,
+        request: "Request",
+        block_ids: list[int],
+    ) -> tuple[bool, Optional[dict[str, Any]]]:
+        """
+        Called when a request has finished, before its blocks are freed.
+
+        Returns:
+            True if the request is being saved/sent asynchronously and blocks
+            should not be freed until the request_id is returned from
+            get_finished().
+            Optional KVTransferParams to be included in the request outputs
+            returned by the engine.
+        """
+        pass
+
+    @delegate_to("connector_scheduler")
+    def request_preempted(
+        self,
+        request: "Request",
+    ) -> None:
+        """
+        Called when a request has been preempted.
+        """
+        return
diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/base.py b/vllm/distributed/kv_transfer/kv_connector/v1/base.py
index f80b5eba2..146fbc2b9 100644
--- a/vllm/distributed/kv_transfer/kv_connector/v1/base.py
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/base.py
@@ -124,6 +124,20 @@ class KVConnectorBase_V1(ABC):
         """
         return
 
+    def start_load_kv_before_update(self, **kwargs) -> dict[str, int]:
+        """
+        Start loading the KV cache from the connector to vLLM's paged
+        KV buffer before gpu runner updating its states.
+
+        Args:
+            **kwargs: additional arguments for the load operation
+
+        Returns:
+            dict[str, int]: a dictionary of request ids and the number of
+            tokens loaded for each request.
+        """
+        return {}
+
     @abstractmethod
     def start_load_kv(self, forward_context: "ForwardContext",
                       **kwargs) -> None:
@@ -187,7 +201,7 @@ class KVConnectorBase_V1(ABC):
 
     def get_finished(
         self, finished_req_ids: set[str]
-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:
+    ) -> tuple[Optional[set[str]], Optional[set[str | tuple[str, int]]]]:
         """
         Notifies worker-side connector ids of requests that have
         finished generating tokens.
@@ -195,7 +209,8 @@ class KVConnectorBase_V1(ABC):
         Returns:
             ids of requests that have finished asynchronous transfer
             (requests that previously returned True from request_finished()),
-            tuple of (sending/saving ids, recving/loading ids).
+            tuple of (sending/saving ids, recving/loading ids or
+            (prefetching id, num. of prefetched tokens) pairs).
             The finished saves/sends req ids must belong to a set provided in a
             call to this method (this call or a prior one).
         """
@@ -281,3 +296,12 @@ class KVConnectorBase_V1(ABC):
             returned by the engine.
         """
         return False, None
+
+    def request_preempted(
+        self,
+        request: "Request",
+    ) -> None:
+        """
+        Called when a request has been preempted.
+        """
+        return
diff --git a/vllm/distributed/kv_transfer/kv_connector_agent.py b/vllm/distributed/kv_transfer/kv_connector_agent.py
index 8633fdaf5..13f5b544f 100644
--- a/vllm/distributed/kv_transfer/kv_connector_agent.py
+++ b/vllm/distributed/kv_transfer/kv_connector_agent.py
@@ -9,6 +9,8 @@ This implementation is a shim wrapper on two APIs exposed by `kv_connector`:
 from typing import TYPE_CHECKING, Union
 
 if TYPE_CHECKING:
+    from .kv_transfer_metrics import (KVTransferMetrics,
+                                      KVTransferMetricsExporter)
     from vllm.worker.model_runner import ModelInputForGPUWithSamplingMetadata
     from vllm.config import VllmConfig
 
@@ -50,6 +52,23 @@ class KVTransferAgent:
         self.connector = KVConnectorFactory.create_connector_v0(
             rank, local_rank, config)
 
+    @property
+    def metrics(self) -> "KVTransferMetrics":
+        """
+        Get the metrics object associated with the connector.
+        Returns:
+            KVTransferMetrics: The metrics object.
+        """
+        return self.connector.metrics
+
+    def get_metrics_exporter_cls(self) -> "KVTransferMetricsExporter":
+        """
+        Get the metrics object associated with the connector.
+        Returns:
+            KVTransferMetrics: The metrics object.
+        """
+        return self.connector.get_metrics_exporter_cls()
+
     def send_kv_caches_and_hidden_states(
         self,
         model_executable: torch.nn.Module,
diff --git a/vllm/distributed/kv_transfer/kv_transfer_metadata.py b/vllm/distributed/kv_transfer/kv_transfer_metadata.py
new file mode 100644
index 000000000..95c129633
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_transfer_metadata.py
@@ -0,0 +1,119 @@
+# SPDX-License-Identifier: Apache-2.0
+
+from dataclasses import dataclass
+from typing import TYPE_CHECKING, List, Optional
+
+from vllm.sequence import SequenceGroupMetadata
+from vllm.utils import PyObjectCache
+
+if TYPE_CHECKING:
+    from vllm.worker.model_runner import GPUModelRunnerBase
+
+
+@dataclass
+class SequenceGroupToKVTransfer:
+    """
+    Metadata mainly used by kv_both scenarios.
+    """
+    prompt_len: Optional[int]
+    # Context tokens in prefix cache
+    context_tokens: Optional[List[int]]
+
+
+def seq_group_to_kv_transfer_builder():
+    return SequenceGroupToKVTransfer(
+        prompt_len=0,
+        context_tokens=None,
+    )
+
+
+class KVTransferMetadataCache:
+    """Used to cache SequenceGroupToKVTransfer objects between
+    scheduler iterations.
+    """
+
+    def __init__(self):
+        self._seq_group_to_kv_transfer_cache: PyObjectCache = PyObjectCache(
+            seq_group_to_kv_transfer_builder)
+
+    def get_cached_seq_group_to_kv_transfer(self):
+        obj = self._seq_group_to_kv_transfer_cache.get_object()
+        obj.prompt_len = 0
+        obj.context_tokens = None
+        return obj
+
+    def reset(self):
+        self._seq_group_to_kv_transfer_cache.reset()
+
+
+class KVTransferMetadata:
+    """Metadata for input sequences. Used in KV transfer.
+
+    Args:
+        seq_groups: List of batched sequence groups.
+        model_input_builder: Model input builder for rebuilding model input.
+    """
+
+    def __init__(
+        self,
+        seq_groups: List[SequenceGroupToKVTransfer],
+    ) -> None:
+        self.seq_groups = seq_groups
+        # only driver has seq_group_metadata_list and runner
+        self.seq_group_metadata_list: Optional[
+            List[SequenceGroupMetadata]] = None
+        self.runner: Optional[GPUModelRunnerBase] = None
+
+    @staticmethod
+    def prepare(
+        seq_group_metadata_list: List[SequenceGroupMetadata],
+        cache: Optional[KVTransferMetadataCache] = None,
+    ) -> "KVTransferMetadata":
+        """
+        context_lens include num of tokens in prefix cache.
+        """
+        seq_groups: List[SequenceGroupToKVTransfer] = []
+        ctx_idx = 0
+        for seq_group_metadata in seq_group_metadata_list:
+            seq_ids = list(seq_group_metadata.seq_data.keys())
+            for i in range(len(seq_ids)):
+                if cache is not None:
+                    seq_group_obj = cache.get_cached_seq_group_to_kv_transfer()
+                else:
+                    seq_group_obj = seq_group_to_kv_transfer_builder()
+
+                seq_id = seq_ids[i]
+                seq_data = seq_group_metadata.seq_data[seq_id]
+
+                seq_group_obj.prompt_len = seq_data.get_prompt_len()
+
+                seq_groups.append(seq_group_obj)
+
+            # prefill sequence group only has one sequence
+            if seq_group_metadata.is_prompt:
+                seq_id = seq_ids[0]
+                nblocks = len(seq_group_metadata.computed_block_nums or [])
+                seq_data = seq_group_metadata.seq_data[seq_id]
+                context_len = seq_data.get_num_cached_tokens()
+                # prefix caching:
+                #     context_len > 0 and nblocks > 0
+                # chunked prefill intermediate steps:
+                #     context_len > 0 and nblocks == 0
+                #
+                # We only carry context_tokens for prefix caching since
+                # chunked prefill intermediate steps can use the cached
+                # context tokens in the offloading connector.
+                if context_len > 0 and nblocks > 0:
+                    seq_groups[-1].context_tokens = seq_data.get_token_ids(
+                    )[:context_len]
+
+            ctx_idx += len(seq_ids)
+
+        if cache is not None:
+            cache.reset()
+
+        metadata = KVTransferMetadata(seq_groups=seq_groups)
+        return metadata
+
+    def __repr__(self) -> str:
+        return f"KVTransferMetadata(seq_groups={self.seq_groups})"
diff --git a/vllm/distributed/kv_transfer/kv_transfer_metrics.py b/vllm/distributed/kv_transfer/kv_transfer_metrics.py
new file mode 100644
index 000000000..e92fc8935
--- /dev/null
+++ b/vllm/distributed/kv_transfer/kv_transfer_metrics.py
@@ -0,0 +1,48 @@
+# SPDX-License-Identifier: Apache-2.0
+from abc import ABC, abstractmethod
+from typing import Dict
+
+
+class KVTransferMetrics(ABC):
+    """
+    KVTransferMetrics is used to collect metrics for KVTransferAgent.
+    """
+
+    @abstractmethod
+    def reset(self) -> None:
+        raise NotImplementedError
+
+    @abstractmethod
+    def __str__(self) -> str:
+        raise NotImplementedError
+
+
+class KVTransferMetricsExporter(ABC):
+    """
+    KVTransferMetrics is used to collect metrics for KVTransferAgent.
+    """
+
+    def __init__(
+        self,
+        *,
+        prefix,
+        labelnames,
+        gauge_cls,
+        counter_cls,
+        histogram_cls,
+    ):
+        self._prefix = prefix
+        self._labelnames = labelnames
+        self._gauge_cls = gauge_cls
+        self._counter_cls = counter_cls
+        self._histogram_cls = histogram_cls
+
+    @abstractmethod
+    def export(
+        self,
+        *,
+        metrics: KVTransferMetrics,
+        labels: Dict[str, str],
+    ) -> None:
+        """Export metrics to external systems."""
+        raise NotImplementedError
diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py
index 8fccf9bd2..f43c2e054 100644
--- a/vllm/engine/llm_engine.py
+++ b/vllm/engine/llm_engine.py
@@ -21,6 +21,7 @@ from vllm.config import (DecodingConfig, LoRAConfig, ModelConfig,
                          ObservabilityConfig, ParallelConfig, SchedulerConfig,
                          VllmConfig)
 from vllm.core.scheduler import ScheduledSequenceGroup, SchedulerOutputs
+from vllm.distributed.kv_transfer import get_kv_transfer_group
 from vllm.engine.arg_utils import EngineArgs
 from vllm.engine.metrics_types import StatLoggerBase, Stats
 from vllm.engine.output_processor.interfaces import (
@@ -1784,6 +1785,12 @@ class LLMEngine:
         else:
             spec_decode_metrics = None
 
+        if (self.vllm_config.kv_transfer_config is not None and
+                self.vllm_config.kv_transfer_config.is_kv_transfer_instance):
+            kv_transfer_metrics = get_kv_transfer_group().metrics
+        else:
+            kv_transfer_metrics = None
+
         return Stats(
             now=now,
             # System stats
@@ -1806,6 +1813,7 @@ class LLMEngine:
             time_per_output_tokens_iter=time_per_output_tokens_iter,
             spec_decode_metrics=spec_decode_metrics,
             num_preemption_iter=num_preemption_iter,
+            kv_transfer_metrics=kv_transfer_metrics,
 
             # Request stats
             #   Latency
diff --git a/vllm/engine/metrics.py b/vllm/engine/metrics.py
index 8d51f0472..6510cda6f 100644
--- a/vllm/engine/metrics.py
+++ b/vllm/engine/metrics.py
@@ -10,6 +10,7 @@ import numpy as np
 import prometheus_client
 
 from vllm.config import SupportsMetricsInfo, VllmConfig
+from vllm.distributed.kv_transfer import get_kv_transfer_group
 from vllm.engine.metrics_types import StatLoggerBase, Stats
 from vllm.executor.ray_utils import ray
 from vllm.logger import init_logger
@@ -223,6 +224,18 @@ class Metrics:
             documentation="Number of emitted tokens.",
             labelnames=labelnames))
 
+        self.kv_transfer_metrics_exporter = None
+        if (vllm_config.kv_transfer_config is not None
+                and vllm_config.kv_transfer_config.is_kv_transfer_instance):
+            exporter_cls = get_kv_transfer_group().get_metrics_exporter_cls()
+            if exporter_cls is not None:
+                self.kv_transfer_metrics_exporter = exporter_cls(
+                    prefix="vllm:",
+                    labelnames=labelnames,
+                    gauge_cls=self._gauge_cls,
+                    counter_cls=self._counter_cls,
+                    histogram_cls=self._histogram_cls,
+                )
 
 # --8<-- [end:metrics-definitions]
 
@@ -440,6 +453,9 @@ class LoggingStatLogger(StatLoggerBase):
                     self._format_spec_decode_metrics_str(
                         self.spec_decode_metrics))
 
+            if stats.kv_transfer_metrics is not None:
+                log_fn(str(stats.kv_transfer_metrics))
+
             self._reset(stats, prompt_throughput, generation_throughput)
 
     def _reset(self, stats, prompt_throughput, generation_throughput) -> None:
@@ -450,6 +466,8 @@ class LoggingStatLogger(StatLoggerBase):
         self.spec_decode_metrics = None
         self.last_prompt_throughput = prompt_throughput
         self.last_generation_throughput = generation_throughput
+        if stats.kv_transfer_metrics is not None:
+            stats.kv_transfer_metrics.reset()
 
     def _format_spec_decode_metrics_str(
             self, metrics: "SpecDecodeWorkerMetrics") -> str:
@@ -569,6 +587,12 @@ class PrometheusStatLogger(StatLoggerBase):
             stats.max_num_generation_tokens_requests)
         self._log_histogram(self.metrics.histogram_max_tokens_request,
                             stats.max_tokens_requests)
+        if (stats.kv_transfer_metrics is not None
+                and self.metrics.kv_transfer_metrics_exporter is not None):
+            self.metrics.kv_transfer_metrics_exporter.export(
+                metrics=stats.kv_transfer_metrics,
+                labels=self.labels,
+            )
 
     def log(self, stats: Stats):
         """Logs to prometheus and tracked stats every iteration."""
@@ -606,6 +630,8 @@ class PrometheusStatLogger(StatLoggerBase):
             self.num_generation_tokens = []
             self.last_local_log = stats.now
             self.spec_decode_metrics = None
+            if stats.kv_transfer_metrics is not None:
+                stats.kv_transfer_metrics.reset()
 
     def info(self, type: str, obj: SupportsMetricsInfo) -> None:
         # Info type metrics are syntactic sugar for a gauge permanently set to 1
diff --git a/vllm/engine/metrics_types.py b/vllm/engine/metrics_types.py
index 9375dc4c4..bdb8e9e6f 100644
--- a/vllm/engine/metrics_types.py
+++ b/vllm/engine/metrics_types.py
@@ -19,6 +19,7 @@ from dataclasses import dataclass
 from typing import List, Optional
 
 from vllm.config import SupportsMetricsInfo, VllmConfig
+from vllm.distributed.kv_transfer.kv_transfer_metrics import KVTransferMetrics
 from vllm.spec_decode.metrics import SpecDecodeWorkerMetrics
 
 
@@ -66,6 +67,7 @@ class Stats:
     max_lora: str
 
     spec_decode_metrics: Optional["SpecDecodeWorkerMetrics"] = None
+    kv_transfer_metrics: Optional["KVTransferMetrics"] = None
 
 
 class StatLoggerBase(ABC):
diff --git a/vllm/envs.py b/vllm/envs.py
index 80c5f289b..a525b0894 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -127,6 +127,9 @@ if TYPE_CHECKING:
     VLLM_TOOL_PARSE_REGEX_TIMEOUT_SECONDS: int = 1
     VLLM_SLEEP_WHEN_IDLE: bool = False
     VLLM_MQ_MAX_CHUNK_BYTES_MB: int = 16
+    VLLM_AIBRIX_SIDE_CHANNEL_PORT: int = 6667
+    VLLM_AIBRIX_SYNC_GRANULARITY: str = "PER_OP"
+    VLLM_AIBRIX_TYPE3_USE_LAYER_WISE_SAVE: bool = True
 
 
 def get_default_cache_root():
@@ -870,6 +873,21 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # processes via zmq.
     "VLLM_MQ_MAX_CHUNK_BYTES_MB":
     lambda: int(os.getenv("VLLM_MQ_MAX_CHUNK_BYTES_MB", "16")),
+
+    # Port used by AIBrix connector side channel
+    "VLLM_AIBRIX_SIDE_CHANNEL_PORT":
+    lambda: int(os.getenv("VLLM_AIBRIX_SIDE_CHANNEL_PORT", "6667")),
+
+    # Specify the sync granularity used by AIBrix controllers. Please refer to
+    # AIBrixOffloadingConnectorSyncGranularity for more details.
+    "VLLM_AIBRIX_SYNC_GRANULARITY":
+    lambda: os.environ.get("VLLM_AIBRIX_SYNC_GRANULARITY", "PER_OP").upper(),
+
+    "VLLM_AIBRIX_TYPE3_USE_LAYER_WISE_SAVE":
+    lambda: (
+        os.environ.get("VLLM_AIBRIX_TYPE3_USE_LAYER_WISE_SAVE", "true").lower()\
+            in ("1", "true")
+    ),
 }
 
 # --8<-- [end:env-vars-definition]
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 3d7bbe7e0..9499ea6b6 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -82,6 +82,10 @@ class Scheduler(SchedulerInterface):
             self.connector = KVConnectorFactory.create_connector_v1(
                 config=self.vllm_config, role=KVConnectorRole.SCHEDULER)
 
+        self.use_three_phase_kv_sched = (
+            self.connector and self.vllm_config.kv_transfer_config.kv_connector
+            == "AIBrixOffloadingConnectorV1Type3")
+
         self.kv_event_publisher = EventPublisherFactory.create(
             self.kv_events_config,
             vllm_config.parallel_config.data_parallel_rank,
@@ -97,6 +101,7 @@ class Scheduler(SchedulerInterface):
         # Priority queues for requests.
         self.waiting: deque[Request] = deque()
         self.running: list[Request] = []
+        self.prefetching: deque[Request] = deque()
 
         # The request IDs that are finished in between the previous and the
         # current steps. This is used to notify the workers about the finished
@@ -107,6 +112,12 @@ class Scheduler(SchedulerInterface):
         # KV Connector: requests in process of async KV loading or recving
         self.finished_recving_kv_req_ids: set[str] = set()
 
+        self.prefetching_req_stats: dict[str, int] = {}
+        self.nonprefetchable_req_ids: set[str] = set()
+        self.local_cached_req_ids: set[str] = set()
+        self.testing_req_ids: set[str] = set()
+        self.max_testing_reqs = 32
+
         # OPTIMIZATION: Cache the CachedRequestData objects to avoid creating
         # them at each scheduling step.
         # Request id -> deque of CachedRequestData
@@ -183,6 +194,7 @@ class Scheduler(SchedulerInterface):
         req_to_new_block_ids: dict[str, tuple[list[int], ...]] = {}
         num_scheduled_tokens: dict[str, int] = {}
         token_budget = self.max_num_scheduled_tokens
+        prefetching_token_budget = token_budget
         # Encoder-related.
         scheduled_encoder_inputs: dict[str, list[int]] = {}
         encoder_budget = self.max_num_encoder_input_tokens
@@ -247,9 +259,22 @@ class Scheduler(SchedulerInterface):
                 if new_blocks is None:
                     # The request cannot be scheduled.
                     # Preempt the lowest-priority request.
-                    preempted_req = self.running.pop()
+                    if self.prefetching:
+                        preempted_req = self.prefetching.pop()
+                        preempted_req.status = RequestStatus.WAITING
+                        assert self.connector is not None
+                        self.connector.request_preempted(preempted_req)
+                        # preempted request becomes prefetchable again
+                        self.nonprefetchable_req_ids.discard(
+                            preempted_req.request_id)
+                        self.local_cached_req_ids.discard(
+                            preempted_req.request_id)
+                        self.prefetching_req_stats.pop(
+                            preempted_req.request_id, None)
+                    else:
+                        preempted_req = self.running.pop()
+                        preempted_req.status = RequestStatus.PREEMPTED
                     self.kv_cache_manager.free(preempted_req)
-                    preempted_req.status = RequestStatus.PREEMPTED
                     preempted_req.num_computed_tokens = 0
                     if self.log_stats:
                         preempted_req.record_event(
@@ -311,13 +336,143 @@ class Scheduler(SchedulerInterface):
                 if req.lora_request and req.lora_request.lora_int_id > 0)
             assert len(scheduled_loras) <= self.lora_config.max_loras
 
+        # Second, schedule the PREFETCHING requests.
+
+        # Use a temporary deque to collect requests that need to be skipped
+        # and put back at the head of the prefetching queue later
+        skipped_prefetching_requests: deque[Request] = deque()
+
+        if not preempted_reqs and self.use_three_phase_kv_sched:
+            while self.prefetching and prefetching_token_budget > 0:
+                request = self.prefetching[0]
+                req_id = request.request_id
+
+                num_prompt_tokens = request.num_prompt_tokens
+                num_loaded_tokens = request.num_computed_tokens
+
+                # will allocate slots for these prefetched tokens
+                num_new_prefetched_tokens = \
+                    self.prefetching_req_stats.get(req_id, 0)
+
+                num_prefetched_tokens = \
+                    num_loaded_tokens + num_new_prefetched_tokens
+
+                if num_new_prefetched_tokens == 0:
+                    # prefetched zero tokens in last prefetching, start to
+                    # compute from here
+                    logger.debug(
+                        "Request[id=%s, prompt_len=%d, loaded_len=%d, "
+                        "prefetched_len=%d] stopped prefetching",
+                        req_id,
+                        num_prompt_tokens,
+                        num_loaded_tokens,
+                        num_prefetched_tokens,
+                    )
+
+                    self.prefetching.popleft()
+                    # append it to the head of waiting queue
+                    request.status = RequestStatus.WAITING
+                    self.waiting.appendleft(request)
+                    self.nonprefetchable_req_ids.add(req_id)
+                    continue
+                elif num_prefetched_tokens == num_prompt_tokens:
+                    # this will be a full hit, let remain the last token as
+                    # uncomputed
+                    num_prefetched_tokens -= 1
+                    num_new_prefetched_tokens -= 1
+                    num_to_prefetch_tokens = 0
+                else:
+                    num_to_prefetch_tokens = max(
+                        0, num_prompt_tokens - num_prefetched_tokens)
+
+                    if (0 < self.scheduler_config.long_prefill_token_threshold
+                            < num_to_prefetch_tokens):
+                        num_to_prefetch_tokens = (
+                            self.scheduler_config.long_prefill_token_threshold)
+
+                    num_to_prefetch_tokens = min(
+                        num_to_prefetch_tokens,
+                        prefetching_token_budget,
+                    )
+
+                if num_new_prefetched_tokens > 0:
+                    logger.debug(
+                        "Request[id=%s, loaded_len=%d] is allocating kvcache "
+                        "for %d tokens",
+                        req_id,
+                        num_loaded_tokens,
+                        num_new_prefetched_tokens,
+                    )
+
+                    new_blocks = self.kv_cache_manager.allocate_slots(
+                        request,
+                        num_new_prefetched_tokens,
+                        num_lookahead_tokens=self.num_lookahead_tokens,
+                        # will cache the allocated blocks after the prefetching
+                        # request finishes prefetching and calls allocate_slots
+                        # again when it is in the waiting queue.
+                        delay_cache_blocks=True,
+                    )
+                    if new_blocks is None:
+                        logger.debug(
+                            "Request[id=%s] failed to allocate slots, skip",
+                            req_id,
+                        )
+                        break
+
+                    assert self.connector is not None
+                    self.connector.update_state_after_alloc(
+                        request,
+                        new_blocks,
+                        0,
+                    )
+
+                assert self.connector is not None
+                num_to_prefetch_tokens = self.connector.load_and_prefetch(
+                    request, num_new_prefetched_tokens, num_to_prefetch_tokens)
+                if num_to_prefetch_tokens > 0:
+                    logger.debug(
+                        "Request[id=%s, prompt_len=%d] loaded tokens: "
+                        "%d, tokens to load: %d, tokens to prefetch: "
+                        "%d. CONT.",
+                        req_id,
+                        request.num_prompt_tokens,
+                        request.num_computed_tokens,
+                        num_new_prefetched_tokens,
+                        num_to_prefetch_tokens,
+                    )
+                    prefetching_token_budget -= num_to_prefetch_tokens
+                    assert prefetching_token_budget >= 0
+
+                else:
+                    logger.debug(
+                        "Request[id=%s, prompt_len=%d] loaded tokens: "
+                        "%d, tokens to load: %d, tokens to prefetch: "
+                        "%d. TERM.",
+                        req_id,
+                        request.num_prompt_tokens,
+                        request.num_computed_tokens,
+                        num_new_prefetched_tokens,
+                        num_to_prefetch_tokens,
+                    )
+
+                self.prefetching.popleft()
+                skipped_prefetching_requests.appendleft(request)
+                continue
+
+        # Put back any skipped requests at the head of the prefetching queue
+        if skipped_prefetching_requests:
+            self.prefetching.extendleft(skipped_prefetching_requests)
+
         # Use a temporary deque to collect requests that need to be skipped
         # and put back at the head of the waiting queue later
         skipped_waiting_requests: deque[Request] = deque()
 
         # Next, schedule the WAITING requests.
         if not preempted_reqs:
-            while self.waiting and token_budget > 0:
+            while self.waiting and (token_budget > 0 or
+                                    (self.use_three_phase_kv_sched
+                                     and prefetching_token_budget > 0)):
                 if len(self.running) == self.max_num_running_reqs:
                     break
 
@@ -358,6 +513,83 @@ class Scheduler(SchedulerInterface):
                     skipped_waiting_requests.appendleft(request)
                     continue
 
+                num_to_prefetch_tokens = 0
+                if (self.use_three_phase_kv_sched
+                        and prefetching_token_budget > 0
+                        and self._is_prefetchable(request)):
+                    # Get locally-cached tokens.
+                    local_matched_blocks, num_local_matched_tokens = \
+                        self.kv_cache_manager.get_computed_blocks(
+                            request)
+
+                    req_id = request.request_id
+
+                    num_to_prefetch_tokens = (request.num_prompt_tokens -
+                                              num_local_matched_tokens)
+
+                    if (0 < self.scheduler_config.long_prefill_token_threshold
+                            < num_to_prefetch_tokens):
+                        num_to_prefetch_tokens = (
+                            self.scheduler_config.long_prefill_token_threshold)
+
+                    num_to_prefetch_tokens = min(
+                        num_to_prefetch_tokens,
+                        prefetching_token_budget,
+                    )
+
+                if num_to_prefetch_tokens > 0:
+                    assert self.connector is not None
+                    request.num_computed_tokens = num_local_matched_tokens
+                    num_to_prefetch_tokens = (self.connector.load_and_prefetch(
+                        request, 0, num_to_prefetch_tokens))
+                    if num_to_prefetch_tokens > 0:
+                        logger.debug(
+                            "Request[id=%s, prompt_len=%d] computed tokens: "
+                            "%d, tokens to prefetch: %d.",
+                            req_id,
+                            request.num_prompt_tokens,
+                            request.num_computed_tokens,
+                            num_to_prefetch_tokens,
+                        )
+                        prefetching_token_budget -= num_to_prefetch_tokens
+                        assert prefetching_token_budget >= 0
+
+                        if (num_local_matched_tokens > 0
+                                and self.kv_cache_manager.enable_caching):
+                            self.kv_cache_manager.block_pool.touch(
+                                local_matched_blocks.blocks)
+                            self.kv_cache_manager.coordinator\
+                                .save_new_computed_blocks(
+                                request.request_id, local_matched_blocks.blocks
+                            )
+                            self.local_cached_req_ids.add(request.request_id)
+                            self.connector.update_state_after_alloc(
+                                request,
+                                local_matched_blocks,
+                                0,
+                            )
+
+                        request.status = RequestStatus.WAITING_FOR_REMOTE_KVS
+                        self.waiting.popleft()
+                        self.prefetching.append(request)
+                        self.testing_req_ids.discard(req_id)
+                        continue
+                    else:
+                        request.num_computed_tokens -= num_local_matched_tokens
+                        self.nonprefetchable_req_ids.add(req_id)
+
+                if self.use_three_phase_kv_sched:
+                    # 1. we don't have token budget but still have prefetching
+                    # budget, skip current request and try the next one in the Q
+                    if prefetching_token_budget > 0 and token_budget == 0:
+                        self.waiting.popleft()
+                        skipped_waiting_requests.appendleft(request)
+                        continue
+                    # 2. we still have token budget but only continue to
+                    # schedule compute if this is a nonprefetchable request
+                    if request.request_id not in self.nonprefetchable_req_ids:
+                        break
+
                 num_external_computed_tokens = 0
                 load_kv_async = False
 
@@ -403,6 +635,7 @@ class Scheduler(SchedulerInterface):
                         num_new_tokens = (
                             self.scheduler_config.long_prefill_token_threshold)
                     num_new_tokens = min(num_new_tokens, token_budget)
+
                     assert num_new_tokens > 0
 
                     # Schedule encoder inputs.
@@ -452,6 +685,10 @@ class Scheduler(SchedulerInterface):
                         request.request_id] = req_index
                 req_index += 1
                 self.running.append(request)
+
+                if self.use_three_phase_kv_sched:
+                    self.testing_req_ids.discard(request.request_id)
+
                 if self.log_stats:
                     request.record_event(EngineCoreEventType.SCHEDULED,
                                          scheduled_timestamp)
@@ -470,6 +707,7 @@ class Scheduler(SchedulerInterface):
                 num_scheduled_tokens[request.request_id] = num_new_tokens
                 token_budget -= num_new_tokens
                 request.status = RequestStatus.RUNNING
+
                 request.num_computed_tokens = num_computed_tokens
                 # Count the number of prefix cached tokens.
                 if request.num_cached_tokens < 0:
@@ -492,11 +730,12 @@ class Scheduler(SchedulerInterface):
         assert total_num_scheduled_tokens <= self.max_num_scheduled_tokens
         assert token_budget >= 0
         assert len(self.running) <= self.max_num_running_reqs
-        # Since some requests in the RUNNING queue may not be scheduled in
-        # this step, the total number of scheduled requests can be smaller than
-        # len(self.running).
-        assert (len(scheduled_new_reqs) + len(scheduled_resumed_reqs) +
-                len(scheduled_running_reqs) <= len(self.running))
+        # Since some requests in the RUNNING/PREFETCHING queue may not be
+        # scheduled in this step, the total number of scheduled requests can
+        # be smaller than len(self.running) + len(self.prefetching).
+        assert len(scheduled_new_reqs) + len(scheduled_resumed_reqs) + len(
+            scheduled_running_reqs) <= len(self.running) + len(
+                self.prefetching)
 
         # Get the longest common prefix among all requests in the running queue.
         # This can be potentially used for cascade attention.
@@ -506,7 +745,9 @@ class Scheduler(SchedulerInterface):
             any_request = self.running[0]
             num_common_prefix_blocks = (
                 self.kv_cache_manager.get_num_common_prefix_blocks(
-                    any_request, len(self.running)))
+                    any_request,
+                    len(self.running) + len(self.local_cached_req_ids),
+                ))
 
         grammar_bitmask = self.structured_output_manager.grammar_bitmask(
             self.requests,
@@ -583,6 +824,34 @@ class Scheduler(SchedulerInterface):
         self.finished_req_ids = set()
         return scheduler_output
 
+    def _is_prefetchable(self, request: Request) -> bool:
+        req_id = request.request_id
+        if (req_id not in self.testing_req_ids
+                and req_id not in self.nonprefetchable_req_ids) or len(
+                    self.testing_req_ids) <= (self.max_testing_reqs // 2):
+            requests: list[Request] = []
+            for i in range(len(self.waiting)):
+                req = self.waiting[i]
+                if (req.request_id in self.testing_req_ids
+                        or req.request_id in self.nonprefetchable_req_ids):
+                    continue
+                if req.status == RequestStatus.WAITING:
+                    self.testing_req_ids.add(req.request_id)
+                    requests.append(req)
+                if len(requests) == self.max_testing_reqs:
+                    break
+
+            assert self.connector is not None
+            self.connector.add_requests(requests)
+
+        if req_id in self.nonprefetchable_req_ids:
+            return False
+        elif self.connector.test_request(request):
+            return True
+        else:
+            self.nonprefetchable_req_ids.add(req_id)
+            return False
+
     def _make_cached_request_data(
         self,
         request: Request,
@@ -866,7 +1135,7 @@ class Scheduler(SchedulerInterface):
 
     def get_request_counts(self) -> tuple[int, int]:
         """Returns (num_running_reqs, num_waiting_reqs)."""
-        return len(self.running), len(self.waiting)
+        return len(self.running), len(self.waiting) + len(self.prefetching)
 
     def add_request(self, request: Request) -> None:
         self.waiting.append(request)
@@ -898,6 +1167,9 @@ class Scheduler(SchedulerInterface):
 
             if request.status == RequestStatus.RUNNING:
                 self.running.remove(request)
+            elif (self.use_three_phase_kv_sched
+                  and request.status == RequestStatus.WAITING_FOR_REMOTE_KVS):
+                self.prefetching.remove(request)
             else:
                 self.waiting.remove(request)
             request.status = finished_status
@@ -912,6 +1184,10 @@ class Scheduler(SchedulerInterface):
         request_id = request.request_id
         self._cached_reqs_data.pop(request_id, None)
         self.finished_req_ids.add(request_id)
+        self.nonprefetchable_req_ids.discard(request_id)
+        self.local_cached_req_ids.discard(request_id)
+        self.testing_req_ids.discard(request_id)
+        self.prefetching_req_stats.pop(request_id, None)
         if self.finished_req_ids_dict is not None:
             self.finished_req_ids_dict[request.client_index].add(request_id)
 
@@ -928,7 +1204,7 @@ class Scheduler(SchedulerInterface):
         del self.requests[request.request_id]
 
     def get_num_unfinished_requests(self) -> int:
-        return len(self.waiting) + len(self.running)
+        return len(self.waiting) + len(self.running) + len(self.prefetching)
 
     def has_finished_requests(self) -> bool:
         return len(self.finished_req_ids) > 0
@@ -946,7 +1222,7 @@ class Scheduler(SchedulerInterface):
         assert prefix_cache_stats is not None
         return SchedulerStats(
             num_running_reqs=len(self.running),
-            num_waiting_reqs=len(self.waiting),
+            num_waiting_reqs=len(self.waiting) + len(self.prefetching),
             gpu_cache_usage=self.kv_cache_manager.usage,
             prefix_cache_stats=prefix_cache_stats,
             spec_decoding_stats=spec_decoding_stats,
@@ -1036,9 +1312,43 @@ class Scheduler(SchedulerInterface):
             scheduler the request during the next step.
         """
         # KV Connector:: update recv and send status from last step.
-        for req_id in (model_runner_output.finished_recving or ()):
-            logger.debug("Finished recving KV transfer for request %s", req_id)
-            self.finished_recving_kv_req_ids.add(req_id)
+        if (model_runner_output.finished_sending is not None
+                or model_runner_output.finished_recving is not None):
+            logger.debug(
+                "Got finished_sending %s, finished_recving %s",
+                model_runner_output.finished_sending,
+                model_runner_output.finished_recving,
+            )
+        for item in (model_runner_output.finished_recving or ()):
+            if isinstance(item, tuple):
+                req_id, num_new_prefetched_tokens = item
+                num_new_loaded_tokens = self.prefetching_req_stats.get(
+                    req_id, 0)
+                if num_new_loaded_tokens > 0:
+                    request = self.requests[req_id]
+                    # cache the blocks that have finished loading
+                    num_computed_tokens = (request.num_computed_tokens +
+                                           num_new_loaded_tokens)
+                    if num_computed_tokens == request.num_prompt_tokens:
+                        num_computed_tokens -= 1
+                    logger.debug(
+                        "Finished loading %d tokens for request %s, computed "
+                        "tokens=%d",
+                        num_new_loaded_tokens,
+                        req_id,
+                        num_computed_tokens,
+                    )
+
+                    # update num_computed_tokens
+                    request.num_computed_tokens = num_computed_tokens
+
+                logger.debug("Finished prefetching for request %s", req_id)
+                self.prefetching_req_stats[req_id] = num_new_prefetched_tokens
+            else:
+                req_id = item
+                logger.debug("Finished recving KV transfer for request %s",
+                             req_id)
+                self.finished_recving_kv_req_ids.add(req_id)
         for req_id in (model_runner_output.finished_sending or ()):
             logger.debug("Finished sending KV transfer for request %s", req_id)
             self._free_blocks(self.requests[req_id])
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index b1bc727e1..ccecc4d15 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -326,7 +326,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
     def _sync_device(self) -> None:
         torch.cuda.synchronize()
 
-    def _update_states(self, scheduler_output: "SchedulerOutput") -> None:
+    def _update_states(self, scheduler_output: "SchedulerOutput",
+                       load_results: dict[str, int]) -> None:
         """Update the cached states and the persistent batch with the scheduler
         output.
 
@@ -381,6 +382,20 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # Add new requests to the cached states.
         for new_req_data in scheduler_output.scheduled_new_reqs:
             req_id = new_req_data.req_id
+
+            num_loaded_tokens = load_results.get(req_id, 0)
+            if num_loaded_tokens > 0:
+                num_scheduled_tokens = \
+                    scheduler_output.num_scheduled_tokens[req_id]
+                if num_loaded_tokens == num_scheduled_tokens:
+                    num_loaded_tokens -= 1
+
+                new_req_data.num_computed_tokens += num_loaded_tokens
+                scheduler_output.num_scheduled_tokens[req_id] -= \
+                    num_loaded_tokens
+                scheduler_output.total_num_scheduled_tokens -= \
+                    num_loaded_tokens
+
             sampling_params = new_req_data.sampling_params
             if sampling_params.sampling_type == SamplingType.RANDOM_SEED:
                 generator = torch.Generator(device=self.device)
@@ -445,9 +460,24 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             req_id = req_data.req_id
             req_state = self.requests[req_id]
 
+            num_loaded_tokens = load_results.get(req_id, 0)
+            if num_loaded_tokens > 0:
+                num_scheduled_tokens = \
+                    scheduler_output.num_scheduled_tokens[req_id]
+                if num_loaded_tokens == num_scheduled_tokens:
+                    num_loaded_tokens -= 1
+
+                req_data.num_computed_tokens += num_loaded_tokens
+                scheduler_output.num_scheduled_tokens[req_id] -= \
+                    num_loaded_tokens
+                scheduler_output.total_num_scheduled_tokens -= \
+                    num_loaded_tokens
+
             # Update the cached states.
-            num_computed_tokens = req_data.num_computed_tokens
-            req_state.num_computed_tokens = num_computed_tokens
+            num_computed_tokens = req_data.num_computed_tokens - \
+                num_loaded_tokens
+            new_num_computed_tokens = req_data.num_computed_tokens
+            req_state.num_computed_tokens = new_num_computed_tokens
             # Add the sampled token(s) from the previous step (if any).
             # This doesn't include "unverified" tokens like spec decode tokens.
             num_new_tokens = (num_computed_tokens +
@@ -482,7 +512,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
 
             # Update the persistent batch.
             self.input_batch.num_computed_tokens_cpu[req_index] = (
-                num_computed_tokens)
+                new_num_computed_tokens)
             self.input_batch.block_table.append_row(req_data.new_block_ids,
                                                     req_index)
             # Add new_token_ids to token_ids_cpu.
@@ -1174,7 +1204,12 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         intermediate_tensors: Optional[IntermediateTensors] = None,
     ) -> Union[ModelRunnerOutput, IntermediateTensors]:
 
-        self._update_states(scheduler_output)
+        load_results = {}
+        if has_kv_transfer_group():
+            load_results = \
+                self.kv_connector_load_before_update(scheduler_output)
+
+        self._update_states(scheduler_output, load_results)
         if not scheduler_output.total_num_scheduled_tokens:
             if not has_kv_transfer_group():
                 # Return empty ModelRunnerOutput if there's no work to do.
@@ -1507,6 +1542,15 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             finished_recving=finished_recving,
         )
 
+    def kv_connector_load_before_update(
+            self, scheduler_output: "SchedulerOutput") -> dict[str, int]:
+        kv_connector = get_kv_transfer_group()
+        assert isinstance(kv_connector, KVConnectorBase_V1)
+        assert scheduler_output.kv_connector_metadata is not None
+        kv_connector.bind_connector_metadata(
+            scheduler_output.kv_connector_metadata)
+        return kv_connector.start_load_kv_before_update()
+
     def kv_connector_no_forward(
             self, scheduler_output: "SchedulerOutput") -> ModelRunnerOutput:
         # KV send/recv even if no work to do.
diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index 82db6617b..9edc9e3fc 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -26,6 +26,8 @@ from vllm.config import CompilationLevel, VllmConfig
 from vllm.core.scheduler import SchedulerOutputs
 from vllm.distributed import broadcast_tensor_dict, get_pp_group
 from vllm.distributed.kv_transfer import get_kv_transfer_group
+from vllm.distributed.kv_transfer.kv_transfer_metadata import (
+    KVTransferMetadata, KVTransferMetadataCache)
 from vllm.distributed.parallel_state import (get_tensor_model_parallel_rank,
                                              graph_capture)
 from vllm.forward_context import get_forward_context, set_forward_context
@@ -58,8 +60,10 @@ from vllm.utils import (DeviceMemoryProfiler, GiB_bytes, PyObjectCache,
 from vllm.worker.model_runner_base import (
     InputProcessingError, ModelRunnerBase, ModelRunnerInputBase,
     ModelRunnerInputBuilderBase, _add_attn_metadata_broadcastable_dict,
+    _add_kv_transfer_metadata_broadcastable_dict,
     _add_sampling_metadata_broadcastable_dict,
     _init_attn_metadata_from_tensor_dict,
+    _init_kv_transfer_metadata_from_tensor_dict,
     _init_sampling_metadata_from_tensor_dict)
 
 if TYPE_CHECKING:
@@ -104,12 +108,15 @@ class ModelInputForGPU(ModelRunnerInputBase):
     async_callback: Optional[Callable] = None
     scheduler_outputs: Optional[SchedulerOutputs] = None
     previous_hidden_states: Optional[torch.Tensor] = None
+    kv_transfer_metadata: Optional[KVTransferMetadata] = None
 
     def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:
         tensor_dict = {
             "input_tokens": self.input_tokens,
             "inputs_embeds": self.inputs_embeds,
             "input_positions": self.input_positions,
+            "seq_lens": self.seq_lens,
+            "query_lens": self.query_lens,
             "lora_requests": self.lora_requests,
             "lora_mapping": self.lora_mapping,
             "multi_modal_kwargs": self.multi_modal_kwargs,
@@ -120,6 +127,8 @@ class ModelInputForGPU(ModelRunnerInputBase):
             "finished_requests_ids": self.finished_requests_ids,
         }
         _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)
+        _add_kv_transfer_metadata_broadcastable_dict(tensor_dict,
+                                                     self.kv_transfer_metadata)
         return tensor_dict
 
     @classmethod
@@ -131,6 +140,7 @@ class ModelInputForGPU(ModelRunnerInputBase):
         if attn_backend is not None:
             tensor_dict = _init_attn_metadata_from_tensor_dict(
                 attn_backend, tensor_dict)
+        _init_kv_transfer_metadata_from_tensor_dict(tensor_dict)
         return cls(**tensor_dict)
 
     # Exclude `async_callback` to be able to pickle this object
@@ -161,6 +171,8 @@ class ModelInputForGPUWithSamplingMetadata(ModelInputForGPU):
             "input_tokens": self.input_tokens,
             "inputs_embeds": self.inputs_embeds,
             "input_positions": self.input_positions,
+            "seq_lens": self.seq_lens,
+            "query_lens": self.query_lens,
             "lora_requests": self.lora_requests,
             "lora_mapping": self.lora_mapping,
             "multi_modal_kwargs": self.multi_modal_kwargs,
@@ -171,6 +183,8 @@ class ModelInputForGPUWithSamplingMetadata(ModelInputForGPU):
             "finished_requests_ids": self.finished_requests_ids,
         }
         _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)
+        _add_kv_transfer_metadata_broadcastable_dict(tensor_dict,
+                                                     self.kv_transfer_metadata)
         _add_sampling_metadata_broadcastable_dict(tensor_dict,
                                                   self.sampling_metadata)
         return tensor_dict
@@ -185,6 +199,7 @@ class ModelInputForGPUWithSamplingMetadata(ModelInputForGPU):
         if attn_backend is not None:
             tensor_dict = _init_attn_metadata_from_tensor_dict(
                 attn_backend, tensor_dict)
+        _init_kv_transfer_metadata_from_tensor_dict(tensor_dict)
         return cls(**tensor_dict)
 
 
@@ -1163,6 +1178,10 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
               SamplingMetadataCache() \
                 if self.parallel_config.pipeline_parallel_size == 1 else None
 
+        self.kv_transfer_metadata_cache: KVTransferMetadataCache = (
+            KVTransferMetadataCache()
+            if self.vllm_config.kv_transfer_config is not None else None)
+
         if hasattr(self, "_builder_cls"):
             # multi-step model runner does not have `_builder_cls`
             self.builder = self._builder_cls(weakref.proxy(self))
@@ -1747,8 +1766,21 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
             sampling_metadata = None
         is_prompt = (seq_group_metadata_list[0].is_prompt
                      if seq_group_metadata_list else None)
+
+        if self.vllm_config.kv_transfer_config is not None:
+            kv_transfer_metadata = KVTransferMetadata.prepare(
+                seq_group_metadata_list,
+                self.kv_transfer_metadata_cache,
+            )
+            # attach seq_group_metadata_list for rebuilding model_input
+            # on the driver side
+            kv_transfer_metadata.seq_group_metadata_list = \
+                seq_group_metadata_list
+        else:
+            kv_transfer_metadata = None
         return dataclasses.replace(model_input,
                                    sampling_metadata=sampling_metadata,
+                                   kv_transfer_metadata=kv_transfer_metadata,
                                    is_prompt=is_prompt,
                                    virtual_engine=virtual_engine)
 
@@ -1814,6 +1846,9 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):
         # NOTE: The receive operation is blocking
         bypass_model_exec = False
         if self.need_recv_kv(model_input, kv_caches):
+            # attach runner for rebuilding model_input
+            model_input.kv_transfer_metadata.runner = weakref.proxy(
+                self)  # type: ignore
             hidden_or_intermediate_states, bypass_model_exec, model_input = \
                 get_kv_transfer_group().recv_kv_caches_and_hidden_states(
                     # model is used to know which layer the current worker
diff --git a/vllm/worker/model_runner_base.py b/vllm/worker/model_runner_base.py
index d567ce4a6..5b5360e75 100644
--- a/vllm/worker/model_runner_base.py
+++ b/vllm/worker/model_runner_base.py
@@ -17,6 +17,8 @@ from vllm.sequence import IntermediateTensors, SequenceGroupMetadata
 if TYPE_CHECKING:
     from vllm.attention import AttentionMetadata
     from vllm.attention.backends.abstract import AttentionBackend
+    from vllm.distributed.kv_transfer.kv_transfer_metadata import (
+        KVTransferMetadata)
     from vllm.model_executor import SamplingMetadata
 
 logger = init_logger(__name__)
@@ -47,7 +49,7 @@ def _init_attn_metadata_from_tensor_dict(
     valid_attn_kwargs = {}
     for field in dataclasses.fields(attn_backend.get_metadata_cls()):
         if field.name in tensor_dict:
-            if field.name == "input_positions":
+            if field.name in ["input_positions", "seq_lens"]:
                 valid_attn_kwargs[field.name] = tensor_dict[field.name]
             else:
                 valid_attn_kwargs[field.name] = tensor_dict.pop(field.name)
@@ -90,6 +92,34 @@ def _add_sampling_metadata_broadcastable_dict(
             sampling_metadata.selected_token_indices)
 
 
+def _init_kv_transfer_metadata_from_tensor_dict(
+        tensor_dict: Dict[str, Any]) -> Dict[str, Any]:
+    """
+    Helper method to initialize KVTransferMetadata based on broadcastable
+    KVTransferMetadata fields.
+    """
+    from vllm.distributed.kv_transfer.kv_transfer_metadata import (
+        KVTransferMetadata)
+
+    kv_transfer_metadata = tensor_dict.pop("kv_transfer_metadata", None)
+    if kv_transfer_metadata is not None:
+        seq_groups = kv_transfer_metadata
+        tensor_dict["kv_transfer_metadata"] = KVTransferMetadata(
+            seq_groups=seq_groups)
+    return tensor_dict
+
+
+def _add_kv_transfer_metadata_broadcastable_dict(
+        tensor_dict: Dict[str, Any],
+        kv_transfer_metadata: Optional["KVTransferMetadata"]) -> None:
+    """
+    Helper method to update tensor_dict with broadcastable
+    KVTransferMetadata fields.
+    """
+    if kv_transfer_metadata is not None:
+        tensor_dict["kv_transfer_metadata"] = kv_transfer_metadata.seq_groups
+
+
 def _init_frozen_model_input_from_tensor_dict(
         frozen_model_input_cls: Type["ModelRunnerInputBase"],
         tensor_dict: Dict[str, Any]) -> Dict[str, Any]:
