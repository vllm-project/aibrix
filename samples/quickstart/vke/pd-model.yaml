apiVersion: orchestration.aibrix.ai/v1alpha1
kind: StormService
metadata:
  name: vllm-1p1d
spec:
  replicas: 1
  updateStrategy:
    type: InPlaceUpdate
  stateful: true
  selector:
    matchLabels:
      app: vllm-1p1d
  template:
    metadata:
      labels:
        app: vllm-1p1d
    spec:
      roles:
        - name: prefill
          replicas: 1
          stateful: true
          template:
            metadata:
              labels:
                model.aibrix.ai/name: deepseek-r1-distill-llama-8b
                model.aibrix.ai/port: "8000"
                model.aibrix.ai/engine: vllm
            spec:
              containers:
                - name: prefill
                  image: aibrix-public-release-cn-beijing.cr.volces.com/aibrix/vllm-openai:0.11.0-cu128-nixl-v0.4.1
                  command: ["sh", "-c"]
                  args:
                    - |
                      python3 -m vllm.entrypoints.openai.api_server \
                      --host "0.0.0.0" \
                      --port "8000" \
                      --uvicorn-log-level warning \
                      --model /models/DeepSeek-R1-Distill-Llama-8B \
                      --served-model-name deepseek-r1-distill-llama-8b
                  env:
                    - name: PYTHONHASHSEED
                      value: "1047"
                    - name: VLLM_SERVER_DEV_MODE
                      value: "1"
                    - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                      value: "0.0.0.0"
                    - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                      value: "5558"
                    - name: VLLM_WORKER_MULTIPROC_METHOD
                      value: spawn
                    - name: VLLM_ENABLE_V1_MULTIPROCESSING
                      value: "0"
                    - name: GLOO_SOCKET_IFNAME
                      value: eth0
                    - name: NCCL_SOCKET_IFNAME
                      value: eth0
                  resources:
                    limits:
                      nvidia.com/gpu: 1
                    requests:
                      nvidia.com/gpu: 1
                  securityContext:
                    capabilities:
                      add:
                        - IPC_LOCK
                  volumeMounts:
                    - mountPath: /models
                      name: model-hostpath
              volumes:
                - name: model-hostpath
                  hostPath:
                    path: /root/models
                    type: DirectoryOrCreate
        - name: decode
          replicas: 1
          stateful: true
          template:
            metadata:
              labels:
                model.aibrix.ai/name: deepseek-r1-distill-llama-8b
                model.aibrix.ai/port: "8000"
                model.aibrix.ai/engine: vllm
            spec:
              containers:
                - name: decode
                  image: aibrix-public-release-cn-beijing.cr.volces.com/aibrix/vllm-openai:0.11.0-cu128-nixl-v0.4.1
                  command: ["sh", "-c"]
                  args:
                    - |
                      python3 -m vllm.entrypoints.openai.api_server \
                      --host "0.0.0.0" \
                      --port "8000" \
                      --uvicorn-log-level warning \
                      --model /models/DeepSeek-R1-Distill-Llama-8B \
                      --served-model-name deepseek-r1-distill-llama-8b \
                      --kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
                  env:
                    - name: PYTHONHASHSEED
                      value: "1047"
                    - name: VLLM_SERVER_DEV_MODE
                      value: "1"
                    - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                      value: "0.0.0.0"
                    - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                      value: "5558"
                    - name: VLLM_WORKER_MULTIPROC_METHOD
                      value: spawn
                    - name: VLLM_ENABLE_V1_MULTIPROCESSING
                      value: "0"
                    - name: GLOO_SOCKET_IFNAME
                      value: eth0
                    - name: NCCL_SOCKET_IFNAME
                      value: eth0
                    - name: NCCL_IB_DISABLE
                      value: "0"
                    - name: NCCL_IB_GID_INDEX
                      value: "7"
                    - name: NCCL_DEBUG
                      value: "INFO"
                    - name: UCX_TLS
                      value: ^gga
                  resources:
                    limits:
                      nvidia.com/gpu: 1
                    requests:
                      nvidia.com/gpu: 1
                  securityContext:
                    capabilities:
                      add:
                        - IPC_LOCK
                  volumeMounts:
                    - mountPath: /models
                      name: model-hostpath
              volumes:
                - name: model-hostpath
                  hostPath:
                    path: /root/models
                    type: DirectoryOrCreate