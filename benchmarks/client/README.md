## Test client locally

Starting vllm server:


```shell
export API_KEY=${API_KEY}
python3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 \
--port "8000" \
--model /root/models/deepseek-llm-7b-chat \
--trust-remote-code \
--max-model-len "4096" \
--api-key ${API_KEY} \
--enable-chunked-prefill
```

Using a sample workload (generated by [the workload generator](../generator/README.md)) in a client:

```shell
export API_KEY=${API_KEY}
python3 client.py \
--workload-path "../generator/output/constant.jsonl" \
--endpoint "http://localhost:8000" \
--model /root/models/deepseek-llm-7b-chat \
--api-key ${API_KEY} \
--client-type streaming
--output-file-path output.jsonl
```

Run analysis on metrics collected:

```shell
python analyze.py --trace output.jsonl --output output
```

The output will be stored as a `.jsonl` file in `output.jsonl`