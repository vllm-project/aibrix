## Test client locally
Starting vllm server:

```shell
python3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 \
--port "8000" \
--model /root/models/deepseek-llm-7b-chat \
--trust-remote-code \
--max-model-len "4096" \
--api-key sk-kFJ12nKsFVfVmGpj3QzX65s4RbN2xJqWzPYCjYu7wT3BlbLi \
--enable-chunked-prefill
```
Using a sample workload (generated by [the workload generator](../generator/README.md)) in a client:

```shell
python3 client.py \
--workload-path "../generator/output/constant.jsonl" \
--endpoint "http://localhost:8000" \
--model /root/models/deepseek-llm-7b-chat \
--api-key sk-kFJ12nKsFVfVmGpj3QzX65s4RbN2xJqWzPYCjYu7wT3BlbLi \
--client-type streaming
--output-file-path output.jsonl
```

The output will be stored as a `.jsonl` file in `output.jsonl`